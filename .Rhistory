l <- large[i]
total_500_final$im_s_large <- total_500_final$im_s_large + total_500_final[,l]}
total_500_final$im_s_large <- total_500_final$im_s_large/length(large)
for(i in 1:375){
if (total_500_final$im_s_large[i] >1){
total_500_final$im_s_large[i] <- 1
}else{
total_500_final$im_s_large[i] <- 0 #they do not have images of this size
}
}
par(mfrow=c(1,1))
barplot(table(total_500_final$im_s_large),col = "darkblue", main="Large images 1:NO, 2:YES")
###########################################################################################################
medium <- c(37,67,68,71,85,87,94,96,121,124,154,180,183,198,207,227,238,269,281,288,290)
total_500_final$im_s_medium <- 0
for(i in 1:21){
m <- medium[i]
total_500_final$im_s_medium <- total_500_final$im_s_medium + total_500_final[,m]}
total_500_final$im_s_medium <- total_500_final$im_s_medium/length(medium)
for(i in 1:375){
if (total_500_final$im_s_medium[i] >1){
total_500_final$im_s_medium[i] <- 1
}else{
total_500_final$im_s_medium[i] <- 0 #they do not have images of this size
}
}
par(mfrow=c(1,1))
barplot(table(total_500_final$im_s_medium),col = "darkgreen", main="Medium images 1:NO, 2:YES")
###########################################################################################################
small <- c(25,26,30,31,32,33,38,39,40,43,44,46,47,51,55,57,60,63,72,73,78,79,88,89,90,95,101,102,103,106,107,109,112,117,120,128,135,143,146,147,148,149,151,152,156,160,161,162,163,164,165,166,167,168,170,171,174,189,190,192,194,196,197,199,201,203,204,206,208,225,226,230,231,233,236,236,237,242,244,246,247,248,252,257,263,264,266,268,271,272,273,275,278,279,282,283,284,289)
total_500_final$im_s_small <- 0
for(i in 1:98){
sl <- small[i]
total_500_final$im_s_small <- total_500_final$im_s_small + total_500_final[,sl]}
total_500_final$im_s_small <- total_500_final$im_s_small/length(small)
for(i in 1:375){
if (total_500_final$im_s_small[i] >1){
total_500_final$im_s_small[i] <- 1
}else{
total_500_final$im_s_small[i] <- 0 #they do not have images of this size
}
}
par(mfrow=c(1,1))
barplot(table(total_500_final$im_s_small),col = "red", main="Small images 1:NO, 2:YES")
###########################################################################################################
thumbnail<- c(22,41,64,84,91,92,93,108,126,127,169,224,240,245,250,27,142,28,144,29,155,35,158,36,172,42,173,175,48,176,49,177,50,178,56,179,58,181,184,59,185,66,186,187,188,77,82,191,86,193,195,104,200,105,202,110,111,205,115,116,223,229,125,239,241,130,132,243,133,134,136,251,253,254,255,256,259,260,261,262,267,270,276)
total_500_final$im_s_thumbnail <- 0
for(i in 1:83){
tl <- thumbnail[i]
total_500_final$im_s_thumbnail <- total_500_final$im_s_thumbnail + total_500_final[,tl]}
total_500_final$im_s_thumbnail <- total_500_final$im_s_thumbnail/length(thumbnail)
for(i in 1:375){
if (total_500_final$im_s_thumbnail[i] >1){
total_500_final$im_s_thumbnail[i] <- 1
}else{
total_500_final$im_s_thumbnail[i] <- 0 #they do not have images of this size
}
}
par(mfrow=c(1,1))
barplot(table(total_500_final$im_s_thumbnail),col = "blue", main="Thumbnail images 1:NO, 2:YES")
#Now we will substract the sizes variables an keep only the new ones we created
total_500_final <- total_500_final[,-c(22:290)]
#str(total_500_final)
total_500_final$Market_Value <- NULL
total_500_final$Assets <- NULL
total_500_final$Ranking <- NULL #we also remove the variable Ranking and we keep only the Revenues varibale from the Fortune 500 variables as we said in the beggining
total_500_final$total.links <- NULL
total_500_final$twitter <- NULL
total_500_final$Total_SH_Equity <- NULL
#We will try to create a regression model to see which of the variables of the websites play the most important part regarding the Ranking of the company.
#We create the empty lm model
model_null = lm(Revenues~1,data=total_500_final)
summary(model_null)
#LASSO and Logistic Regression models
library(glmnet)
#We create a full model for the variable Ranking
full <- lm(Revenues~.,data=total_500_final)
summary(full)
x <- model.matrix(full) [,-1]
dim(x)
lasso <- glmnet (x, total_500_final$Revenues)
par(mfrow=c(1,1),no.readonly = TRUE)
plot(lasso,label=T)
plot(lasso, xvar='lambda', label=T)
lassob <- cv.glmnet(x,total_500_final$Revenues)
lassob$lambda.min
lassob$lambda.1se
plot(lassob)
#We see the coefficients for lamda min
blasso <- coef(lassob, s="lambda.min")
blasso
dim(blasso)
zblasso <- blasso[-1] * apply(x,2,sd)
zbolt <- coef (full) [-1] * apply (x,2,sd)
azbolt <- abs(zbolt)
sum(azbolt)
#since the sum is NA that means we have to substract some variables
# in order to find which variables to substract we run the coefficients and we see which of them has NA as result
coef(full)
#Now we create a new model with only the variables with coef different from NA
total_500_final_r <- total_500_final[,-c(2)]
full_2 <- lm(Revenues~.,data=total_500_final_r)
summary(full_2)
x <- model.matrix(full_2) [,-1]
dim(x)
lasso <- glmnet (x, total_500_final_r$Revenues)
par(mfrow=c(1,1),no.readonly = TRUE)
plot(lasso,label=T)
plot(lasso, xvar='lambda', label=T)
lassob <- cv.glmnet(x,total_500_final_r$Revenues)
lassob$lambda.min
lassob$lambda.1se
plot(lassob)
#coefiecinets for lammda min
blasso <- coef(lassob, s="lambda.min")
blasso
dim(blasso)
zblasso <- blasso[-1] * apply(x,2,sd)
zbolt <- coef (full_2) [-1] * apply (x,2,sd)
azbolt <- abs(zbolt)
sum(azbolt)
s <- sum(abs(zblasso))/sum(abs(azbolt))
s
#The model based on the lasso method bu taking the lambda.min is
full_3 <- lm(Revenues~ number_of_errors+number_of_warning+external+internal+instagram+pinterest+youtube+total_words+total.images+loading.time+ im_s_verylarge+ im_s_large+ im_s_medium+ im_s_small+ im_s_thumbnail,data=total_500_final_r)
summary(full_3)
adj_r_square_full3 <- summary(full_3)$adj.r.squared
aic_full3 <- AIC(full_3)
par(mfrow=c(1,1))
#We create the 2 basic plots so as to be able to explain the regression model
plot(full_3,which=1:3)
#Now we create a new model with only the variables with coef different from NA
total_500_final_r <- total_500_final[,-c(2,22)]
full_2 <- lm(Revenues~.,data=total_500_final_r)
summary(full_2)
x <- model.matrix(full_2) [,-1]
dim(x)
lasso <- glmnet (x, total_500_final_r$Revenues)
par(mfrow=c(1,1),no.readonly = TRUE)
plot(lasso,label=T)
plot(lasso, xvar='lambda', label=T)
lassob <- cv.glmnet(x,total_500_final_r$Revenues)
lassob$lambda.min
lassob$lambda.1se
plot(lassob)
#coefiecinets for lammda min
blasso <- coef(lassob, s="lambda.min")
blasso
dim(blasso)
zblasso <- blasso[-1] * apply(x,2,sd)
zbolt <- coef (full_2) [-1] * apply (x,2,sd)
azbolt <- abs(zbolt)
sum(azbolt)
s <- sum(abs(zblasso))/sum(abs(azbolt))
s
full_3 <- lm(Revenues~ non.document.error+external+instagram+youtube+total.images+im_s_verylarge+im_s_medium+ im_s_small+ im_s_thumbnail,data=total_500_final_r)
summary(full_3)
adj_r_square_full3 <- summary(full_3)$adj.r.squared
aic_full3 <- AIC(full_3)
par(mfrow=c(1,1))
#We create the 2 basic plots so as to be able to explain the regression model
plot(full_3,which=1:3)
blassob <- coef(lassob, s="lambda.1se")
blassob
zblassob <- blassob[-1] * apply(x,2,sd)
zboltb <- coef (full_2) [-1] * apply (x,2,sd)
s <- sum(abs(zblassob))/sum(abs(zboltb))
s
full_4 <- lm(Revenues~ im_s_verylarge+im_s_medium+ im_s_small+ im_s_thumbnail,data=total_500_final_r)
summary(full_4)
adj_r_square_full4 <- summary(full_4)$adj.r.squared
aic_full4 <- AIC(full_4)
par(mfrow=c(1,1))
#We create the 2 basic plots so as to be able to explain the regression model
plot(full_4,which=1:3)
###############################################
#We use the "both" method to compare the full_3 model with the null model to see how many variables are indeed important
model_a <- step(model_null, scope = list(lower = model_null, upper=full_3), direction = "both")
coef <- round(summary(model_a)$coefficients,2)
coef
ad_r_sq_ma <- summary(model_a)$adj.r.squared
aic_ma <- AIC(model_a)
par(mfrow=c(1,1))
#We create the 2 basic plots so as to be able to explain the regression model
plot(model_a,which=1:3)
summary(model_a)
#We compare the Adjusted R squares of the models and also the AIC of the models we created to find the best one
adj_r_square_full3
adj_r_square_full4
ad_r_sq_ma
aic_full3
aic_full4
aic_ma
summary(full_3)
names(total_500_final_r)
total_500_final_reg <- total_500_final_r[,c(1,2,5,8,11,20,21,22,23)]
names(total_500_final_reg)
set.seed(20)
fortuneCluster <- kmeans(total_500_final_reg[, 1:9], 5, nstart = 20)
table(fortuneCluster$cluster)
fortuneCluster$cluster <- as.factor(fortuneCluster$cluster)
ggplot(total_500_final_reg, aes(Revenues, number_of_errors, color = fortuneCluster$cluster)) + geom_point(size=3)
ggplot(total_500_final_reg, aes(Revenues, number_of_warning, color = fortuneCluster$cluster)) + geom_point(size=3)
ggplot(total_500_final_reg, aes(number_of_warning, number_of_errors, color = fortuneCluster$cluster)) + geom_point(size=3)
ggplot(total_500_final_reg, aes(Revenues, internal, color = fortuneCluster$cluster)) + geom_point(size=3)
ggplot(total_500_final_reg, aes(Revenues, pinterest, color = fortuneCluster$cluster)) + geom_point(size=3)
ggplot(total_500_final_reg, aes(Revenues, youtube, color = fortuneCluster$cluster)) + geom_point(size=3)
ggplot(total_500_final_reg, aes(Revenues, total.images, color = fortuneCluster$cluster)) + geom_point(size=3)
ggplot(total_500_final_reg, aes(Revenues, loading.time, color = fortuneCluster$cluster)) + geom_point(size=3)
ggplot(total_500_final_reg, aes(Revenues, im_s_large, color = fortuneCluster$cluster)) + geom_point(size=3)
ggplot(total_500_final_reg, aes(Revenues, im_s_medium, color = fortuneCluster$cluster)) + geom_point(size=3)
ggplot(total_500_final_reg, aes(Revenues, im_s_small, color = fortuneCluster$cluster)) + geom_point(size=3)
ggplot(total_500_final_reg, aes(Revenues, im_s_thumbnail, color = fortuneCluster$cluster)) + geom_point(size=3)
table(fortuneCluster$cluster)
predictions <- predict(full_3, total_500_final_test)
summary(predictions)
mse_fv <- compute_mse(full_3$fitted.values, total_500_final_test$Revenues)
mse_fv
mse_p <- compute_mse(predictions, total_500_final_test$Revenues)
mse_p
coef(full)
```{r}
#we upload the dataset
total_500 <- read.csv("~/GitHub/thesis_msc_business_analytics/total_500.csv", sep=";", na.strings="n/a")
#we see how many observations and how many variables we have
dim(total_500)
#We create a subset to make some changes to the data
total_500_sub <- total_500
#Change the decimal point for the 4 variables
total_500_sub$Assets.. <- gsub(",", ".", total_500_sub$Assets.. )
total_500_sub$Market.value.. <- gsub(",", ".", total_500_sub$Market.value.. )
total_500_sub$Revenues.. <- gsub(",", ".", total_500_sub$Revenues.. )
total_500_sub$Total.Stockholder.Equity.. <- gsub(",", ".", total_500_sub$Total.Stockholder.Equity.. )
#Make the variables numeric
for(i in 1:741){
total_500_sub[,i] <- as.numeric(total_500_sub[,i])}
#We omit the nas from the analysis
total_500_final <- na.omit(total_500_sub)
#We rename variable X as Ranking
colnames(total_500_final)[1] <- "Ranking"
#Change the names of some variables to be more easily readable
colnames(total_500_final)[2] <- "Assets"
colnames(total_500_final)[3] <- "Market_Value"
colnames(total_500_final)[4] <- "Revenues"
colnames(total_500_final)[6] <- "Total_SH_Equity"
#Delete the variables we will not need
total_500_final[5] <- NULL #Revenues %
total_500_final[6] <- NULL #company name
total_500_final[21] <- NULL # company url
total_500_final[20] <- NULL # readability index
#we upload the libraries beneath that we will use in the analysis
library(ggplot2)
library(reshape2)
library(DAAG)
#######################################################################################################
#######################################################################################################
#we first see the summary of the Fortune variables and then we create their histogram so as to have a
#good grasp of how they are distributed
ggplot(data=total_500_final,aes(x=Revenues))+geom_histogram(binwidth=50, colour = "green", fill ="darkgreen")
ggplot(data=total_500_final,aes(x=Assets))+geom_histogram(binwidth=100, colour = "red", fill ="darkred")
ggplot(data=total_500_final,aes(x=Market_Value))+geom_histogram(binwidth=100, colour = "blue", fill ="darkblue")
ggplot(data=total_500_final,aes(x=Total_SH_Equity))+geom_histogram(binwidth=100, colour = "purple", fill ="pink")
###############################################################################################
#We make plots to see how the variables we got from Fortune 500 are related with the Ranking
ggplot(total_500_final, aes(Assets,Ranking)) + geom_point()
ggplot(total_500_final, aes(Market_Value, Ranking)) + geom_point()
ggplot(total_500_final, aes(Total_SH_Equity, Ranking)) + geom_point()
ggplot(total_500_final, aes(Revenues, Ranking)) + geom_point()
#We can see that the Ranking has a linear relationship with the Revenues so we will use one of those 2 variables to check the relationships with the websites metrics
#In order to have a more clear look we also create a correlation diagram
total_500_fortune <- total_500_final[,c(1:5)]
library(corrplot)
library(caret)
sm <- cor(total_500_fortune)
sm
corrplot(cor(total_500_fortune),method="number")
#From this plot we understand that the Ranking and the Revenues have very high correlation.
##########################################################################################################
```
```{r}
#Firstly we will analyze the social media relevance with the sites.
#We will see how many of the sites have social media and what type of social media
#Facebook
social_media_facebook <- round(table(total_500_final$facebook)/375,3)
social_media_facebook
slicelable <- c(paste(38.8,"% no"),paste(61.2,"% yes"))
pie(social_media_facebook,label = slicelable,main="Share of companies with Facebook",col=rainbow(length(social_media_facebook)))
ggplot(total_500_final, aes(Revenues, facebook)) + geom_point(size=3, colour = "darkblue")
#Twitter
social_media_twitter <- round(table(total_500_final$twitter)/375,3)
social_media_twitter
slicelable <- c(paste(33.7,"% no"),paste(66.3,"% yes"))
pie(social_media_twitter,label = slicelable,main="Share of companies with Twitter",col=rainbow(length(social_media_twitter)))
ggplot(total_500_final, aes(Revenues, twitter)) + geom_point(size=3, colour = "darkgreen")
#Instagram
social_media_instagram <- round(table(total_500_final$instagram)/375,3)
social_media_instagram
slicelable <- c(paste(79.1,"% no"),paste(20.9,"% yes"))
pie(social_media_instagram,label = slicelable,main="Share of companies with Instagram",col=rainbow(length(social_media_instagram)))
ggplot(total_500_final, aes(Revenues, instagram)) + geom_point(size=3, colour = "pink")
#Pinterest
social_media_pinterest <- round(table(total_500_final$pinterest)/375,3)
social_media_pinterest
slicelable <- c(paste(90.4,"% no"),paste(9.6,"% yes"))
pie(social_media_pinterest,label = slicelable,main="Share of companies with Pinterest",col=rainbow(length(social_media_pinterest)))
ggplot(total_500_final, aes(Revenues, pinterest)) + geom_point(size=3, colour = "darkred")
#Youtube
social_media_youtube <- round(table(total_500_final$youtube)/375,3)
social_media_youtube
slicelable <- c(paste(44.3,"% no"),paste(55.7,"% yes"))
pie(social_media_youtube,label = slicelable,main="Share of companies with Youtube",col=rainbow(length(social_media_youtube)))
ggplot(total_500_final, aes(Revenues, youtube)) + geom_point(size=3, colour = "red")
#LinkedIn
social_media_linkedin <- round(table(total_500_final$linkedin)/375,3)
social_media_linkedin
slicelable <- c(paste(45.4,"% no"),paste(54.6,"% yes"))
pie(social_media_linkedin,label = slicelable,main="Share of companies with Linkedin",col=rainbow(length(social_media_linkedin)))
ggplot(total_500_final, aes(Revenues, linkedin)) + geom_point(size=3, colour = "blue")
#And we can also see for correlations
total_500_social_media <- total_500_final[,c(1,13:18)]
library(corrplot)
library(caret)
sm <- cor(total_500_social_media)
sm
corrplot(cor(total_500_social_media),method="number")
#The most high correlation is between facebook and twitter 69%
#While the second highest is between twitter and linkedIn 59%
#So we will not include the twitter variable in the regression model
#########################################################################################################
```
```{r}
#We will now check the links by creating an histogram
#Then we create ggplots in order to see in what frequency the links appear
par(mfrow=c(1,1))
library(ggplot2)
ggplot(data=total_500_final,aes(x=total.links))+geom_histogram(binwidth=50, colour = "darkblue", fill ="blue")
ggplot(total_500_final, aes(Revenues, total.links)) + geom_point(size=3, colour = "darkblue")
ggplot(data=total_500_final,aes(x=external))+geom_histogram(binwidth=50, colour = "darkred", fill ="red")
ggplot(total_500_final, aes(Revenues, external)) + geom_point(size=3, colour = "darkred")
ggplot(data=total_500_final,aes(x=internal))+geom_histogram(binwidth=50, colour = "darkgreen", fill ="green")
ggplot(total_500_final, aes(Revenues, internal)) + geom_point(size=3, colour = "darkgreen")
#And we can also see for correlations
total_500_links <- total_500_final[,c(1,10:12)]
library(corrplot)
library(caret)
tl <- cor(total_500_links)
tl
corrplot(cor(total_500_links),method="number")
#We can see that the total links with the internal links have a correlation almost 97%.
#So we will not include the total links in the regression model
#########################################################################################################
#Now we will see the loading time per site
ggplot(data=total_500_final,aes(x=loading.time))+geom_histogram(binwidth=1, colour = "pink", fill ="purple")
ggplot(total_500_final, aes(Revenues, loading.time)) + geom_point(size=3, colour = "purple")
#########################################################################################################
#Now we will see the words in total and in unique count in relation with the readability index
ggplot(data=total_500_final,aes(x=total_words,fill=Readability))+geom_histogram(binwidth=50, colour = "darkred", fill ="red")
ggplot(data=total_500_final,aes(x=unique_words, fill=Readability))+geom_histogram(binwidth=50, colour = "darkred", fill ="red")
#########################################################################################################
#Now we will see the number of errors and warnings alone and in relationship with the Revenues
ggplot(data=total_500_final,aes(x=number_of_errors))+geom_histogram(binwidth=50, colour = "red")
ggplot(total_500_final, aes(Revenues, number_of_errors)) + geom_point(size=3, colour = "dark red")
ggplot(data=total_500_final,aes(x=number_of_warning))+geom_histogram(binwidth=20, colour = "red")
ggplot(total_500_final, aes(Revenues, number_of_warning)) + geom_point(size=3, colour = "dark blue")
#########################################################################################################
```
```{r}
ggplot(data=total_500_final,aes(x=total.images))+geom_histogram(binwidth=100, colour = "darkred", fill ="red")
#########################################################################################################
#We will see now the frequency of image types that is being used
dev.off()
par(mfrow=c(3,3))
k = c(727:735)
for(i in 1:9){
a <- k[i]
image_type<- round(table(total_500_final[,a])/375,3)
barplot(image_type,xlab=names(total_500_final)[a],ylab = "Shares of images per site", col = "dark green")}
#It is obvious that the most common images type are .jpg,.png and .gif
#So they will be the ones that we will keep
#Delete the variables we will not need
total_500_final[727] <- NULL #.bmp
total_500_final[727] <- NULL #.dib
total_500_final[728] <- NULL # .jpe
total_500_final[728] <- NULL # .jpeg
total_500_final[730] <- NULL # .tif
total_500_final[730] <- NULL # .tiff
##########################################################################################################
#Now we will check the sizes of the images used
#2 means YES and 1 means NO
dev.off()
par(mfrow=c(3,3))
ks = c(22:726)
for(i in 1:705){
a <- ks[i]
image_type<- round(table(total_500_final[,a])/375,3)
barplot(image_type,xlab=names(total_500_final)[a],ylab = "Shares of images per site", col = "dark red")}
#Firstly we will keep the image sizes that exist in more than the half od the sites that we are examining
#So we will keep until the size x210x420 [304]
total_500_final <- total_500_final[,-c(305:726)]
#We will also subtrack the sizes that are not clear
#x1x1 [24], x11x8 [42], x [44], x1x700 [50], X1x10 [53], "X10x1" [54],"X1x660" [56],"X19x1"[57], 2x2 [60],  x0x0 [74],"X1x110"[208], "autox100." [249],"autox200" [250], "X2x213" [255],
total_500_final <- total_500_final[,-c(24,42,44,50,53,54,56,57,60,74,208,249,250,255)]
#names(total_500_final)
#We still have many bariables in order to make a regression model
#So we will group the sizes based  on the following 5 categories so as to have a more calable information
#If at least one of the dimensions belongs to a category we choose the higher category that a dimension belongs
#Very large size: more than 800pixels
#Large: 500 - 799 pixels
#Medium: 300 - 499 pixels
#Small: 100 - 299 pixels
#Thumbnail: less than 100 pixels
############################################################################################################
verylarge <- c(24,34,53,54,61,62,69,70,74,75,76,80,81,83,97,98,99,114,122,123,131,138,140,141,145,153,157,182,209,210,212,213,214,215,216,217,219,220,221,222,228,234,258,274,277,280,285,286,287)
total_500_final$im_s_verylarge <- 0
k<-0
for(i in 1:49){
k <- verylarge[i]
for(i in 1:375){
total_500_final$im_s_verylarge[i] <- total_500_final$im_s_verylarge[i] + total_500_final[i,k]
}}
total_500_final$im_s_verylarge <- total_500_final$im_s_verylarge/length(verylarge)
for(i in 1:375){
if (total_500_final$im_s_verylarge[i] >1){
total_500_final$im_s_verylarge[i] <- 1 #they have images of this size
}else{
total_500_final$im_s_verylarge[i] <- 0 #they do not have images of this size
}
}
par(mfrow=c(1,1))
barplot(table(total_500_final$im_s_verylarge),col = "darkred", main="Very Large images 1:NO, 2:YES")
###########################################################################################################
large <- c(23,45,52,65,100,113,118,119,129,137,139,150,159,211,218,232,235,249,265)
total_500_final$im_s_large <- 0
for(i in 1:19){
l <- large[i]
total_500_final$im_s_large <- total_500_final$im_s_large + total_500_final[,l]}
total_500_final$im_s_large <- total_500_final$im_s_large/length(large)
for(i in 1:375){
if (total_500_final$im_s_large[i] >1){
total_500_final$im_s_large[i] <- 1
}else{
total_500_final$im_s_large[i] <- 0 #they do not have images of this size
}
}
par(mfrow=c(1,1))
barplot(table(total_500_final$im_s_large),col = "darkblue", main="Large images 1:NO, 2:YES")
###########################################################################################################
medium <- c(37,67,68,71,85,87,94,96,121,124,154,180,183,198,207,227,238,269,281,288,290)
total_500_final$im_s_medium <- 0
for(i in 1:21){
m <- medium[i]
total_500_final$im_s_medium <- total_500_final$im_s_medium + total_500_final[,m]}
total_500_final$im_s_medium <- total_500_final$im_s_medium/length(medium)
for(i in 1:375){
if (total_500_final$im_s_medium[i] >1){
total_500_final$im_s_medium[i] <- 1
}else{
total_500_final$im_s_medium[i] <- 0 #they do not have images of this size
}
}
par(mfrow=c(1,1))
barplot(table(total_500_final$im_s_medium),col = "darkgreen", main="Medium images 1:NO, 2:YES")
###########################################################################################################
small <- c(25,26,30,31,32,33,38,39,40,43,44,46,47,51,55,57,60,63,72,73,78,79,88,89,90,95,101,102,103,106,107,109,112,117,120,128,135,143,146,147,148,149,151,152,156,160,161,162,163,164,165,166,167,168,170,171,174,189,190,192,194,196,197,199,201,203,204,206,208,225,226,230,231,233,236,236,237,242,244,246,247,248,252,257,263,264,266,268,271,272,273,275,278,279,282,283,284,289)
total_500_final$im_s_small <- 0
for(i in 1:98){
sl <- small[i]
total_500_final$im_s_small <- total_500_final$im_s_small + total_500_final[,sl]}
total_500_final$im_s_small <- total_500_final$im_s_small/length(small)
for(i in 1:375){
if (total_500_final$im_s_small[i] >1){
total_500_final$im_s_small[i] <- 1
}else{
total_500_final$im_s_small[i] <- 0 #they do not have images of this size
}
}
par(mfrow=c(1,1))
barplot(table(total_500_final$im_s_small),col = "red", main="Small images 1:NO, 2:YES")
###########################################################################################################
thumbnail<- c(22,41,64,84,91,92,93,108,126,127,169,224,240,245,250,27,142,28,144,29,155,35,158,36,172,42,173,175,48,176,49,177,50,178,56,179,58,181,184,59,185,66,186,187,188,77,82,191,86,193,195,104,200,105,202,110,111,205,115,116,223,229,125,239,241,130,132,243,133,134,136,251,253,254,255,256,259,260,261,262,267,270,276)
total_500_final$im_s_thumbnail <- 0
for(i in 1:83){
tl <- thumbnail[i]
total_500_final$im_s_thumbnail <- total_500_final$im_s_thumbnail + total_500_final[,tl]}
total_500_final$im_s_thumbnail <- total_500_final$im_s_thumbnail/length(thumbnail)
for(i in 1:375){
if (total_500_final$im_s_thumbnail[i] >1){
total_500_final$im_s_thumbnail[i] <- 1
}else{
total_500_final$im_s_thumbnail[i] <- 0 #they do not have images of this size
}
}
par(mfrow=c(1,1))
barplot(table(total_500_final$im_s_thumbnail),col = "blue", main="Thumbnail images 1:NO, 2:YES")
###########################################################################################################
```
```{r}
#Now we will substract the sizes variables an keep only the new ones we created
total_500_final <- total_500_final[,-c(22:290)]
#str(total_500_final)
total_500_final$Market_Value <- NULL
total_500_final$Assets <- NULL
total_500_final$Ranking <- NULL #we also remove the variable Ranking and we keep only the Revenues varibale from the Fortune 500 variables as we said in the beggining
total_500_final$total.links <- NULL
total_500_final$twitter <- NULL
total_500_final$Total_SH_Equity <- NULL
#We split the set to training and test set
library(caret)
set.seed(20)
sampling_vector <- createDataPartition(total_500_final$Revenues, p = 0.85, list = FALSE)
total_500_final_train <- total_500_final[sampling_vector,]
total_500_final_test <- total_500_final[-sampling_vector,]
