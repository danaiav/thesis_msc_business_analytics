---
title: "Spinellis"
author: "Danai Avratoglou"
date: "31 Οκτωβρίου 2016"
output: html_document
---

```{r}
#we upload the dataset
total_500 <- read.csv("F:/Dropbox/Dani/Spinellis - Diplwmatiki/Jupyter markdown/total_500.csv", sep=";", na.strings="n/a")
#total_500 <- read.csv("F:/Dropbox/Dani/Spinellis - Diplwmatiki/Jupyter markdown/total_500.csv", sep=";")
#we see how many observations and how many variables we have and then the names of the variables we have
dim(total_500)
names(total_500)
str(total_500)
total_500.sub <- total_500
#We make the factors numbers where it is possible
total_500.sub$Assets.. <- gsub(",", ".", total_500.sub$Assets.. )
total_500.sub$Market.value.. <- gsub(",", ".", total_500.sub$Market.value.. )
total_500.sub$Revenues.. <- gsub(",", ".", total_500.sub$Revenues.. )
total_500.sub$Total.Stockholder.Equity.. <- gsub(",", ".", total_500.sub$Total.Stockholder.Equity.. )
num <- c(2,3,4,5,6,9,10,11,13,14,12,24,25,757,758,759,760,761,762,763,764,765,766,767)
for(i in 1:24){
  k <- num[i]
  total_500.sub[,k] <- as.numeric(as.character(total_500.sub[,k]))}  
str(total_500.sub[757:767])
#We omit the nas from the analysis
total_500_final <- na.omit(total_500.sub)
#we remove the extra value X since it is not necessary for the analysis
total_500_final$X <- NULL
str(total_500_final)
#we upload the libraries beneath that we will use in the analysis
library(ggplot2)
library(reshape2)
library(DAAG)
##########################################################################################################
#Firstly we will analyze the social media relevance with the sites.
#We will see how many of the sites have social media and what type of social media
par(mfrow=c(2,3))
#Facebook
social_media_facebook <- round(table(total_500_final$facebook)/439,3)
slicelable <- c(paste(39.86,"% no"),paste(60.13,"% yes"))
pie(social_media_facebook,label = slicelable,main="Share of companies with Facebook",col=rainbow(length(social_media_facebook)))
#Twitter
social_media_twitter <- round(table(total_500_final$twitter)/439,3)
social_media_twitter
slicelable <- c(paste(35.30,"% no"),paste(64.7,"% yes"))
pie(social_media_twitter,label = slicelable,main="Share of companies with Twitter",col=rainbow(length(social_media_twitter)))
#Instagram
social_media_instagram <- round(table(total_500_final$instagram)/439,3)
social_media_instagram
slicelable <- c(paste(80,"% no"),paste(20,"% yes"))
pie(social_media_instagram,label = slicelable,main="Share of companies with Instagram",col=rainbow(length(social_media_instagram)))
#Pinterest
social_media_pinterest <- round(table(total_500_final$pinterest)/439,3)
social_media_pinterest
slicelable <- c(paste(90,"% no"),paste(10,"% yes"))
pie(social_media_pinterest,label = slicelable,main="Share of companies with Pinterest",col=rainbow(length(social_media_pinterest)))
#Youtube
social_media_youtube <- round(table(total_500_final$youtube)/439,3)
social_media_youtube
slicelable <- c(paste(45.3,"% no"),paste(54.7,"% yes"))
pie(social_media_youtube,label = slicelable,main="Share of companies with Youtube",col=rainbow(length(social_media_youtube)))
#LinkedIn
social_media_linkedin <- round(table(total_500_final$linkedin)/439,3)
social_media_linkedin
slicelable <- c(paste(47.6,"% no"),paste(52.4,"% yes"))
pie(social_media_linkedin,label = slicelable,main="Share of companies with Linkedin",col=rainbow(length(social_media_linkedin)))

#########################################################################################################
#We will now check the links by creating an histogram
#Then we create ggplots in order to see in what frequency the links appear
par(mfrow=c(3,1))
library(ggplot2)
ggplot(data=total_500_final,aes(x=total.links))+geom_histogram(binwidth=50, colour = "darkblue", fill ="blue")
ggplot(data=total_500_final,aes(x=external))+geom_histogram(binwidth=50, colour = "darkred", fill ="red")
ggplot(data=total_500_final,aes(x=internal))+geom_histogram(binwidth=50, colour = "darkgreen", fill ="green")
#########################################################################################################
#Now we will see the loading time per site
ggplot(data=total_500_final,aes(x=loading.time))+geom_histogram()
#########################################################################################################
#Now we will see the words in total and fo unique count
ggplot(data=total_500_final,aes(x=total_words))+geom_histogram()
ggplot(data=total_500_final,aes(x=unique_words))+geom_histogram()
#########################################################################################################
#Now we will see the readability of the sites
par(mfrow=c(1,1))
readability <- table(total_500_final$Readability)
plot(readability,col = "DARKRED")
#########################################################################################################
ggplot(data=total_500_final,aes(x=total.images))+geom_histogram()
ggplot(data=total_500_final,aes(x=.tif))+geom_histogram()
ggplot(data=total_500_final,aes(x=.bmp))+geom_histogram()
ggplot(data=total_500_final,aes(x=.tiff))+geom_histogram()
#########################################################################################################
names(total_500_final)
  
