---
title: "Spinellis"
author: "Danai Avratoglou"
date: "31 Οκτωβρίου 2016"
output: html_document
---

```{r}
#we upload the dataset
total_500 <- read.csv("~/GitHub/thesis_msc_business_analytics/total_500.csv", sep=";", na.strings="n/a")
#total_500 <- read.csv("F:/Dropbox/Dani/Spinellis - Diplwmatiki/Jupyter markdown/total_500.csv", sep=";")
#we see how many observations and how many variables we have and then the names of the variables we have
dim(total_500)
#names(total_500)
#str(total_500)
total_500_sub <- total_500
#str(total_500_sub[720:741])
#Change the decimal point for the 4 variables
total_500_sub$Assets.. <- gsub(",", ".", total_500_sub$Assets.. )
total_500_sub$Market.value.. <- gsub(",", ".", total_500_sub$Market.value.. )
total_500_sub$Revenues.. <- gsub(",", ".", total_500_sub$Revenues.. )
total_500_sub$Total.Stockholder.Equity.. <- gsub(",", ".", total_500_sub$Total.Stockholder.Equity.. )
#Make the variables numeric
for(i in 1:741){
  total_500_sub[,i] <- as.numeric(total_500_sub[,i])}  
#str(total_500_sub[720:741])
#We omit the nas from the analysis
total_500_final <- na.omit(total_500_sub)
#we remove the extra value X since it is not necessary for the analysis
#str(total_500_final)
#we make the variables from fortune binomial so as to be more easily examined
#In order to achieve that we first see their summary and then we create their histogram so as to have a 
#good grasp of how they are distributed
#we upload the libraries beneath that we will use in the analysis
library(ggplot2)
library(reshape2)
library(DAAG)
summary(total_500_final$Revenues..)
ggplot(data=total_500_final,aes(x=Revenues..))+geom_histogram(binwidth=50, colour = "green", fill ="darkgreen")
total_500_final$Revenues.. <- cut(total_500_final$Revenues..,c(-1,50,483))
summary(total_500_final$Assets..)
ggplot(data=total_500_final,aes(x=Assets..))+geom_histogram(binwidth=100, colour = "red", fill ="darkred")
total_500_final$Assets.. <- cut(total_500_final$Assets..,c(-1,500,1000))
summary(total_500_final$Market.value..)
ggplot(data=total_500_final,aes(x=Market.value..))+geom_histogram(binwidth=100, colour = "blue", fill ="darkblue")
total_500_final$Market.value.. <- cut(total_500_final$Market.value..,c(-1,400,1000))
summary(total_500_final$Total.Stockholder.Equity..)
ggplot(data=total_500_final,aes(x=Total.Stockholder.Equity..))+geom_histogram(binwidth=100, colour = "purple", fill ="pink")
total_500_final$Total.Stockholder.Equity.. <- cut(total_500_final$Total.Stockholder.Equity..,c(-1000,250,1000))
#names(total_500_final)
#######################################################################################################
#names(total_500_final)[1]
#Change the names of some variables to be more easily readable
colnames(total_500_final)[1] <- "Ranking"
colnames(total_500_final)[2] <- "Assets"
colnames(total_500_final)[3] <- "Market_Value"
colnames(total_500_final)[4] <- "Revenues"
colnames(total_500_final)[6] <- "Total_SH_Equity"
total_500_final$Ranking <- cut(total_500_final$Ranking,c(-1,9,500)) #we start from -1 so as to take 0 also
total_500_final$Ranking <- as.numeric(total_500_final$Ranking)
#Delete the variables we will not need
total_500_final[5] <- NULL #Revenues %
total_500_final[6] <- NULL #company name
total_500_final[21] <- NULL # company url
total_500_final[20] <- NULL # readability index
#names(total_500_final)
##########################################################################################################
#Firstly we will analyze the social media relevance with the sites.
#We will see how many of the sites have social media and what type of social media
par(mfrow=c(1,2))
#Facebook
social_media_facebook <- round(table(total_500_final$facebook)/439,3)
social_media_facebook
slicelable <- c(paste(38.8,"% no"),paste(61.2,"% yes"))
pie(social_media_facebook,label = slicelable,main="Share of companies with Facebook",col=rainbow(length(social_media_facebook)))
#Twitter
social_media_twitter <- round(table(total_500_final$twitter)/439,3)
social_media_twitter
slicelable <- c(paste(33.7,"% no"),paste(66.3,"% yes"))
pie(social_media_twitter,label = slicelable,main="Share of companies with Twitter",col=rainbow(length(social_media_twitter)))
#Instagram
social_media_instagram <- round(table(total_500_final$instagram)/439,3)
social_media_instagram
slicelable <- c(paste(79.1,"% no"),paste(20.9,"% yes"))
pie(social_media_instagram,label = slicelable,main="Share of companies with Instagram",col=rainbow(length(social_media_instagram)))
#Pinterest
social_media_pinterest <- round(table(total_500_final$pinterest)/439,3)
social_media_pinterest
slicelable <- c(paste(90.4,"% no"),paste(9.6,"% yes"))
pie(social_media_pinterest,label = slicelable,main="Share of companies with Pinterest",col=rainbow(length(social_media_pinterest)))
#Youtube
social_media_youtube <- round(table(total_500_final$youtube)/439,3)
social_media_youtube
slicelable <- c(paste(44.3,"% no"),paste(55.7,"% yes"))
pie(social_media_youtube,label = slicelable,main="Share of companies with Youtube",col=rainbow(length(social_media_youtube)))
#LinkedIn
social_media_linkedin <- round(table(total_500_final$linkedin)/439,3)
social_media_linkedin
slicelable <- c(paste(45.4,"% no"),paste(54.6,"% yes"))
pie(social_media_linkedin,label = slicelable,main="Share of companies with Linkedin",col=rainbow(length(social_media_linkedin)))
#And we can also see for correlations
total_500_social_media <- total_500_final[,c(1,13:18)]
par(mfrow=c(1,1))
library(corrplot)
library(caret)
sm <- cor(total_500_social_media)
sm
corrplot(cor(total_500_social_media),method="number")
#The most high correlation is between facebook and twitter 69%
#While the second highest is between twitter and linkedIn 59%
#########################################################################################################
```

```{r}
#We will now check the links by creating an histogram
#Then we create ggplots in order to see in what frequency the links appear
par(mfrow=c(1,1))
library(ggplot2)
ggplot(data=total_500_final,aes(x=total.links))+geom_histogram(binwidth=50, colour = "darkblue", fill ="blue")
ggplot(data=total_500_final,aes(x=external))+geom_histogram(binwidth=50, colour = "darkred", fill ="red")
ggplot(data=total_500_final,aes(x=internal))+geom_histogram(binwidth=50, colour = "darkgreen", fill ="green")
#And we can also see for correlations
total_500_links <- total_500_final[,c(1,10:12)]
library(corrplot)
library(caret)
tl <- cor(total_500_links)
tl
corrplot(cor(total_500_links),method="number")
#########################################################################################################
#Now we will see the loading time per site
ggplot(data=total_500_final,aes(x=loading.time))+geom_histogram(binwidth=1, colour = "pink", fill ="purple")
#########################################################################################################
#Now we will see the words in total and in unique count in relation with the readability index
ggplot(data=total_500_final,aes(x=total_words,fill=Readability))+geom_histogram(binwidth=50, colour = "darkred", fill ="red")
ggplot(data=total_500_final,aes(x=unique_words, fill=Readability))+geom_histogram(binwidth=50, colour = "darkred", fill ="red")
#########################################################################################################
ggplot(data=total_500_final,aes(x=total.images))+geom_histogram(binwidth=100, colour = "darkred", fill ="red")
#########################################################################################################
#We will see now the frequency of image types that is being used
par(mfrow=c(2,2))
k = c(727:735)
for(i in 1:9){
  a <- k[i]
  image_type<- round(table(total_500_final[,a])/439,3)
  barplot(image_type,xlab=names(total_500_final)[a],ylab = "Shares of images per site", col = "dark green")}
#It is obvious that the most common images type are .jpg,.png and .gif
#So they will be the ones that we will keep
#Delete the variables we will not need
total_500_final[727] <- NULL #.bmp
total_500_final[727] <- NULL #.dib
total_500_final[728] <- NULL # .jpe
total_500_final[728] <- NULL # .jpeg
total_500_final[730] <- NULL # .tif
total_500_final[730] <- NULL # .tiff
#names(total_500_final)
par(mfrow=c(1,1))
##########################################################################################################
```

```{r}
#Now we will check the sizes of the images used
#2 means YES and 1 means NO
par(mfrow=c(2,2))
ks = c(22:726)
for(i in 1:705){
  a <- ks[i]
  image_type<- round(table(total_500_final[,a])/439,3)
  barplot(image_type,xlab=names(total_500_final)[a],ylab = "Shares of images per site", col = "dark red")}
#Firstly we will keep the image sizes that exist in more than the half od the sites that we are examining
#So we will keep until the size x210x420 [304]
#names(total_500_final)
total_500_final <- total_500_final[,-c(305:726)]
#names(total_500_final)
#We will also subtrack the sizes that are not clear
#x1x1 [24], x11x8 [42], x [44], x1x700 [50], X1x10 [53], "X10x1" [54],"X1x660" [56],"X19x1"[57], 2x2 [60],  x0x0 [74],"X1x110"[208], "autox100." [249],"autox200" [250], "X2x213" [255],
total_500_final <- total_500_final[,-c(24,42,44,50,53,54,56,57,60,74,208,249,250,255)]
#names(total_500_final)
#We still have many bariables in order to make a regression model
#So we will group the sizes based  on the following 5 categories so as to have a more calable information
#If at least one of the dimensions belongs to a category we choose the higher category that a dimension belongs
#Very large size: more than 800pixels
#Large: 500 - 799 pixels
#Medium: 300 - 499 pixels
#Small: 100 - 299 pixels
#Thumbnail: less than 100 pixels
############################################################################################################
verylarge <- c(24,34,53,54,61,62,69,70,74,75,76,80,81,83,97,98,99,114,122,123,131,138,140,141,145,153,157,182,209,210,212,213,214,215,216,217,219,220,221,222,228,234,258,274,277,280,285,286,287)
total_500_final$im_s_verylarge <- 0
k<-0
for(i in 1:49){
  k <- verylarge[i]
  for(i in 1:439){
    total_500_final$im_s_verylarge[i] <- total_500_final$im_s_verylarge[i] + total_500_final[i,k]
  }}
total_500_final$im_s_verylarge <- total_500_final$im_s_verylarge/length(verylarge)
for(i in 1:439){
  if (total_500_final$im_s_verylarge[i] >1){
    total_500_final$im_s_verylarge[i] <- 2 #they have images of this size
  }else{
    total_500_final$im_s_verylarge[i] <- 1 #they do not have images of this size
  }
}
par(mfrow=c(1,1))
barplot(table(total_500_final$im_s_verylarge),col = "darkred")
```

```{r}
###########################################################################################################
large <- c(23,45,52,65,100,113,118,119,129,137,139,150,159,211,218,232,235,249,265)
total_500_final$im_s_large <- 0
for(i in 1:19){
  l <- large[i]
  total_500_final$im_s_large <- total_500_final$im_s_large + total_500_final[,l]}
total_500_final$im_s_large <- total_500_final$im_s_large/length(large)
for(i in 1:439){
  if (total_500_final$im_s_large[i] >1){
    total_500_final$im_s_large[i] <- 2
  }
}
par(mfrow=c(1,1))
barplot(table(total_500_final$im_s_large),col = "darkblue")
###########################################################################################################
medium <- c(37,67,68,71,85,87,94,96,121,124,154,180,183,198,207,227,238,269,281,288,290)
total_500_final$im_s_medium <- 0
for(i in 1:21){
  m <- medium[i]
  total_500_final$im_s_medium <- total_500_final$im_s_medium + total_500_final[,m]}
total_500_final$im_s_medium <- total_500_final$im_s_medium/length(medium)
for(i in 1:439){
  if (total_500_final$im_s_medium[i] >1){
    total_500_final$im_s_medium[i] <- 2
  }
}
par(mfrow=c(1,1))
barplot(table(total_500_final$im_s_medium),col = "darkgreen")
###########################################################################################################
small <- c(25,26,30,31,32,33,38,39,40,43,44,46,47,51,55,57,60,63,72,73,78,79,88,89,90,95,101,102,103,106,107,109,112,117,120,128,135,143,146,147,148,149,151,152,156,160,161,162,163,164,165,166,167,168,170,171,174,189,190,192,194,196,197,199,201,203,204,206,208,225,226,230,231,233,236,236,237,242,244,246,247,248,252,257,263,264,266,268,271,272,273,275,278,279,282,283,284,289)
total_500_final$im_s_small <- 0
for(i in 1:98){
  sl <- small[i]
  total_500_final$im_s_small <- total_500_final$im_s_small + total_500_final[,sl]}
total_500_final$im_s_small <- total_500_final$im_s_small/length(small)
for(i in 1:439){
  if (total_500_final$im_s_small[i] >1){
    total_500_final$im_s_small[i] <- 2
  }
}
par(mfrow=c(1,1))
barplot(table(total_500_final$im_s_small),col = "red")
###########################################################################################################
thumbnail<- c(22,41,64,84,91,92,93,108,126,127,169,224,240,245,250,27,142,28,144,29,155,35,158,36,172,42,173,175,48,176,49,177,50,178,56,179,58,181,184,59,185,66,186,187,188,77,82,191,86,193,195,104,200,105,202,110,111,205,115,116,223,229,125,239,241,130,132,243,133,134,136,251,253,254,255,256,259,260,261,262,267,270,276)
total_500_final$im_s_thumbnail <- 0
for(i in 1:83){
  tl <- thumbnail[i]
  total_500_final$im_s_thumbnail <- total_500_final$im_s_thumbnail + total_500_final[,tl]}
total_500_final$im_s_thumbnail <- total_500_final$im_s_thumbnail/length(thumbnail)
for(i in 1:439){
  if (total_500_final$im_s_thumbnail[i] >1){
    total_500_final$im_s_thumbnail[i] <- 2
  }
}
par(mfrow=c(1,1))
barplot(table(total_500_final$im_s_thumbnail),col = "blue")
###########################################################################################################
```

```{r}
#Now we will substract the sizes variables an keep only the new ones we created
total_500_final <- total_500_final[,-c(22:290)]
#str(total_500_final)
total_500_final$Market_Value <- as.numeric(total_500_final$Market_Value)
total_500_final$Assets <- as.numeric(total_500_final$Assets)
total_500_final$Revenues <- as.numeric(total_500_final$Revenues)
total_500_final$Total_SH_Equity <- as.numeric(total_500_final$Total_SH_Equity)
#We will try to create a regression model to see which of the variables of the websites play the most important part regarding the Ranking of the company. 
#We create the empty lm model
model_null = lm(Ranking~1,data=total_500_final)
summary(model_null)
#And we create a full model to check which variables influence the ranking
full_model <- lm(Ranking~.,data=total_500_final)
anova(full_model)
#####################################################################################################
#Use of LASSO
library(glmnet)
#for posts_no a first approach (we remove LDA04 since it gives us NA)
full <- lm(Ranking~.,data=total_500_final)
anova(full)
x <- model.matrix(full) [,-1]
dim(x)
lasso <- glmnet (x, total_500_final$Ranking)
par(mfrow=c(1,1),no.readonly = TRUE)
plot(lasso,label=T)
plot(lasso, xvar='lambda', label=T)
lassob <- cv.glmnet(x,total_500_final$Ranking)
lassob$lambda.min
lassob$lambda.1se
plot(lassob)
#coefiecinets for lammda min with the min CV - MSE for posts3
blasso <- coef(lassob, s="lambda.min")
blasso
dim(blasso)
zblasso <- blasso[-1] * apply(x,2,sd)
zbolt <- coef (full) [-1] * apply (x,2,sd)
azbolt <- abs(zbolt)
sum(azbolt)
#since the sum is NA that means we have to substract some variables
# in order to find which variables to substract we run the coefficients and we see which of them has NA as result
coef(full)
#Now we create a new model with only the variables with coef different from NA
total_500_final_r <- total_500_final[,-c(6,12)]
full_2 <- lm(Ranking~.,data=total_500_final_r)
anova(full_2)
x <- model.matrix(full_2) [,-1]
dim(x)
lasso <- glmnet (x, total_500_final_r$Ranking)
par(mfrow=c(1,1),no.readonly = TRUE)
plot(lasso,label=T)
plot(lasso, xvar='lambda', label=T)
lassob <- cv.glmnet(x,total_500_final_r$Ranking)
lassob$lambda.min
lassob$lambda.1se
plot(lassob)
#coefiecinets for lammda min with the min CV - MSE for posts3
blasso <- coef(lassob, s="lambda.min")
blasso
dim(blasso)
zblasso <- blasso[-1] * apply(x,2,sd)
zbolt <- coef (full_2) [-1] * apply (x,2,sd)
azbolt <- abs(zbolt)
sum(azbolt)
s <- sum(abs(zblasso))/sum(abs(azbolt))
s
blassob <- coef(lassob, s="lambda.1se")
blassob
zblassob <- blassob[-1] * apply(x,2,sd)
zboltb <- coef (full_2) [-1] * apply (x,2,sd)
s <- sum(abs(zblassob))/sum(abs(zboltb))
s
#We use the forward method to compare the full model woth the null model to see how many variables are indeed important
model_a <- step(model_null, scope = list(lower = model_null, upper=full_2), direction = "forward")
summary(model_a)
ad_r_sq_ma <- summary(model_a)$adj.r.squared
ad_r_sq_ma
aic_ma <- AIC(model_a)
aic_ma
par(mfrow=c(3,1))
plot(model_a,which=1:3)
1
2
3
#We create the intevals of the model
confint(model_a)
#From this model we can conlude that for the Ranking the variables that play the most important role are
#whether or not they use small images, medium images, the number of external links, whether or not they have pinterest, the number of warnings in the html code and the number of total images
######################################################################################################
```

```{r}
#Next we will try to see which variables play more important roll for the market value of a company
#We create the empty lm model
MV_model_null = lm(Market_Value~1,data=total_500_final)
summary(MV_model_null)
#And we create a full model to check which variables influence the Market_Value
MV_full_model <- lm(Market_Value~.,data=total_500_final)
anova(MV_full_model)
#####################################################################################################
#Use of LASSO
library(glmnet)
#for posts_no a first approach (we remove LDA04 since it gives us NA)
MV_full <- lm(Market_Value~.,data=total_500_final)
anova(MV_full)
MV_x <- model.matrix(MV_full) [,-1]
dim(MV_x)
MV_lasso <- glmnet (MV_x, total_500_final$Market_Value)
par(mfrow=c(1,1),no.readonly = TRUE)
plot(MV_lasso,label=T)
plot(MV_lasso, xvar='lambda', label=T)
MV_lassob <- cv.glmnet(MV_x,total_500_final$Market_Value)
MV_lassob$lambda.min
MV_lassob$lambda.1se
plot(MV_lassob)
#coefiecinets for lammda min with the min CV - MSE for posts3
MV_blasso <- coef(MV_lassob, s="lambda.min")
MV_blasso
dim(MV_blasso)
MV_zblasso <- MV_blasso[-1] * apply(MV_x,2,sd)
MV_zbolt <- coef (MV_full) [-1] * apply (MV_x,2,sd)
MV_azbolt <- abs(MV_zbolt)
sum(MV_azbolt)
#since the sum is NA that means we have to substract some variables
# in order to find which variables to substract we run the coefficients and we see which of them has NA as result
coef(MV_full)
#Now we create a new model with only the variables with coef different from NA
MV_total_500_final <- total_500_final[,-c(6,12)]
MV_full_2 <- lm(Market_Value~.,data=MV_total_500_final)
anova(MV_full_2)
MV_x <- model.matrix(MV_full_2) [,-1]
dim(MV_x)
MV_lasso <- glmnet (MV_x, MV_total_500_final$Market_Value)
par(mfrow=c(1,1),no.readonly = TRUE)
plot(MV_lasso,label=T)
plot(MV_lasso, xvar='lambda', label=T)
MV_lassob <- cv.glmnet(MV_x,MV_total_500_final$Market_Value)
MV_lassob$lambda.min
MV_lassob$lambda.1se
plot(MV_lassob)
#coefiecinets for lammda min with the min CV - MSE for posts3
MV_blasso <- coef(MV_lassob, s="lambda.min")
MV_blasso
dim(MV_blasso)
MV_zblasso <- MV_blasso[-1] * apply(MV_x,2,sd)
MV_zbolt <- coef (MV_full_2) [-1] * apply (MV_x,2,sd)
MV_azbolt <- abs(MV_zbolt)
sum(MV_azbolt)
MV_s <- sum(abs(MV_zblasso))/sum(abs(MV_azbolt))
MV_s
MV_blassob <- coef(MV_lassob, s="lambda.1se")
MV_blassob
MV_zblassob <- MV_blassob[-1] * apply(MV_x,2,sd)
MV_zboltb <- coef (MV_full_2) [-1] * apply (MV_x,2,sd)
MV_s <- sum(abs(MV_zblassob))/sum(abs(MV_zboltb))
MV_s
#We use the forward method to compare the full model woth the null model to see how many variables are indeed important
MV_model_a <- step(MV_model_null, scope = list(lower = MV_model_null, upper=MV_full_2), direction = "forward")
summary(MV_model_a)
MV_ad_r_sq_ma <- summary(MV_model_a)$adj.r.squared
MV_ad_r_sq_ma
MV_aic_ma <- AIC(MV_model_a)
MV_aic_ma
par(mfrow=c(3,1))
plot(MV_model_a,which=1:3)
1
2
3
#We create the intevals of the model
confint(MV_model_a)
#From this model we can conlude that for the Market Value the variables that play the most important role are
#whether or not they use thumbnail images,whether or not they have facebook and the total sh equity
######################################################################################################
```

```{r}
#Next we will try to see which variables play more important roll for the Revenue of a company
#We create the empty lm model
RV_model_null = lm(Revenues~1,data=total_500_final)
summary(RV_model_null)
#And we create a full model to check which variables influence the Market_Value
RV_full_model <- lm(Revenues~.,data=total_500_final)
anova(RV_full_model)
#####################################################################################################
#Use of LASSO
library(glmnet)
#for posts_no a first approach (we remove LDA04 since it gives us NA)
RV_full <- lm(Revenues~.,data=total_500_final)
anova(RV_full)
RV_x <- model.matrix(RV_full) [,-1]
dim(RV_x)
RV_lasso <- glmnet (RV_x, total_500_final$Revenues)
par(mfrow=c(1,1),no.readonly = TRUE)
plot(RV_lasso,label=T)
plot(RV_lasso, xvar='lambda', label=T)
RV_lassob <- cv.glmnet(RV_x,total_500_final$Revenues)
RV_lassob$lambda.min
RV_lassob$lambda.1se
plot(RV_lassob)
#coefiecinets for lammda min with the min CV - MSE for posts3
RV_blasso <- coef(RV_lassob, s="lambda.min")
RV_blasso
dim(RV_blasso)
RV_zblasso <- RV_blasso[-1] * apply(RV_x,2,sd)
RV_zbolt <- coef (RV_full) [-1] * apply (RV_x,2,sd)
RV_azbolt <- abs(RV_zbolt)
sum(RV_azbolt)
#since the sum is NA that means we have to substract some variables
# in order to find which variables to substract we run the coefficients and we see which of them has NA as result
coef(RV_full)
#Now we create a new model with only the variables with coef different from NA
RV_total_500_final <- total_500_final[,-c(6,12)]
RV_full_2 <- lm(Revenues~.,data=RV_total_500_final)
anova(RV_full_2)
RV_x <- model.matrix(RV_full_2) [,-1]
dim(RV_x)
RV_lasso <- glmnet (RV_x, RV_total_500_final$Revenues)
par(mfrow=c(1,1),no.readonly = TRUE)
plot(RV_lasso,label=T)
plot(RV_lasso, xvar='lambda', label=T)
RV_lassob <- cv.glmnet(RV_x,RV_total_500_final$Revenues)
RV_lassob$lambda.min
RV_lassob$lambda.1se
plot(RV_lassob)
#coefiecinets for lammda min with the min CV - MSE for posts3
RV_blasso <- coef(RV_lassob, s="lambda.min")
RV_blasso
dim(RV_blasso)
RV_zblasso <- RV_blasso[-1] * apply(RV_x,2,sd)
RV_zbolt <- coef (RV_full_2) [-1] * apply (RV_x,2,sd)
RV_azbolt <- abs(RV_zbolt)
sum(RV_azbolt)
RV_s <- sum(abs(RV_zblasso))/sum(abs(RV_azbolt))
RV_s
RV_blassob <- coef(RV_lassob, s="lambda.1se")
RV_blassob
RV_zblassob <- RV_blassob[-1] * apply(RV_x,2,sd)
RV_zboltb <- coef (RV_full_2) [-1] * apply (RV_x,2,sd)
RV_s <- sum(abs(RV_zblassob))/sum(abs(RV_zboltb))
RV_s
#We use the forward method to compare the full model woth the null model to see how many variables are indeed important
RV_model_a <- step(RV_model_null, scope = list(lower = RV_model_null, upper=RV_full_2), direction = "forward")
summary(RV_model_a)
RV_ad_r_sq_ma <- summary(RV_model_a)$adj.r.squared
RV_ad_r_sq_ma
RV_aic_ma <- AIC(RV_model_a)
RV_aic_ma
par(mfrow=c(3,1))
plot(RV_model_a,which=1:3)
1
2
3
#We create the intevals of the model
confint(RV_model_a)
#From this model we can conlude that for the Revenue the variables that play the most important role are
#whether or not they use medium images,the assets, total images, jpg, total sh equity, png
```

```{r}
#############################################################################################################
#Now that we have a first glimps of the important variables we will check again the variables that can affect the Ranking but we will begin by keeping only the ones that were dimmed important in the previous models but without any other information from the fortune 500 just to see the site variables
#names(total_500_final)
total_500_final_after_reg <- total_500_final[,c(1,9,10,13,16,23,24,25,29,30,31)]
par(mfrow=c(1,1))
corrplot(cor(total_500_final_after_reg),method="number")
model_after <- lm(Ranking~.,data=total_500_final_after_reg)
summary(model_after)
#From this model we can confer that the most important variables for a sites ranking good in fortune 500
#Are the number of external links
#Whether they have or not pinterest
#whether they use medium and small images
#And also the number of warning and the total images that the site has
```

