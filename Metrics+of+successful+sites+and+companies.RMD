---
title: "Spinellis"
author: "Danai Avratoglou"
date: "31 Οκτωβρίου 2016"
output: html_document
---

```{r}
#we upload the dataset
total_500 <- read.csv("~/GitHub/thesis_msc_business_analytics/total_500.csv", sep=";", na.strings="n/a")
#total_500 <- read.csv("F:/Dropbox/Dani/Spinellis - Diplwmatiki/Jupyter markdown/total_500.csv", sep=";")
#we see how many observations and how many variables we have and then the names of the variables we have
dim(total_500)
names(total_500)
str(total_500)
total_500_sub <- total_500
str(total_500_sub[720:741])
#Change the decimal point for the 4 variables
total_500_sub$Assets.. <- gsub(",", ".", total_500_sub$Assets.. )
total_500_sub$Market.value.. <- gsub(",", ".", total_500_sub$Market.value.. )
total_500_sub$Revenues.. <- gsub(",", ".", total_500_sub$Revenues.. )
total_500_sub$Total.Stockholder.Equity.. <- gsub(",", ".", total_500_sub$Total.Stockholder.Equity.. )
#Make the variables numeric
for(i in 1:741){
  total_500_sub[,i] <- as.numeric(total_500_sub[,i])}  
str(total_500_sub[720:741])
#We omit the nas from the analysis
total_500_final <- na.omit(total_500_sub)
#we remove the extra value X since it is not necessary for the analysis
#total_500_final$X <- NULL
str(total_500_final)
#we make the variables from fortune binomial so as to be more easily examined
#In order to achieve that we first see their summary and then we create their histogram so as to have a 
#good grasp of how they are distributed
#we upload the libraries beneath that we will use in the analysis
library(ggplot2)
library(reshape2)
library(DAAG)
summary(total_500_final$Revenues..)
ggplot(data=total_500_final,aes(x=Revenues..))+geom_histogram(binwidth=50)
total_500_final$Revenues.. <- cut(total_500_final$Revenues..,c(-1,50,483))
summary(total_500_final$Assets..)
ggplot(data=total_500_final,aes(x=Assets..))+geom_histogram(binwidth=100)
total_500_final$Assets.. <- cut(total_500_final$Assets..,c(-1,500,1000))
summary(total_500_final$Market.value..)
ggplot(data=total_500_final,aes(x=Market.value..))+geom_histogram(binwidth=100)
total_500_final$Market.value.. <- cut(total_500_final$Market.value..,c(-1,400,1000))
summary(total_500_final$Total.Stockholder.Equity..)
ggplot(data=total_500_final,aes(x=Total.Stockholder.Equity..))+geom_histogram(binwidth=100)
total_500_final$Total.Stockholder.Equity.. <- cut(total_500_final$Total.Stockholder.Equity..,c(-1000,250,1000))
names(total_500_final)
#######################################################################################################
names(total_500_final)[1]
#Change the names of some variables to be more easily readable
colnames(total_500_final)[1] <- "Ranking"
colnames(total_500_final)[2] <- "Assets"
colnames(total_500_final)[3] <- "Market_Value"
colnames(total_500_final)[4] <- "Revenues"
colnames(total_500_final)[6] <- "Total_SH_Equity"
total_500_final$Ranking <- cut(total_500_final$Ranking,c(-1,9,500)) #we start from -1 so as to take 0 also
total_500_final$Ranking <- as.numeric(total_500_final$Ranking)
#Delete the variables we will not need
total_500_final[5] <- NULL #Revenues %
total_500_final[6] <- NULL #company name
total_500_final[21] <- NULL # company url
total_500_final[20] <- NULL # readability index
names(total_500_final)
##########################################################################################################
#Firstly we will analyze the social media relevance with the sites.
#We will see how many of the sites have social media and what type of social media
par(mfrow=c(2,3))
#Facebook
social_media_facebook <- round(table(total_500_final$facebook)/436,3)
social_media_facebook
slicelable <- c(paste(38.8,"% no"),paste(61.2,"% yes"))
pie(social_media_facebook,label = slicelable,main="Share of companies with Facebook",col=rainbow(length(social_media_facebook)))
#Twitter
social_media_twitter <- round(table(total_500_final$twitter)/436,3)
social_media_twitter
slicelable <- c(paste(33.7,"% no"),paste(66.3,"% yes"))
pie(social_media_twitter,label = slicelable,main="Share of companies with Twitter",col=rainbow(length(social_media_twitter)))
#Instagram
social_media_instagram <- round(table(total_500_final$instagram)/436,3)
social_media_instagram
slicelable <- c(paste(79.1,"% no"),paste(20.9,"% yes"))
pie(social_media_instagram,label = slicelable,main="Share of companies with Instagram",col=rainbow(length(social_media_instagram)))
#Pinterest
social_media_pinterest <- round(table(total_500_final$pinterest)/436,3)
social_media_pinterest
slicelable <- c(paste(90.4,"% no"),paste(9.6,"% yes"))
pie(social_media_pinterest,label = slicelable,main="Share of companies with Pinterest",col=rainbow(length(social_media_pinterest)))
#Youtube
social_media_youtube <- round(table(total_500_final$youtube)/436,3)
social_media_youtube
slicelable <- c(paste(44.3,"% no"),paste(55.7,"% yes"))
pie(social_media_youtube,label = slicelable,main="Share of companies with Youtube",col=rainbow(length(social_media_youtube)))
#LinkedIn
social_media_linkedin <- round(table(total_500_final$linkedin)/436,3)
social_media_linkedin
slicelable <- c(paste(45.4,"% no"),paste(54.6,"% yes"))
pie(social_media_linkedin,label = slicelable,main="Share of companies with Linkedin",col=rainbow(length(social_media_linkedin)))
#And we can also see for correlations
total_500_social_media <- total_500_final[,c(1,13:18)]
par(mfrow=c(1,1))
library(corrplot)
library(caret)
sm <- cor(total_500_social_media)
sm
corrplot(cor(total_500_social_media),method="number")
#The most high correlation is between facebook and twitter 68%
#While the second highest is between twitter and linkedIn
#########################################################################################################
#We will now check the links by creating an histogram
#Then we create ggplots in order to see in what frequency the links appear
par(mfrow=c(3,1))
library(ggplot2)
ggplot(data=total_500_final,aes(x=total.links))+geom_histogram(binwidth=50, colour = "darkblue", fill ="blue")
ggplot(data=total_500_final,aes(x=external))+geom_histogram(binwidth=50, colour = "darkred", fill ="red")
ggplot(data=total_500_final,aes(x=internal))+geom_histogram(binwidth=50, colour = "darkgreen", fill ="green")
par(mfrow=c(1,1))
#And we can also see for correlations
total_500_links <- total_500_final[,c(1,10:12)]
par(mfrow=c(1,1))
library(corrplot)
library(caret)
tl <- cor(total_500_links)
tl
corrplot(cor(total_500_links),method="number")
#########################################################################################################
#Now we will see the loading time per site
ggplot(data=total_500_final,aes(x=loading.time))+geom_histogram(binwidth=1, colour = "pink", fill ="purple")
#########################################################################################################
#Now we will see the words in total and in unique count in relation with the readability index
ggplot(data=total_500_final,aes(x=total_words,fill=Readability))+geom_histogram(binwidth=50)
ggplot(data=total_500_final,aes(x=unique_words, fill=Readability))+geom_histogram(binwidth=50)
#########################################################################################################
ggplot(data=total_500_final,aes(x=total.images))+geom_histogram(binwidth=100)
#########################################################################################################
#We will see now the frequency of image types that is being used
par(mfrow=c(3,3))
k = c(727:735)
for(i in 1:9){
  a <- k[i]
  image_type<- round(table(total_500_final[,a])/436,3)
  barplot(image_type,xlab=names(total_500_final)[a],ylab = "Shares of images per site", col = "dark green")}
#It is obvious that the most common images type are .jpg,.png and .gif
#So they will be the ones that we will keep
#Delete the variables we will not need
total_500_final[727] <- NULL #.bmp
total_500_final[727] <- NULL #.dib
total_500_final[728] <- NULL # .jpe
total_500_final[728] <- NULL # .jpeg
total_500_final[730] <- NULL # .tif
total_500_final[730] <- NULL # .tiff
names(total_500_final)
par(mfrow=c(1,1))
##########################################################################################################
#Now we will check the sizes of the images used
#2 means YES and 1 means NO
total_500_final$num_companies <- c(1:436)
par(mfrow=c(3,3))
ks = c(22:726)
for(i in 1:706){
  a <- ks[i]
  image_type<- round(table(total_500_final[,a])/436,3)
  barplot(image_type,xlab=names(total_500_final)[a],ylab = "Shares of images per site", col = "dark red")}
#Firstly we will keep the image sizes that exist in more than the half od the sites that we are examining
#So we will keep until the size x210x420 [304]
names(total_500_final)
total_500_final <- total_500_final[,-c(305:726)]
names(total_500_final)
#We will also subtrack the sizes that are not clear
#x1x1 [24], x11x8 [42], x [44], x1x700 [50], X1x10 [53], "X10x1" [54],"X1x660" [56],"X19x1"[57], 2x2 [60],  x0x0 [74],"X1x110"[208], "autox100." [249],"autox200" [250], "X2x213" [255],
total_500_final <- total_500_final[,-c(24,42,44,50,53,54,56,57,60,74,208,249,250,255)]
names(total_500_final)
#We still have many bariables in order to make a regression model
#So we will group the sizes based  on the following 6 categories so as to have a more calable information
#If at least one of the dimensions belongs to a category we choose the higher category that a dimension belongs
#Very large size: more than 800pixels
#Large: 500 - 799 pixels
#Medium: 300 - 499 pixels
#Small: 100 - 299 pixels
#Thumbnail: 75 - 100 pixels
#Thumb: less than 75
verylarge <- c(24,34,53,54,61,62,69,70,74,75,76,80,81,83,97,98,99,114,122,123,131,138,140,141,145,153,157,182,209,210,212,213,214,215,216,217,219,220,221,222,228,234,258,274,277,280,285,286,287)
total_500_final$im_s_verylarge <- 0
for(i in 1:49){
  vl <- verylarge[i]
  total_500_final$im_s_verylarge <- total_500_final$im_s_verylarge + total_500_final[,vl]}
total_500_final$im_s_verylarge <- total_500_final$im_s_verylarge/length(verylarge)
par(mfrow=c(1,1))
plot(total_500_final$im_s_verylarge)
###########################################################################################################

#We will try to create a regression model to see which of the variables of the websites play the most important part regarding the total reveneue of the company. 
#We create the empty lm model
model_null = lm(Revenues..~1,data=total_500_final)
summary(model_null)
#And we create a full model
total_500_final$Rank <- total_500_final$X +1
total_500_final_for_regression <- total_500_final
total_500_final_for_regression$X <- NULL
total_500_final_for_regression$The_page_opened <- NULL
total_500_final_for_regression$company <- NULL
total_500_final_for_regression$url <- NULL
total_500_final_for_regression$num_companies <- NULL
total_500_final_for_regression$Assets.. <- NULL
total_500_final_for_regression$Market.value.. <- NULL
total_500_final_for_regression$Total.Stockholder.Equity.. <- NULL
total_500_final_for_regression$Revenues...1 <- NULL
full_model <- lm(Revenues..~.,data=total_500_final_for_regression)
anova(full_model)
#############
#Use of LASSO
library(glmnet)
#for posts_no a first approach (we remove LDA04 since it gives us NA)
full <- lm(Revenues..~.,data=total_500_final_for_regression)
anova(full)
x <- model.matrix(full) [,-1]
dim(x)
lasso <- glmnet (x, total_500_final_for_regression$Revenues..)
par(mfrow=c(1,1),no.readonly = TRUE)
plot(lasso,label=T)
plot(lasso, xvar='lambda', label=T)
lassob <- cv.glmnet(x,total_500_final_for_regression$Revenues..)
lassob$lambda.min
lassob$lambda.1se
plot(lassob)
#coefiecinets for lammda min with the min CV - MSE for posts3
blasso <- coef(lassob, s="lambda.min")
blasso
dim(blasso)
zblasso <- blasso[-1] * apply(x,2,sd)
zbolt <- coef (full) [-1] * apply (x,2,sd)
azbolt <- abs(zbolt)
sum(azbolt)
#since the sum is NA that means we have to substract some variables
# in order to find which variables to substract we run the coefficients and we see which of them has NA as result
coef(full)
#Now we create a new model with only the variables with coef different from NA
total_500_final_for_regression_sub <- total_500_final_for_regression[,-c(7,15,20,23,28:31,34,37:41,44:47,50:53,58,60,62,65,66,68,74:76,78:81,83,86,87,89,90,95:100,103:107,109,112:119,121,122,124,125,126,128,129,130,133,137:142,144,147:149,152,156:168,170:177,179,180,182:185,187,189,190,192:204,206,208,209,211,212,215:218,220:230,232,2326,237,240:242,244,247,248,250,252:268,270:272,276:280,282,283,286:290,292,295,297,298,300,302,304,306:308,310:316,318,319,322,324,326,328:333,335,337:339,341,343:345,347:349,351,354,356:364,366:368,370:383,385:388,391,392,394:398,400:414,416:420,422:424,426,427,430,431,433:440,442:445,447,449:452,456:472,474,476:478,480,481,485,487:494,496,497,499,501:505,507,509:516,519,522:524,526,527,532,533,535:592,594:606,609:612,614:616,620:626,628:630,632:641,644,645,646,648,649,651,653,654,656,659,661:664,667,668,670:674,676,678:683,688,689,691:694,696,698:700,702,703,705:713,718:722,724,734,736)]
full_2 <- lm(Revenues..~.,data=total_500_final_for_regression_sub)
anova(full_2)
x <- model.matrix(full_2) [,-1]
dim(x)
lasso <- glmnet (x, total_500_final_for_regression_sub$Revenues..)
par(mfrow=c(1,1),no.readonly = TRUE)
plot(lasso,label=T)
plot(lasso, xvar='lambda', label=T)
lassob <- cv.glmnet(x,total_500_final_for_regression_sub$Revenues..)
lassob$lambda.min
lassob$lambda.1se
plot(lassob)
#coefiecinets for lammda min with the min CV - MSE for posts3
blasso <- coef(lassob, s="lambda.min")
blasso
dim(blasso)
zblasso <- blasso[-1] * apply(x,2,sd)
zbolt <- coef (full_2) [-1] * apply (x,2,sd)
azbolt <- abs(zbolt)
sum(azbolt)
#we do the same thing here sinze again the sum is NA
coef (full_2)
names(total_500_final_for_regression_sub)
total_500_final_for_regression_sub_2 <- total_500_final_for_regression_sub[,-c(86,91,92,94,97,103,106,108,118,129,135,142,149,155,165,169,170,174,177,178,182,185,189,191,194,197,198,201,206,207)]
full_3 <- lm(Revenues..~.,data=total_500_final_for_regression_sub_2)
anova(full_3)
x <- model.matrix(full_3) [,-1]
dim(x)
lasso <- glmnet (x, total_500_final_for_regression_sub_2$Revenues..)
par(mfrow=c(1,1),no.readonly = TRUE)
plot(lasso,label=T)
plot(lasso, xvar='lambda', label=T)
lassob <- cv.glmnet(x,total_500_final_for_regression_sub_2$Revenues..)
lassob$lambda.min
lassob$lambda.1se
plot(lassob)
#coefiecinets for lammda min with the min CV - MSE for posts3
blasso <- coef(lassob, s="lambda.min")
blasso
dim(blasso)
zblasso <- blasso[-1] * apply(x,2,sd)
zbolt <- coef (full_3) [-1] * apply (x,2,sd)
azbolt <- abs(zbolt)
sum(azbolt)
s <- sum(abs(zblasso))/sum(abs(zbolt))
s
blassob <- coef(lassob, s="lambda.1se")
blassob
zblassob <- blassob[-1] * apply(x,2,sd)
zboltb <- coef (full_3) [-1] * apply (x,2,sd)
s <- sum(abs(zblassob))/sum(abs(zboltb))
s
#We use the forward method to compare the full model woth the null model to see how many variables are indeed important
model_a <- step(model_null, scope = list(lower = model_null, upper=full_3), direction = "forward")
summary(model_a)
ad_r_sq_ma <- summary(model_a)$adj.r.squared
ad_r_sq_ma
# with the forward method we had a cut off to 8 variables but with worst adjusted R2
ma_residuals <- model_a$residuals
ma_fitted_values <- model_a$fitted.values
pa <- qplot(ma_fitted_values,ma_residuals) 
pa <- pa + ggtitle("Residual Plot model a")
pa <- pa + theme(plot.title = element_text(lineheight=.8, face="bold"))
pa <- pa + xlab("Fitted Values")  
pa <- pa + ylab("Residuals")
pa
par(mfrow=c(1,1))
qqnorm(ma_residuals, main = "Normal Q-Q Plot for Revenues")
qqline(ma_residuals)
aic_ma <- AIC(model_a)
aic_ma
par(mfrow=c(3,2))
plot(model_a,which=1:6)
#dhmiourgoume ta diastimata empistosunis twn syntelestwn tou modelou
confint(model_a)
######################################################################################################
#Next we will try to see which variables play more important roll for the market value of a company
library(psych)
for(i in 1:742){
total_500_final[,i] <- as.numeric(total_500_final[,i])}
total_500_final_images <- total_500_final[,-c(1,2,4:23,729:742)]
total_500_final_without_images <- total_500_final[,c(3,6:23,729:742)]
str(total_500_final_without_images)
fit <- principal(total_500_final, nfactors=5)
plot(fit)
library(ggfortify)
#Revenue
autoplot(prcomp(total_500_final), data = total_500_final, colour = 'rev')
#Market Value
summary(total_500_final$Market.value..)

