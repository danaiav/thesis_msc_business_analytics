---
title: "Metrics of Succesful websites and companies"
author: "Danai Avratoglou"
date: "18 February 2017"
output: html_document
---

```{r}
#we upload the dataset
total_500 <- read.csv("~/GitHub/thesis_msc_business_analytics/Python/total_500_new.csv", sep=";", na.strings="n/a")
#we see how many observations and how many variables we have
dim(total_500)
#We create a subset to make some changes to the data
total_500_sub <- total_500
#Change the decimal point for the 4 variables
total_500_sub$Assets.. <- gsub(",", ".", total_500_sub$Assets.. )
total_500_sub$Market.value.. <- gsub(",", ".", total_500_sub$Market.value.. )
total_500_sub$Revenues.. <- gsub(",", ".", total_500_sub$Revenues.. )
total_500_sub$Total.Stockholder.Equity.. <- gsub(",", ".", total_500_sub$Total.Stockholder.Equity.. )
#Make the variables numeric
for(i in 1:18){
 total_500_sub[,i] <- as.numeric(total_500_sub[,i])}  
for(i in 20:730){
 total_500_sub[,i] <- as.numeric(total_500_sub[,i])} 
#We omit the nas from the analysis
total_500_final <- na.omit(total_500_sub)
#We rename variable X as Ranking
colnames(total_500_final)[1] <- "Ranking"
#Change the names of some variables to be more easily readable
colnames(total_500_final)[2] <- "Assets"
colnames(total_500_final)[3] <- "Market_Value"
colnames(total_500_final)[4] <- "Revenues"
colnames(total_500_final)[6] <- "Total_SH_Equity"
#Delete the variables we will not need
total_500_final$Revenues...1 <- NULL #Revenues %
total_500_final$company <- NULL #company name
total_500_final$url<- NULL # company url
#we upload the libraries beneath that we will use in the analysis
library(ggplot2)
library(reshape2)
library(DAAG)
#Final number of observation and variables we will use
dim(total_500_final)
```

```{r}
#######################################################################################################
#we first see the summary of the Fortune variables and then we create their histogram so as to have a 
#good grasp of how they are distributed
ggplot(data=total_500_final,aes(x=Revenues))+geom_histogram(binwidth=50, colour = "green", fill ="darkgreen")
ggplot(data=total_500_final,aes(x=Assets))+geom_histogram(binwidth=100, colour = "red", fill ="darkred")
ggplot(data=total_500_final,aes(x=Market_Value))+geom_histogram(binwidth=100, colour = "blue", fill ="darkblue")
ggplot(data=total_500_final,aes(x=Total_SH_Equity))+geom_histogram(binwidth=100, colour = "purple", fill ="pink")
###############################################################################################
#We make plots to see how the variables we got from Fortune 500 are related with the Ranking
ggplot(total_500_final, aes(Assets,Ranking)) + geom_point(colour = "red")
ggplot(total_500_final, aes(Market_Value, Ranking)) + geom_point(colour = "blue")
ggplot(total_500_final, aes(Total_SH_Equity, Ranking)) + geom_point(colour = "purple")
ggplot(total_500_final, aes(Revenues, Ranking)) + geom_point(colour = "green")
#We can see that the Ranking has a linear relationship with the Revenues so we will use one of those 2 variables to check the relationships with the websites metrics
#In order to have a more clear look we also create a correlation diagram
total_500_fortune <- total_500_final[,c(1:5)]
library(corrplot)
library(caret)
sm <- cor(total_500_fortune)
sm
corrplot(cor(total_500_fortune),method="number")
#From this plot we understand that the Ranking and the Revenues have very high correlation.
##########################################################################################################
```

```{r}
#Firstly we will analyze the social media relevance with the sites.
#We will see how many of the sites have social media and what type of social media
#Facebook
social_media_facebook <- round(table(total_500_final$facebook)/408,3)
social_media_facebook
slicelable <- c(paste(35.3,"% no"),paste(64.7,"% yes"))
pie(social_media_facebook,label = slicelable,main="Share of companies with Facebook",col=rainbow(length(social_media_facebook)))
ggplot(total_500_final, aes(Revenues, facebook)) + geom_point(size=3, colour = "darkblue")
```

```{r}
#Twitter
social_media_twitter <- round(table(total_500_final$twitter)/408,3)
social_media_twitter
slicelable <- c(paste(31.4,"% no"),paste(68.6,"% yes"))
pie(social_media_twitter,label = slicelable,main="Share of companies with Twitter",col=rainbow(length(social_media_twitter)))
ggplot(total_500_final, aes(Revenues, twitter)) + geom_point(size=3, colour = "darkgreen")
```

```{r}
#Instagram
social_media_instagram <- round(table(total_500_final$instagram)/408,3)
social_media_instagram
slicelable <- c(paste(77.7,"% no"),paste(22.3,"% yes"))
pie(social_media_instagram,label = slicelable,main="Share of companies with Instagram",col=rainbow(length(social_media_instagram)))
ggplot(total_500_final, aes(Revenues, instagram)) + geom_point(size=3, colour = "pink")
```

```{r}
#Pinterest
social_media_pinterest <- round(table(total_500_final$pinterest)/408,3)
social_media_pinterest
slicelable <- c(paste(90.2,"% no"),paste(9.8,"% yes"))
pie(social_media_pinterest,label = slicelable,main="Share of companies with Pinterest",col=rainbow(length(social_media_pinterest)))
ggplot(total_500_final, aes(Revenues, pinterest)) + geom_point(size=3, colour = "darkred")
```

```{r}
#Youtube
social_media_youtube <- round(table(total_500_final$youtube)/408,3)
social_media_youtube
slicelable <- c(paste(41.7,"% no"),paste(58.3,"% yes"))
pie(social_media_youtube,label = slicelable,main="Share of companies with Youtube",col=rainbow(length(social_media_youtube)))
ggplot(total_500_final, aes(Revenues, youtube)) + geom_point(size=3, colour = "red")
```

```{r}
#LinkedIn
social_media_linkedin <- round(table(total_500_final$linkedin)/408,3)
social_media_linkedin
slicelable <- c(paste(42.9,"% no"),paste(57.1,"% yes"))
pie(social_media_linkedin,label = slicelable,main="Share of companies with Linkedin",col=rainbow(length(social_media_linkedin)))
ggplot(total_500_final, aes(Revenues, linkedin)) + geom_point(size=3, colour = "blue")
```

```{r}
#And we can also see for correlations
total_500_social_media <- total_500_final[,c(4,10:15)]
library(corrplot)
library(caret)
sm <- cor(total_500_social_media)
sm
corrplot(cor(total_500_social_media),method="number")
#we see that facebook has correlation more than 50% with twitter, youtube and linkedin
#And that the smallest correlations are those of pinterest and instagram
#########################################################################################################
```

```{r}
#We will now check the links by creating an histogram
#Then we create ggplots in order to see in what frequency the links appear
par(mfrow=c(1,1))
library(ggplot2)
ggplot(data=total_500_final,aes(x=total.links))+geom_histogram(binwidth=50, colour = "darkblue", fill ="blue")
ggplot(total_500_final, aes(Revenues, total.links)) + geom_point(size=3, colour = "darkblue")
ggplot(data=total_500_final,aes(x=external))+geom_histogram(binwidth=50, colour = "darkred", fill ="red")
ggplot(total_500_final, aes(Revenues, external)) + geom_point(size=3, colour = "darkred")
ggplot(data=total_500_final,aes(x=internal))+geom_histogram(binwidth=50, colour = "darkgreen", fill ="green")
ggplot(total_500_final, aes(Revenues, internal)) + geom_point(size=3, colour = "darkgreen")
#And we can also see for correlations
total_500_links <- total_500_final[,c(4,21:23)]
library(corrplot)
library(caret)
tl <- cor(total_500_links)
tl
corrplot(cor(total_500_links),method="number")
#We can see that the total links with the internal links have a correlation almost 95%.
#So we will not include the total links in the regression model
```

```{r}
#########################################################################################################
#Now we will see the loading time per site
ggplot(data=total_500_final,aes(x=loading.time))+geom_histogram(binwidth=1, colour = "pink", fill ="purple")
ggplot(total_500_final, aes(Revenues, loading.time)) + geom_point(size=3, colour = "purple")
```

```{r}
#########################################################################################################
#Now we will see the total words, the unique words and the sentences how are distributed alone and in relationhsip with the revenues.
ggplot(data=total_500_final,aes(x=Sentences))+geom_histogram(binwidth=50, colour = "darkred", fill ="red")
ggplot(total_500_final, aes(Revenues, Sentences)) + geom_point(size=3, colour = "purple")
#########################
ggplot(data=total_500_final,aes(x=Unique.words))+geom_histogram(binwidth=50, colour = "darkred", fill ="red")
ggplot(total_500_final, aes(Revenues, Unique.words)) + geom_point(size=3, colour = "purple")
#########################
ggplot(data=total_500_final,aes(x=Words))+geom_histogram(binwidth=50, colour = "darkred", fill ="red")
ggplot(total_500_final, aes(Revenues, Words)) + geom_point(size=3, colour = "purple")
#############################
```
```{r}
#And we can also see for correlations
total_500_lt_w <- total_500_final[,c(4,18:20,727)]
library(corrplot)
library(caret)
tl <- cor(total_500_lt_w)
tl
corrplot(cor(total_500_lt_w),method="number")
```

```{r}
################################

#Next we will check the Flesh Measure alone and in relationship with revenues
ggplot(data=total_500_final,aes(x=Flesh_Mesaure))+geom_histogram(binwidth=50, colour = "darkred", fill ="red")
ggplot(total_500_final, aes(Revenues, Flesh_Mesaure)) + geom_point(size=3, colour = "purple")
############################
```

```{r}
total_500_final$Readability <- gsub("Very easy", "01_VE", total_500_final$Readability )
total_500_final$Readability <- gsub("Easy", "02_E", total_500_final$Readability )
total_500_final$Readability <- gsub("Fairly easy", "03_FE", total_500_final$Readability )
total_500_final$Readability <- gsub("Standard", "04_St", total_500_final$Readability )
total_500_final$Readability <- gsub("Fairly difficult", "05_FD", total_500_final$Readability )
total_500_final$Readability <- gsub("Difficult", "06_D", total_500_final$Readability )
total_500_final$Readability <- gsub("Very Confusing", "07_VC", total_500_final$Readability )
barplot(table(total_500_final$Readability),col ="dark red")
```

```{r}
total_500_final$Readability <- gsub("01_VE","1", total_500_final$Readability )
total_500_final$Readability <- gsub("02_E", "2", total_500_final$Readability )
total_500_final$Readability <- gsub("03_FE", "3", total_500_final$Readability )
total_500_final$Readability <- gsub("04_St", "4", total_500_final$Readability )
total_500_final$Readability <- gsub("05_FD", "5", total_500_final$Readability )
total_500_final$Readability <- gsub("06_D", "6" ,total_500_final$Readability )
total_500_final$Readability <- gsub("07_VC", "7",total_500_final$Readability )
total_500_final$Readability <- as.numeric(total_500_final$Readability )
ggplot(data=total_500_final,aes(x=Readability))+geom_bar(binwidth=1, colour = "darkred", fill ="red")
ggplot(total_500_final, aes(Revenues, Readability)) + geom_point(size=3, colour = "purple")
#And we can also see for correlations
total_500_r <- total_500_final[,c(4,16,17)]
library(corrplot)
library(caret)
tl <- cor(total_500_r)
tl
corrplot(cor(total_500_r),method="number")

```


```{r}
#########################################################################################################
#Now we will see the number of errors and warnings alone and in relationship with the Revenues
ggplot(data=total_500_final,aes(x=number_of_errors))+geom_histogram(binwidth=50, colour = "red")
ggplot(total_500_final, aes(Revenues, number_of_errors)) + geom_point(size=3, colour = "dark red")
ggplot(data=total_500_final,aes(x=number_of_warning))+geom_histogram(binwidth=20, colour = "red")
ggplot(total_500_final, aes(Revenues, number_of_warning)) + geom_point(size=3, colour = "dark blue")
#########################################################################################################
#########################################################################################################
#Now we will see the non.document.error and the page not opened variables alone and in relationship with the Revenues
ggplot(data=total_500_final,aes(x=non.document.error))+geom_histogram(binwidth=1, colour = "red")
ggplot(total_500_final, aes(Revenues, non.document.error)) + geom_point(size=1, colour = "dark red")
ggplot(data=total_500_final,aes(x=The_page_opened))+geom_histogram(binwidth=1, colour = "red")
ggplot(total_500_final, aes(Revenues, The_page_opened)) + geom_point(size=3, colour = "dark blue")
#In the page not opened we can see that the variable has only the price 1 that means that the page opened so there is no point in using it in the analysis as it does not affect the outcome
#########################################################################################################
#And we can also see for correlations
total_500_html <- total_500_final[,c(4,7:9)]
library(corrplot)
library(caret)
tl <- cor(total_500_html)
tl
corrplot(cor(total_500_html),method="number")
```

```{r}
#Now we will see the total images alone and in relationship with the revenues
ggplot(data=total_500_final,aes(x=total.images))+geom_histogram(binwidth=100, colour = "darkred", fill ="red")
```

```{r}
ggplot(total_500_final, aes(Revenues, total.images)) + geom_point(size=3, colour = "dark blue")
```

```{r}
#########################################################################################################
#We will see now the frequency of image types that is being used

par(mfrow=c(1,1))
k = c(717:725)
for(i in 1:9){
  a <- k[i]
  image_type<- round(table(total_500_final[,a])/408,3)
  barplot(image_type,xlab=names(total_500_final)[a],ylab = "Shares of images per site", col = "dark green")}
```

```{r}
#It is obvious that the most common images type are .jpg, gif and .png
#We will check now the types in relationship with the revenues
ggplot(total_500_final, aes(Revenues, .bmp)) + geom_point(size=3, colour = "dark blue")
ggplot(total_500_final, aes(Revenues, .dib)) + geom_point(size=3, colour = "dark blue")
ggplot(total_500_final, aes(Revenues, .gif)) + geom_point(size=3, colour = "dark blue")
ggplot(total_500_final, aes(Revenues, .jpe)) + geom_point(size=3, colour = "dark blue")
ggplot(total_500_final, aes(Revenues, .jpeg)) + geom_point(size=3, colour = "dark blue")
ggplot(total_500_final, aes(Revenues, .jpg)) + geom_point(size=3, colour = "dark blue")
ggplot(total_500_final, aes(Revenues, .png)) + geom_point(size=3, colour = "dark blue")
ggplot(total_500_final, aes(Revenues, .tif)) + geom_point(size=3, colour = "dark blue")
ggplot(total_500_final, aes(Revenues, .tiff)) + geom_point(size=3, colour = "dark blue")
```
```{r}
#And we can also see for correlations
total_500_im<- total_500_final[,c(4,717:726)]
library(corrplot)
library(caret)
tl <- cor(total_500_im)
tl
corrplot(cor(total_500_im),method="number")
```



```{r}
#We will see now the frequency of image sizes that is being used
k = c()
#Check for sizes that are half and half divided in existing and not
for(i in 24:716){
  image_size<- round(table(total_500_final[,i]))
  if ((image_size[[1]]==408)==TRUE){
    k <- union(k, c(i))
  }}
#####################
#Number 24 is all onw price so we want use it
names(total_500_final)[24]
total_500_final$X144x144 <- NULL
```

```{r}
false_not_existing = c()
#Check for sizes that are less than half divided in existing and not
for(i in 24:715){
  image_size<- round(table(total_500_final[,i]))
  if ((image_size[[2]]<204)==TRUE){
    false_not_existing <- union(false_not_existing, c(i))
  }}
########################
```

```{r}
#Now we will take the sizes that exist in less than half the instances and check graphically the deviations between the 408 sites
par(mfrow=c(3,3))
for(i in 1:416){
  a = false_not_existing[i]
  plot(total_500_final[,a],total_500_final$Revenues)
  image_size<- round(table(total_500_final[,a]))
  barplot(image_size,xlab=names(total_500_final)[a],ylab = "Has or not the size", col = "dark green")}
```

```{r}
true_existing = c()
#Check for sizes that are more than half divided in existing and not
for(i in 24:715){
  image_size<- round(table(total_500_final[,i]))
  if ((image_size[[2]]>204)==TRUE){
    true_existing <- union(true_existing, c(i))
  }}
```

```{r}
#Now we will take the sizes that exist in more than half the instances and check graphically the deviations between the 408 sites
par(mfrow=c(3,3))
for(i in 1:276){
  a = true_existing[i]
  image_size<- round(table(total_500_final[,a]))
  plot(total_500_final[,a],total_500_final$Revenues)
  barplot(image_size,xlab=names(total_500_final)[a],ylab = "Has or not the size", col = "dark green")}
```

```{r}
#By checking the above plots we can see that the 24 first sizes do appear to have some differentiation regarding the revenues. While most sites do have those sizes when it comes to the high revienues they do not have them
par(mfrow=c(3,3))
keep = c()
for(i in 1:24){
  a = true_existing[i]
  keep = union (keep, c(a))}
keep
#As we can see they are the variables from 24 to 47 and these are the only sizes we are going to keep for the further analysis
total_500_final <- total_500_final[,-c(48:715)]
```

```{r}
#Also we remove the other Fortune 500 variables since they will interfer in the outcome of the model and we keep only the variable we want to examine the Revenues
total_500_final$Market_Value <- NULL
total_500_final$Assets <- NULL
total_500_final$Ranking <- NULL 
total_500_final$Total_SH_Equity <- NULL
total_500_final$The_page_opened <- NULL
#We split the set to training and test set
library(caret)
set.seed(20)
sampling_vector <- createDataPartition(total_500_final$Revenues, p = 0.85, list = FALSE)
total_500_final_train <- total_500_final[sampling_vector,]
total_500_final_test <- total_500_final[-sampling_vector,]
```

```{r}
#We will try to create a regression model to see which of the variables of the websites play the most important part regarding the Ranking of the company. 
#We create the empty lm model
model_null = lm(Revenues~1,data=total_500_final_train)
summary(model_null)
#####################################################################################################
#LASSO and Logistic Regression models
library(glmnet)
#We create a full model for the variable Ranking
full <- lm(Revenues~.,data=total_500_final_train)
summary(full)
```

```{r}
x <- model.matrix(full) [,-1]
dim(x)
lasso <- glmnet (x, total_500_final_train$Revenues)
par(mfrow=c(1,1),no.readonly = TRUE)
plot(lasso, xvar='lambda', label=T)
lassob <- cv.glmnet(x,total_500_final_train$Revenues)
lassob$lambda.min
lassob$lambda.1se
```

```{r}
plot(lassob)
```

```{r}
#We see the coefficients for lamda min
blasso <- coef(lassob, s="lambda.min")
blasso
dim(blasso)
zblasso <- blasso[-1] * apply(x,2,sd)
zbolt <- coef (full) [-1] * apply (x,2,sd)
azbolt <- abs(zbolt)
sum(azbolt)
#since the sum is NA that means we have to substract some variables
# in order to find which variables to substract we run the coefficients and we see which of them has NA as result
coef(full)
```

```{r}
#Now we create a new model with only the variables with coef different from NA
full_2 <- lm(Revenues~. - total.images - total.links - X1x1 - X21pxx173px - X46x214 - X49x49 - X200pxx200px - X1279pxx984px - X300pxx1500px - X160x233 -  X300x993 - X41x192 - X28x221 - X15x12,data=total_500_final_train)
summary(full_2)
x <- model.matrix(full_2) [,-c(18,22,28,26,27,34,32,33,41,37,38,39,40,52)]
dim(x)
lasso <- glmnet (x, total_500_final_train$Revenues)
par(mfrow=c(1,1),no.readonly = TRUE)
```

```{r}
plot(lasso, xvar='lambda', label=T)
```

```{r}
lassob <- cv.glmnet(x,total_500_final_train$Revenues)
lassob$lambda.min
lassob$lambda.1se
```

```{r}
plot(lassob)
```

```{r}
#coefiecinets for lammda min
blasso <- coef(lassob, s="lambda.min")
blasso
dim(blasso)
zblasso <- blasso[-1] * apply(x,2,sd)
zbolt <- coef (full_2) [-1] * apply (x,2,sd)
azbolt <- abs(zbolt)
sum(azbolt)
s <- sum(abs(zblasso))/sum(abs(azbolt))
s
```

```{r}
full_3 <- lm(Revenues~1 +X8x15  +X44x556 +X800x1200 +X24pxx133px +X50x45 +X400x300 +X60x60  +.bmp +.dib ,data=total_500_final_train)
summary(full_3)
ad_r_sq_f3 <- summary(full_3)$adj.r.squared
aic_f3 <- AIC(full_3)
```
```{r}
plot(full_3,which=1:3)
```

```{r}
##############################################
blassob <- coef(lassob, s="lambda.1se")
blassob
zblassob <- blassob[-1] * apply(x,2,sd)
zboltb <- coef (full_2) [-1] * apply (x,2,sd)
s <- sum(abs(zblassob))/sum(abs(zboltb))
s
#The model based on the lasso method by taking the lambda.1se is the null model only with the intercept
```
```{r}
full_4 <- lm(Revenues~1 +X8x15  +X44x556 +X800x1200 +X24pxx133px +X50x45 +X400x300 +X60x60 ,data=total_500_final_train)
summary(full_4)
ad_r_sq_f4 <- summary(full_4)$adj.r.squared
aic_f4 <- AIC(full_4)
```
```{r}
plot(full_4,which=1:3)
```

```{r}
###############################################
#We use the "both" method to compare the full_3 model with the null model to see how many variables are indeed important
model_a <- step(model_null, scope = list(lower = model_null, upper=full_2), direction = "both")
summary(model_a)
ad_r_sq_ma <- summary(model_a)$adj.r.squared
aic_ma <- AIC(model_a)
#We create the 2 basic plots so as to be able to explain the regression model
```

```{r}
plot(model_a,which=1:3)
```

```{r}
################
#We compare the Adjusted R squares of the models and also the AIC of the models we created to find the best one
ad_r_sq_f3 
ad_r_sq_f4 
ad_r_sq_ma
#The best Adkusted R square is the one in full 3 (the closer to 1 the better)
aic_f3
aic_f4
aic_ma 
#The best AIC and the best Adjusted R square is for model ma
```

```{r}
#######################################################################################################
par(mfrow=c(2,2))
Actual_Revenues<- total_500_final_test$Revenues
plot (Actual_Revenues, col = "blue")
###########################################
predictions_ma <- predict(model_a,total_500_final_test)
plot (predictions_ma, col = "Red",main = "Model a")
#####################################
predictions_full3 <- predict(full_3,total_500_final_test)
plot (predictions_full3, col = "Red",main = "Full_3 model")
#####################################
predictions_full4 <- predict(full_4,total_500_final_test)
plot (predictions_full4, col = "Red",main = "Full_4 model")
#####################################
#From the plots above we can see that the actual Revenues have a more smooth way of leveling up except from the Revenues of the #1 ranking company that are extremely high in relationship with the other sites.
#The prediction model that is more smooth is the model a which has as we said before the best Adjusted R Square and the best AIC price
```

```{r}
par(mfrow=c(1,1))
total_500_final_reg <- total_500_final_train[,c(1,6,12,20,21,25,30,42,43,47,53)]
corrplot(cor(total_500_final_reg),method="number")
#We can see here that the variable x8x15 has a very high correlation with the variable x44x556 and also the variable x24pxx133px has also a very high correlation with the variable x400x300.
```

```{r}
#So we can try creating a new model excluding the 2 variables that are correlated from each pair to see if there will be any improvement in the model
full_5 <- lm(Revenues~1 +X60x60 +X44x556 +X400x300 + .bmp +loading.time + .jpeg + Readability + instagram ,data=total_500_final_train)
summary(full_5)
adj_r_square_full5 <- summary(full_5)$adj.r.squared
aic_full5 <- AIC(full_5)
```

```{r}
#We create the 2 basic plots so as to be able to explain the regression model
plot(full_5,which=1:3)
```

```{r}
ad_r_sq_ma
adj_r_square_full5 
aic_ma 
aic_full5 
#The adjusted R square and the aic are a little worse than before
```

```{r}
#######################################################################################################
##################################################################################################
#Clustering
#Based on those results we will try to cluster the companies based on the results of the regression
set.seed(220)
fortuneCluster <- kmeans(total_500_final_reg[, 1:11], 3, iter.max = 100,nstart = 1)
cluster <- table(fortuneCluster$cluster)
fortuneCluster$cluster <- as.factor(fortuneCluster$cluster)
```

```{r}
ggplot(total_500_final_reg, aes(Revenues, loading.time, color = fortuneCluster$cluster)) + geom_point(size=3)
```

```{r}
ggplot(total_500_final_reg, aes(Revenues, Readability, color = fortuneCluster$cluster)) + geom_point(size=3)
```

```{r}
ggplot(total_500_final_reg, aes(Revenues, instagram, color = fortuneCluster$cluster)) + geom_point(size=3)
```

```{r}
ggplot(total_500_final_reg, aes(Revenues, .bmp, color = fortuneCluster$cluster)) + geom_point(size=3)
```

```{r}
ggplot(total_500_final_reg, aes(Revenues, .jpeg, color = fortuneCluster$cluster)) + geom_point(size=3)
```
```{r}
ggplot(total_500_final_reg, aes(Revenues, X60x60, color = fortuneCluster$cluster)) + geom_point(size=3)
```

```{r}
ggplot(total_500_final_reg, aes(Revenues, X44x556, color = fortuneCluster$cluster)) + geom_point(size=3)
```

```{r}
ggplot(total_500_final_reg, aes(Revenues, X400x300, color = fortuneCluster$cluster)) + geom_point(size=3)
```

```{r}
#From the clustering we can see that the variables do indeed devide the most high revenues from the smallest ones
summary(model_a)
```

```{r}
#We can see from the model that the basic variable that effect a companys ranking is whether or not it has an image in size X60x60
#We will try to make a model that we will not take into consideration this variable at all just in order to see how it will explain the revenues
full_6 <- lm(Revenues~1 +X44x556 +X400x300 + .bmp +loading.time + .jpeg + Readability + instagram ,data=total_500_final_train)
summary(full_6)
adj_r_square_full6 <- summary(full_6)$adj.r.squared
aic_full6 <- AIC(full_6)
```

```{r}
#We create the 2 basic plots so as to be able to explain the regression model
plot(full_6,which=1:3)
```

```{r}
predictions_ma <- predict(model_a,total_500_final_test)
Actual_Revenues<- total_500_final_test$Revenues
```

```{r}
par(mfrow=c(2,2))
plot (Actual_Revenues, col = "blue")
plot (predictions_ma, col = "Red",main = "Model A")
#####################################
predictions_full_6 <- predict(full_6,total_500_final_test)
plot (predictions_full_6, col = "Red",main = "Full_6 model")
#######################################################
```

```{r}
#We can see that here the prediction of the new model is not as good as the previous one so now that we have checked this option as well we can conclude that the most important factors are the ones of model_a
summary(model_a)
```


