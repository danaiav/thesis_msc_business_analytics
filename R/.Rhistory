if ((image_size[[1]]<204)==TRUE && (image_size[[1]]>368)==TRUE){
true_existing <- union(true_existing, c(i))
}}
#We will see now the frequency of image sizes that is being used
k = c()
#Check for sizes that are half and half divided in existing and not
for(i in 24:716){
image_size<- round(table(total_500_final[,i]))
if ((image_size[[1]]=408)==TRUE){
k <- union(k, c(i))
}}
#####################
false_not_existing = c()
#Check for sizes that are less than half divided in existing and not
for(i in 24:716){
image_size<- round(table(total_500_final[,i]))
if ((image_size[[1]]>204)==TRUE){
false_not_existing <- union(false_not_existing, c(i))
}}
########################
true_existing = c()
#Check for sizes that are more than half divided in existing and not
for(i in 24:716){
image_size<- round(table(total_500_final[,i]))
if ((image_size[[1]]<204)==TRUE && (image_size[[1]]>40)==TRUE){
true_existing <- union(true_existing, c(i))
}}
#Now we will take the sizes that exist in more than half the instances and check graphically the deviations between the 408 sites
par(mfrow=c(3,3))
for(i in 1:226){
a = true_existing[i]
barplot(image_size,xlab=names(total_500_final)[a],ylab = "Has or not the size", col = "dark green")}
exists(image_size[[2]]
)
image_size[[2]] exists()
#We will see now the frequency of image sizes that is being used
k = c()
#Check for sizes that are half and half divided in existing and not
for(i in 24:716){
image_size<- round(table(total_500_final[,i]))
if ((image_size[[1]]=408)==TRUE){
k <- union(k, c(i))
}}
#####################
false_not_existing = c()
#Check for sizes that are less than half divided in existing and not
for(i in 24:716){
image_size<- round(table(total_500_final[,i]))
if ((image_size[[1]]>204)==TRUE){
false_not_existing <- union(false_not_existing, c(i))
}}
########################
true_existing = c()
#Check for sizes that are more than half divided in existing and not
for(i in 24:716){
image_size<- round(table(total_500_final[,i]))
if ((image_size[[1]]<204)==TRUE && (image_size[[1]]>40)==TRUE){
true_existing <- union(true_existing, c(i))
}}
#We will see now the frequency of image sizes that is being used
k = c()
#Check for sizes that are half and half divided in existing and not
for(i in 24:716){
image_size<- round(table(total_500_final[,i]))
if ((image_size[[2]]=408)==TRUE){
k <- union(k, c(i))
}}
#####################
false_not_existing = c()
#Check for sizes that are less than half divided in existing and not
for(i in 24:716){
image_size<- round(table(total_500_final[,i]))
if ((image_size[[1]]>204)==TRUE){
false_not_existing <- union(false_not_existing, c(i))
}}
########################
true_existing = c()
#Check for sizes that are more than half divided in existing and not
for(i in 24:716){
image_size<- round(table(total_500_final[,i]))
if ((image_size[[1]]<204)==TRUE && (image_size[[1]]>40)==TRUE){
true_existing <- union(true_existing, c(i))
}}
#We will see now the frequency of image sizes that is being used
k = c()
#Check for sizes that are half and half divided in existing and not
for(i in 24:716){
image_size<- round(table(total_500_final[,i]))
if ((image_size[[2]]=408)==TRUE){
k <- union(k, c(i))
}}
#####################
false_not_existing = c()
#Check for sizes that are less than half divided in existing and not
for(i in 24:716){
image_size<- round(table(total_500_final[,i]))
if ((image_size[[2]]>204)==TRUE){
false_not_existing <- union(false_not_existing, c(i))
}}
image_size
image_size[1]
image_size[[1]]
image_size[[0]]
image_size[[2]]
#We will see now the frequency of image sizes that is being used
k = c(0)
#Check for sizes that are half and half divided in existing and not
for(i in 24:716){
image_size<- round(table(total_500_final[,i]))
if ((image_size[[1]]=408)==TRUE){
k <- union(k, c(i))
}}
#####################
false_not_existing = c()
#Check for sizes that are less than half divided in existing and not
for(i in 24:716){
image_size<- round(table(total_500_final[,i]))
if ((image_size[[2]]>204)==TRUE){
false_not_existing <- union(false_not_existing, c(i))
}}
(image_size[[1]]=408)
(image_size[[1]]=408)==TRUE)
#We will see now the frequency of image sizes that is being used
k = c(0)
#Check for sizes that are half and half divided in existing and not
for(i in 24:716){
image_size<- round(table(total_500_final[,i]))
if ((image_size[[1]]==408)==TRUE){
k <- union(k, c(i))
}}
#####################
false_not_existing = c()
#Check for sizes that are less than half divided in existing and not
for(i in 24:716){
image_size<- round(table(total_500_final[,i]))
if ((image_size[[2]]>204)==TRUE){
false_not_existing <- union(false_not_existing, c(i))
}}
#We will see now the frequency of image sizes that is being used
k = c()
#Check for sizes that are half and half divided in existing and not
for(i in 24:716){
image_size<- round(table(total_500_final[,i]))
if ((image_size[[1]]==408)==TRUE){
k <- union(k, c(i))
}}
#####################
false_not_existing = c()
#Check for sizes that are less than half divided in existing and not
for(i in 24:716){
image_size<- round(table(total_500_final[,i]))
if ((image_size[[2]]>204)==TRUE){
false_not_existing <- union(false_not_existing, c(i))
}}
names(total_500_final)[24]
#We will see now the frequency of image sizes that is being used
k = c()
#Check for sizes that are half and half divided in existing and not
for(i in 24:716){
image_size<- round(table(total_500_final[,i]))
if ((image_size[[1]]==408)==TRUE){
k <- union(k, c(i))
}}
#####################
#Number 24 is all onw price so we want use it
names(total_500_final)[24]
total_500_final$X144x144 <- NULL
false_not_existing = c()
#Check for sizes that are less than half divided in existing and not
for(i in 24:715){
image_size<- round(table(total_500_final[,i]))
if ((image_size[[2]]<204)==TRUE){
false_not_existing <- union(false_not_existing, c(i))
}}
########################
#Now we will take the sizes that exist in less than half the instances and check graphically the deviations between the 408 sites
par(mfrow=c(3,3))
for(i in 1:416){
a = false_not_existing[i]
image_size<- round(table(total_500_final[,a]))
barplot(image_size,xlab=names(total_500_final)[a],ylab = "Has or not the size", col = "dark green")}
true_existing = c()
#Check for sizes that are more than half divided in existing and not
for(i in 24:715){
image_size<- round(table(total_500_final[,i]))
if ((image_size[[2]]>204)==TRUE){
true_existing <- union(true_existing, c(i))
}}
#Now we will take the sizes that exist in more than half the instances and check graphically the deviations between the 408 sites
par(mfrow=c(3,3))
for(i in 1:226){
a = true_existing[i]
image_size<- round(table(total_500_final[,a]))
barplot(image_size,xlab=names(total_500_final)[a],ylab = "Has or not the size", col = "dark green")}
#Now we will take the sizes that exist in less than half the instances and check graphically the deviations between the 408 sites
par(mfrow=c(3,3))
for(i in 1:416){
a = false_not_existing[i]
plot(total_500_final[a],total_500_final$Revenues)
image_size<- round(table(total_500_final[,a]))
barplot(image_size,xlab=names(total_500_final)[a],ylab = "Has or not the size", col = "dark green")}
#Now we will take the sizes that exist in less than half the instances and check graphically the deviations between the 408 sites
par(mfrow=c(3,3))
for(i in 1:416){
a = false_not_existing[i]
plot(total_500_final[,a],total_500_final$Revenues)
image_size<- round(table(total_500_final[,a]))
barplot(image_size,xlab=names(total_500_final)[a],ylab = "Has or not the size", col = "dark green")}
true_existing = c()
#Check for sizes that are more than half divided in existing and not
for(i in 24:715){
image_size<- round(table(total_500_final[,i]))
if ((image_size[[2]]>204)==TRUE){
true_existing <- union(true_existing, c(i))
}}
#Now we will take the sizes that exist in more than half the instances and check graphically the deviations between the 408 sites
par(mfrow=c(3,3))
for(i in 1:276){
a = true_existing[i]
image_size<- round(table(total_500_final[,a]))
plot(total_500_final[,a],total_500_final$Revenues)
barplot(image_size,xlab=names(total_500_final)[a],ylab = "Has or not the size", col = "dark green")}
#By checking the above plots we can see that the 24 first sizes do appear to have some differentiation regarding the revenues. While most sites do have those sizes when it comes to the high revienues they do not have them
par(mfrow=c(3,3))
keep = c()
for(i in 1:24){
a = true_existing[i]
keep = union (keep, c(a))}
#By checking the above plots we can see that the 24 first sizes do appear to have some differentiation regarding the revenues. While most sites do have those sizes when it comes to the high revienues they do not have them
par(mfrow=c(3,3))
keep = c()
for(i in 1:24){
a = true_existing[i]
keep = union (keep, c(a))}
keep
#By checking the above plots we can see that the 24 first sizes do appear to have some differentiation regarding the revenues. While most sites do have those sizes when it comes to the high revienues they do not have them
par(mfrow=c(3,3))
keep = c()
for(i in 1:24){
a = true_existing[i]
keep = union (keep, c(a))}
keep
#As we can see they are the variables from 24 to 47 and these are the only sizes we are going to keep for the further analysis
total_500_final <- total_500_final(-c(48:715))
#By checking the above plots we can see that the 24 first sizes do appear to have some differentiation regarding the revenues. While most sites do have those sizes when it comes to the high revienues they do not have them
par(mfrow=c(3,3))
keep = c()
for(i in 1:24){
a = true_existing[i]
keep = union (keep, c(a))}
keep
#As we can see they are the variables from 24 to 47 and these are the only sizes we are going to keep for the further analysis
total_500_final <- total_500_final(,-c(48:715))
#By checking the above plots we can see that the 24 first sizes do appear to have some differentiation regarding the revenues. While most sites do have those sizes when it comes to the high revienues they do not have them
par(mfrow=c(3,3))
keep = c()
for(i in 1:24){
a = true_existing[i]
keep = union (keep, c(a))}
keep
#As we can see they are the variables from 24 to 47 and these are the only sizes we are going to keep for the further analysis
total_500_final <- total_500_final[,-c(48:715)]
#Also we remove the other Fortune 500 variables since they will interfer in the outcome of the model and we keep only the variable we want to examine the Revenues
total_500_final$Market_Value <- NULL
total_500_final$Assets <- NULL
total_500_final$Ranking <- NULL
total_500_final$Total_SH_Equity <- NULL
total_500_final$The_page_opened <- NULL
#We split the set to training and test set
library(caret)
set.seed(20)
sampling_vector <- createDataPartition(total_500_final$Revenues, p = 0.85, list = FALSE)
total_500_final_train <- total_500_final[sampling_vector,]
total_500_final_test <- total_500_final[-sampling_vector,]
#We will try to create a regression model to see which of the variables of the websites play the most important part regarding the Ranking of the company.
#We create the empty lm model
model_null = lm(Revenues~1,data=total_500_final_train)
summary(model_null)
#####################################################################################################
#LASSO and Logistic Regression models
library(glmnet)
#We create a full model for the variable Ranking
full <- lm(Revenues~.,data=total_500_final_train)
summary(full)
x <- model.matrix(full) [,-1]
dim(x)
lasso <- glmnet (x, total_500_final_train$Revenues)
par(mfrow=c(1,1),no.readonly = TRUE)
plot(lasso, xvar='lambda', label=T)
lassob <- cv.glmnet(x,total_500_final_train$Revenues)
lassob$lambda.min
lassob$lambda.1se
plot(lassob)
#We see the coefficients for lamda min
blasso <- coef(lassob, s="lambda.min")
blasso
dim(blasso)
zblasso <- blasso[-1] * apply(x,2,sd)
zbolt <- coef (full) [-1] * apply (x,2,sd)
azbolt <- abs(zbolt)
sum(azbolt)
#since the sum is NA that means we have to substract some variables
# in order to find which variables to substract we run the coefficients and we see which of them has NA as result
coef(full)
#Now we create a new model with only the variables with coef different from NA
full_2 <- lm(Revenues~. - total.images - total.links - X1x1 - X21pxx173px - X46x214 - X49x49 - X200pxx200px - X1279pxx984px - X300pxx1500px - X160x233 -  X300x993 - X41x192 - X28x221 - X15x12,data=total_500_final_train)
summary(full_2)
x <- model.matrix(full_2) [,-c(18,22,28,26,27,34,32,33,41,37,38,39,40,52)]
dim(x)
lasso <- glmnet (x, total_500_final_train$Revenues)
par(mfrow=c(1,1),no.readonly = TRUE)
plot(lasso, xvar='lambda', label=T)
lassob <- cv.glmnet(x,total_500_final_train$Revenues)
lassob$lambda.min
lassob$lambda.1se
plot(lassob)
#coefiecinets for lammda min
blasso <- coef(lassob, s="lambda.min")
blasso
dim(blasso)
zblasso <- blasso[-1] * apply(x,2,sd)
zbolt <- coef (full_2) [-1] * apply (x,2,sd)
azbolt <- abs(zbolt)
sum(azbolt)
s <- sum(abs(zblasso))/sum(abs(azbolt))
s
full_3 <- lm(Revenues~1 +X8x15  +X44x556 +X800x1200 +X24pxx133px +X50x45 +X400x300 +X60x60  +.bmp +.dib ,data=total_500_final_train)
summary(full_2)
full_3 <- lm(Revenues~1 +X8x15  +X44x556 +X800x1200 +X24pxx133px +X50x45 +X400x300 +X60x60  +.bmp +.dib ,data=total_500_final_train)
summary(full_3)
full_3 <- lm(Revenues~1 +X8x15  +X44x556 +X800x1200 +X24pxx133px +X50x45 +X400x300 +X60x60  +.bmp +.dib ,data=total_500_final_train)
summary(full_3)
ad_r_sq_f3 <- summary(full_3)$adj.r.squared
aic_f3 <- AIC(full_3)
##############################################
blassob <- coef(lassob, s="lambda.1se")
blassob
zblassob <- blassob[-1] * apply(x,2,sd)
zboltb <- coef (full_2) [-1] * apply (x,2,sd)
s <- sum(abs(zblassob))/sum(abs(zboltb))
s
#The model based on the lasso method by taking the lambda.1se is the null model only with the intercept
full_4 <- lm(Revenues~1 +X8x15  +X44x556 +X800x1200 +X24pxx133px +X50x45 +X400x300 +X60x60 ,data=total_500_final_train)
summary(full_4)
ad_r_sq_f4 <- summary(full_4)$adj.r.squared
aic_f4 <- AIC(full_4)
###############################################
#We use the "both" method to compare the full_3 model with the null model to see how many variables are indeed important
model_a <- step(model_null, scope = list(lower = model_null, upper=full_2), direction = "both")
summary(model_a)
ad_r_sq_ma <- summary(model_a)$adj.r.squared
aic_ma <- AIC(model_a)
#We create the 2 basic plots so as to be able to explain the regression model
plot(model_a,which=1:3)
################
#We compare the Adjusted R squares of the models and also the AIC of the models we created to find the best one
adj_r_square_f3
################
#We compare the Adjusted R squares of the models and also the AIC of the models we created to find the best one
ad_r_sq_f3
ad_r_sq_f4
ad_r_sq_ma
#The best Adkusted R square is the one in full 3 (the closer to 1 the better)
aic_f3
aic_f4
aic_ma
#The best AIC is for full ma but the difference is not very big  in comparison to the full 3 model that has the best Adjusted R square
#######################################################################################################
predictions_ma <- predict(model_a,total_500_final_test)
Actual_Revenues<- total_500_final_test$Revenues
par(mfrow=c(2,2))
plot (Actual_Revenues, col = "blue")
plot (predictions_ma, col = "Red",main = "Model A")
#####################################
predictions_full3 <- predict(full_3,total_500_final_test)
plot (predictions_full3, col = "Red",main = "Full_3 model")
#####################################
predictions_full4 <- predict(full_4,total_500_final_test)
plot (predictions_full4, col = "Red",main = "Full_4 model")
#####################################
#From the plots above we can see that the actual Revenues have a more smooth way of leveling up except from the Revenues of the #1 ranking company that are extremely high in relationship with the other sites.
#The prediction model that is more smooth is the model a which has as we said before the best Adjusted R Square and the best AIC price
#######################################################################################################
predictions_ma <- predict(model_a,total_500_final_test)
Actual_Revenues<- total_500_final_test$Revenues
par(mfrow=c(2,2))
plot (Actual_Revenues, col = "blue")
plot (predictions_ma, col = "Red",main = "Model A")
#####################################
predictions_full3 <- predict(full_3,total_500_final_test)
plot (predictions_full3, col = "Red",main = "Full_3 model")
#####################################
predictions_full4 <- predict(full_4,total_500_final_test)
plot (predictions_full4, col = "Red",main = "Full_4 model")
#####################################
#From the plots above we can see that the actual Revenues have a more smooth way of leveling up except from the Revenues of the #1 ranking company that are extremely high in relationship with the other sites.
#The prediction model that is more smooth is the model a which has as we said before the best Adjusted R Square and the best AIC price
par(mfrow=c(2,2))
plot (Actual_Revenues, col = "blue")
plot (predictions_ma, col = "Red",main = "Model a")
#####################################
predictions_full3 <- predict(full_3,total_500_final_test)
plot (predictions_full3, col = "Red",main = "Full_3 model")
#####################################
predictions_full4 <- predict(full_4,total_500_final_test)
plot (predictions_full4, col = "Red",main = "Full_4 model")
#####################################
predictions_full2 <- predict(full_2,total_500_final_test)
plot (predictions_full2, col = "Red",main = "Full_4 model")
#####################################
#From the plots above we can see that the actual Revenues have a more smooth way of leveling up except from the Revenues of the #1 ranking company that are extremely high in relationship with the other sites.
#The prediction model that is more smooth is the model a which has as we said before the best Adjusted R Square and the best AIC price
par(mfrow=c(3,2))
plot (Actual_Revenues, col = "blue")
plot (predictions_ma, col = "Red",main = "Model a")
#####################################
predictions_full3 <- predict(full_3,total_500_final_test)
plot (predictions_full3, col = "Red",main = "Full_3 model")
#####################################
predictions_full4 <- predict(full_4,total_500_final_test)
plot (predictions_full4, col = "Red",main = "Full_4 model")
#####################################
predictions_full2 <- predict(full_2,total_500_final_test)
plot (predictions_full2, col = "Red",main = "Full_2 model")
#####################################
#From the plots above we can see that the actual Revenues have a more smooth way of leveling up except from the Revenues of the #1 ranking company that are extremely high in relationship with the other sites.
#The prediction model that is more smooth is the model a which has as we said before the best Adjusted R Square and the best AIC price
#######################################################################################################
par(mfrow=c(2,2))
Actual_Revenues<- total_500_final_test$Revenues
plot (Actual_Revenues, col = "blue")
###########################################
predictions_ma <- predict(model_a,total_500_final_test)
plot (predictions_ma, col = "Red",main = "Model a")
#####################################
predictions_full3 <- predict(full_3,total_500_final_test)
plot (predictions_full3, col = "Red",main = "Full_3 model")
#####################################
predictions_full4 <- predict(full_4,total_500_final_test)
plot (predictions_full4, col = "Red",main = "Full_4 model")
#####################################
#From the plots above we can see that the actual Revenues have a more smooth way of leveling up except from the Revenues of the #1 ranking company that are extremely high in relationship with the other sites.
#The prediction model that is more smooth is the model a which has as we said before the best Adjusted R Square and the best AIC price
names(total_500_final_train)
par(mfrow=c(1,1))
total_500_final_reg <- total_500_final_train[,c(1,6,12,20,21,25,30,42,43,47,53)]
corrplot(cor(total_500_final_reg),method="number")
#We xan see here that the variable im_small has a very high correlation with the variable very_large and also the variable very large has also a very high correlation with the variable thumbnail.
#So we can try creating a new model excluding the 2 variables that are correlated from each pair to see if there will be any improvement in the model
full_5 <- lm(Revenues~1 +X60x60 +X44x556 +X400x300 + .bmp +loading.time + .jpeg + Readability + instagram ,data=total_500_final_train)
summary(full_5)
adj_r_square_full5 <- summary(full_5)$adj.r.squared
aic_full5 <- AIC(full_5)
#We create the 2 basic plots so as to be able to explain the regression model
plot(full_5,which=1:3)
ad_r_sq_ma
adj_r_square_full5
aic_ma
aic_full5
#The adjusted R square and the aic are a little worse than before
#######################################################################################################
##################################################################################################
#Clustering
#Based on those results we will try to cluster the companies based on the results of the regression
set.seed(220)
fortuneCluster <- kmeans(total_500_final_reg[, 1:11], 3, iter.max = 100,nstart = 1)
cluster <- table(fortuneCluster$cluster)
fortuneCluster$cluster <- as.factor(fortuneCluster$cluster)
ggplot(total_500_final_reg, aes(Revenues, external, color = fortuneCluster$cluster)) + geom_point(size=3)
#######################################################################################################
##################################################################################################
#Clustering
#Based on those results we will try to cluster the companies based on the results of the regression
set.seed(220)
fortuneCluster <- kmeans(total_500_final_reg[, 1:11], 3, iter.max = 100,nstart = 1)
cluster <- table(fortuneCluster$cluster)
fortuneCluster$cluster <- as.factor(fortuneCluster$cluster)
ggplot(total_500_final_reg, aes(Revenues, loading.time, color = fortuneCluster$cluster)) + geom_point(size=3)
ggplot(total_500_final_reg, aes(Revenues, Readability, color = fortuneCluster$cluster)) + geom_point(size=3)
ggplot(total_500_final_reg, aes(Revenues, instagram, color = fortuneCluster$cluster)) + geom_point(size=3)
#We can see from the plots that the variable that plays the most important part on the creation of the clusters is the number are the number of external links.
ggplot(total_500_final_reg, aes(Revenues, .bmp, color = fortuneCluster$cluster)) + geom_point(size=3)
ggplot(total_500_final_reg, aes(Revenues, .jpeg, color = fortuneCluster$cluster)) + geom_point(size=3)
ggplot(total_500_final_reg, aes(Revenues, instagram, color = fortuneCluster$cluster)) + geom_point(size=3)
ggplot(total_500_final_reg, aes(Revenues, Readability, color = fortuneCluster$cluster)) + geom_point(size=3)
ggplot(total_500_final_reg, aes(Revenues, loading.time, color = fortuneCluster$cluster)) + geom_point(size=3)
ggplot(total_500_final_reg, aes(Revenues, X60x60, color = fortuneCluster$cluster)) + geom_point(size=3)
ggplot(total_500_final_reg, aes(Revenues, X44x556, color = fortuneCluster$cluster)) + geom_point(size=3)
ggplot(total_500_final_reg, aes(Revenues, X400x300, color = fortuneCluster$cluster)) + geom_point(size=3)
#From the clustering we can see that the variables do indeed devide the most high revenues from the smallest ones
summary(model_a)
#We can see from the model that the basic variable that effect a companys ranking is whether or not it has an image in size X60x60
#We will try to make a model that we will not take into consideration this variable at all just in order to see how it will explain the revenues
full_6 <- lm(Revenues~1 +X44x556 +X400x300 + .bmp +loading.time + .jpeg + Readability + instagram ,data=total_500_final_train)
summary(full_6)
adj_r_square_full6 <- summary(full_6)$adj.r.squared
aic_full6 <- AIC(full_6)
#We create the 2 basic plots so as to be able to explain the regression model
plot(full_6,which=1:3)
predictions_ma <- predict(model_a,total_500_final_test)
Actual_Revenues<- total_500_final_test$Revenues
par(mfrow=c(2,2))
plot (Actual_Revenues, col = "blue")
plot (predictions_ma, col = "Red",main = "Model A")
#####################################
predictions_full_6 <- predict(full_6,total_500_final_test)
plot (predictions_full_6, col = "Red",main = "Full_6 model")
#######################################################
#We can see that here the prediction of the new model is not as good as the previous one so now that we have checked this option as well we can conclude that the most important factors are the ones of model_a
summary(model_a)
ggplot(total_500_final_reg, aes(X44x556, color = fortuneCluster$cluster)) + geom_point(size=3)
ggplot(total_500_final_reg, aes(Revenues, X44x556, color = fortuneCluster$cluster)) + geom_point(size=3)
