{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#First we import the libraries we will need\n",
    "import urllib\n",
    "import urllib2\n",
    "import time\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#First of all we need to find all the name of the sites that belong to fortune 500. This can happen if we seperate\n",
    "#The information needed from the below link\n",
    "url = \"http://www.zyxware.com/articles/4344/list-of-fortune-500-companies-and-their-websites\"\n",
    "list_company_number =[]\n",
    "list_company_name = []\n",
    "list_company_website = []\n",
    "list500_sites = []\n",
    "list500_names = []\n",
    "list500_num = []\n",
    "list500_url = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#In order to extract the needed informations we will create 3 lists. The first one will contain the rank of each site, the\n",
    "#second one will contain the name of the company and the 3rd one will contain the actual link of the company's site.\n",
    "#For achieving this purpose we will create a funstion that will in its turn create those three list.\n",
    "#In order to know if the function worked we will ask it to return the first element of each list along with a sentence.\n",
    "def websites (url): \n",
    "    from time import time # I used it to see how much time it does to run the function\n",
    "    start = time ()\n",
    "    browser = urllib2.build_opener() \n",
    "    browser.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
    "    response = browser.open(url)# this might throw an exception if something goes wrong.\n",
    "    myHTML = response.read()\n",
    "    soup = BeautifulSoup(myHTML,\"lxml\")    \n",
    "    o = 0\n",
    "    td_list =[]\n",
    "    for row2 in soup.html.body.findAll('td'):\n",
    "        td_list.insert(o, row2)\n",
    "        o = o + 1\n",
    "    a = 0\n",
    "    b = 1\n",
    "    c = 2\n",
    "    list_numbering = 0\n",
    "    for i in range (0,500):        \n",
    "        num = str(td_list[a])\n",
    "        company = str(td_list[b])\n",
    "        site = str(td_list[c])\n",
    "        c_num = re.findall('>(.+?)</td>',num)  \n",
    "        c_num = str(c_num[0])\n",
    "        c_name = re.findall('>(.+?)</td>',company)\n",
    "        c_name = str(c_name[0])\n",
    "        c_site = re.findall('\">(.+?)</a>',site)\n",
    "        c_site = str(c_site[0])        \n",
    "        list_company_number.insert(list_numbering,c_num)\n",
    "        list_company_name.insert(list_numbering,c_name)\n",
    "        list_company_website.insert(list_numbering,c_site)\n",
    "        a = a + 3\n",
    "        b = b + 3\n",
    "        c = c + 3\n",
    "        list_numbering =  list_numbering + 1 \n",
    "    end = time ()\n",
    "    duration = round (end - start, 1)\n",
    "    minutes = round (duration /60, 1)\n",
    "    print 'The lists are ready in ', duration, ' seconds'\n",
    "    print 'The lists are ready in ', minutes, ' minutes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lists are ready in  1.2  seconds\n",
      "The lists are ready in  0.0  minutes\n"
     ]
    }
   ],
   "source": [
    "# After creating the function we should now test that it actually works correctly\n",
    "websites (url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The validation is complete! There were 0 not valid pages\n"
     ]
    }
   ],
   "source": [
    "#Try to validate each page url #pip install validators\n",
    "import validators\n",
    "nv = 0\n",
    "for num in range(len(list_company_website)):\n",
    "    line = 'http://' + str(list_company_website[num])\n",
    "    x = validators.url(line)    \n",
    "    if x != True:\n",
    "        nv = nv +1\n",
    "print \"The validation is complete! There were\" , nv, \"not valid pages\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The site 62 has NOT been downloaded!\n",
      "The site 90 has NOT been downloaded!\n",
      "The site 97 has NOT been downloaded!\n",
      "The site 118 has NOT been downloaded!\n",
      "The site 135 has NOT been downloaded!\n",
      "The site 141 has NOT been downloaded!\n",
      "The site 161 has NOT been downloaded!\n",
      "The site 164 has NOT been downloaded!\n",
      "The site 195 has NOT been downloaded!\n",
      "The site 216 has NOT been downloaded!\n",
      "The site 228 has NOT been downloaded!\n",
      "The site 242 has NOT been downloaded!\n",
      "The site 243 has NOT been downloaded!\n",
      "The site 275 has NOT been downloaded!\n",
      "The site 306 has NOT been downloaded!\n",
      "The site 326 has NOT been downloaded!\n",
      "The site 363 has NOT been downloaded!\n",
      "The site 377 has NOT been downloaded!\n",
      "The site 397 has NOT been downloaded!\n",
      "The site 414 has NOT been downloaded!\n",
      "The site 441 has NOT been downloaded!\n",
      "The site 464 has NOT been downloaded!\n"
     ]
    }
   ],
   "source": [
    "#def list_company_HTML (list_company_website,list_company_name,start,end):\n",
    "import time\n",
    "browser2 = urllib2.build_opener()\n",
    "browser2.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
    "for i in range (0,500):\n",
    "    k = str(i + 1)\n",
    "    lc = str(list_company_website[i])\n",
    "    lc = lc.replace(\"'\",\"\")\n",
    "    lc = lc.replace(\"[\",\"\")\n",
    "    lc = lc.replace(\"]\",\"\")\n",
    "    lcn = str(list_company_name[i])\n",
    "    lcn = lcn.replace(\"'\",\"\")\n",
    "    lcn = lcn.replace(\"[\",\"\")\n",
    "    lcn = lcn.replace(\"]\",\"\")\n",
    "    url2= 'http://' + lc\n",
    "    list500_names.insert(i,lcn)\n",
    "    list500_url.insert(i,lc)\n",
    "    list500_num.insert(i,k)\n",
    "    if i == 118 or i == 464:#The site 118(119) has a problem and the whole code is stacking \n",
    "        #when I run it so we will thing of this site as a not downloadable\n",
    "        list500_sites.insert(i,0)  \n",
    "        print (\"The site \" + str(i) + \" has NOT been downloaded!\")\n",
    "    else:\n",
    "        #an exception might be thrown, so the code should be in a try-except block\n",
    "        try:\n",
    "            response2=browser2.open(url2)\n",
    "        except Exception: # this describes what to do if an exception is thrown\n",
    "            list500_sites.insert(i,0)\n",
    "            print (\"The site \" + str(i) + \" has NOT been downloaded!\") \n",
    "            continue     \n",
    "            #read the response in html format. This is essentially a long piece of text\n",
    "        myHTML2=response2.read()\n",
    "        list500_sites.insert(i,myHTML2)\n",
    "        #wait for 2 seconds\n",
    "        time.sleep(2)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#As we can see there is one site that hasn't been downloaded in order\n",
    "#to keep track of the sites that we could not download\n",
    "#we will create a new list that we will keep them all together there\n",
    "not_d = []\n",
    "not_d_n = []\n",
    "num = []\n",
    "def not_downloadables (list500_names,list500_sites):\n",
    "    met = 0       \n",
    "    for i in range(len(list500_names)):       \n",
    "        if list500_sites[i] == 0:\n",
    "            ct = list500_names[i]\n",
    "            not_d.insert(met,ct)\n",
    "            not_d_n.insert(met,str(i))\n",
    "            num.insert(met,met)\n",
    "            met = met + 1\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HCA Holdings</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nike</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tesoro</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arrow Electronics</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AutoNation</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Southwest Airlines</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Southern</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>American Electric Power</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Office Depot</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PBF Energy</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Consolidated Edison</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Dominion Resources</td>\n",
       "      <td>242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>United States Steel</td>\n",
       "      <td>243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Global Partners</td>\n",
       "      <td>275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>PayPal Holdings</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>News Corp.</td>\n",
       "      <td>326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Williams</td>\n",
       "      <td>363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Huntington Ingalls Industries</td>\n",
       "      <td>377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Auto-Owners Insurance</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Tractor Supply</td>\n",
       "      <td>414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Old Republic International</td>\n",
       "      <td>441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>St. Jude Medical</td>\n",
       "      <td>464</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          company number\n",
       "0                    HCA Holdings     62\n",
       "1                            Nike     90\n",
       "2                          Tesoro     97\n",
       "3               Arrow Electronics    118\n",
       "4                      AutoNation    135\n",
       "5              Southwest Airlines    141\n",
       "6                        Southern    161\n",
       "7         American Electric Power    164\n",
       "8                    Office Depot    195\n",
       "9                      PBF Energy    216\n",
       "10            Consolidated Edison    228\n",
       "11             Dominion Resources    242\n",
       "12            United States Steel    243\n",
       "13                Global Partners    275\n",
       "14                PayPal Holdings    306\n",
       "15                     News Corp.    326\n",
       "16                       Williams    363\n",
       "17  Huntington Ingalls Industries    377\n",
       "18          Auto-Owners Insurance    397\n",
       "19                 Tractor Supply    414\n",
       "20     Old Republic International    441\n",
       "21               St. Jude Medical    464"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now we will run the function to see which sites havent been downloaded\n",
    "not_downloadables (list500_names,list500_sites)\n",
    "d = {'company' : pd.Series(not_d, index=[num]),\n",
    "     'number' : pd.Series(not_d_n, index=[num])}\n",
    "nd = pd.DataFrame(d)    \n",
    "nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "empty=[]\n",
    "keyf = []\n",
    "flesch = []\n",
    "sentence =[] \n",
    "word = []\n",
    "unique_w = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Site', '0', 'is not validated')\n",
      "('Site', '1', 'is not validated')\n",
      "('Site', '11', 'is not validated')\n",
      "('Site', '15', 'is not validated')\n",
      "('Site', '33', 'is not validated')\n",
      "('Site', '37', 'is not validated')\n",
      "('Site', '58', 'is not validated')\n",
      "('Site', '67', 'is not validated')\n",
      "('Site', '70', 'is not validated')\n",
      "('Site', '82', 'is not validated')\n",
      "('Site', '90', 'is not validated')\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 524: Origin Time-out",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-9bcdd86667ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mbrowser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murllib2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_opener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mbrowser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddheaders\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'User-agent'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Mozilla/5.0'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbrowser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl_check\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0mhtml_r\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mcheck\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhtml_r\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Danai\\Anaconda2\\lib\\urllib2.pyc\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Danai\\Anaconda2\\lib\\urllib2.pyc\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m             response = self.parent.error(\n\u001b[0;32m--> 548\u001b[0;31m                 'http', request, response, code, msg, hdrs)\n\u001b[0m\u001b[1;32m    549\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Danai\\Anaconda2\\lib\\urllib2.pyc\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'default'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'http_error_default'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[1;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Danai\\Anaconda2\\lib\\urllib2.pyc\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Danai\\Anaconda2\\lib\\urllib2.pyc\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m         \u001b[1;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_full_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 524: Origin Time-out"
     ]
    }
   ],
   "source": [
    "import time # I used it to see how much time it does to run the function\n",
    "for num in range(0, 500):\n",
    "            line = list500_url[num] \n",
    "            url_check = \"http://www.webpagefx.com/tools/read-able/check.php?tab=Test+By+Url&uri=http://\" + line\n",
    "            browser = urllib2.build_opener()\n",
    "            browser.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
    "            response = browser.open(url_check)\n",
    "            html_r = response.read()\n",
    "            check = str(html_r)       \n",
    "            if check != empty:\n",
    "                        soup = BeautifulSoup(check,\"lxml\")\n",
    "                        o = 0\n",
    "                        keyf = []\n",
    "                        for row in soup.html.body.findAll('tr'):\n",
    "                            keyf.insert(o,row)\n",
    "                            o = o + 1  \n",
    "                        if keyf != empty:\n",
    "                            #Flesh measurement\n",
    "                            readability = str(keyf[0])\n",
    "                            split1 = readability.split('>')\n",
    "                            readability2 = str(split1[4])\n",
    "                            split2 = readability2.split('<')\n",
    "                            readability3 = str(split2[0])\n",
    "                            flesch.insert(num,readability3)\n",
    "                            #Number of sentences            \n",
    "                            sentences = str(keyf[6])\n",
    "                            spli1 = sentences.split('>')\n",
    "                            sentences2 = str(spli1[4])\n",
    "                            spli2 = sentences2.split('<')\n",
    "                            sentences3 = str(spli2[0])\n",
    "                            sentence.insert(num,sentences3)\n",
    "                            #Number of words            \n",
    "                            words = str(keyf[7])\n",
    "                            spl1 = words.split('>')\n",
    "                            words2 = str(spl1[4])\n",
    "                            spl2 = words2.split('<')\n",
    "                            words3 = str(spl2[0])\n",
    "                            word.insert(num,words3)\n",
    "                            #No. of complex words          \n",
    "                            unique_ws = str(keyf[8])\n",
    "                            sp1 = unique_ws.split('>')\n",
    "                            unique_ws2 = str(sp1[4])\n",
    "                            sp2 = unique_ws2.split('<')\n",
    "                            unique_ws3 = str(sp2[0])\n",
    "                            unique_w.insert(num,unique_ws3)\n",
    "                        else:\n",
    "                            print(\"Site\", str(num), \"is not validated\")\n",
    "                            flesch.insert(num,\"n/a\")\n",
    "                            sentence.insert(num,\"n/a\")\n",
    "                            word.insert(num,\"n/a\")\n",
    "                            unique_w.insert(num,\"n/a\")\n",
    "            else:\n",
    "                        print(\"Site\", str(num), \"is not validated\")\n",
    "                        flesch.insert(num,\"n/a\")\n",
    "                        sentence.insert(num,\"n/a\")\n",
    "                        word.insert(num,\"n/a\")\n",
    "                        unique_w.insert(num,\"n/a\")\n",
    "            time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "readability = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readable (flesch):\n",
    "    for i in range (len(flesch)):\n",
    "            f_n = flesch[i]\n",
    "            if f_n == \"n/a\":\n",
    "                readability.insert(i,\"n/a\")                \n",
    "            else:\n",
    "                a = int(float(f_n))\n",
    "                if a > 90:    \n",
    "                    readability.insert(i,\"Very easy\")                    \n",
    "                elif a > 80:\n",
    "                    readability.insert(i,\"Easy\")\n",
    "                elif a > 70:\n",
    "                    readability.insert(i,\"Fairly easy\")\n",
    "                elif a > 60:\n",
    "                    readability.insert(i,\"Standard\")\n",
    "                elif a > 50:\n",
    "                    readability.insert(i,\"Fairly difficult\")\n",
    "                elif a > 30:\n",
    "                    readability.insert(i,\"Difficult\")\n",
    "                else:\n",
    "                    readability.insert(i,\"Very Confusing\")                    \n",
    "    print \"The function is completed!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "readable (flesch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d1 = {'company' : pd.Series(list500_names, index=[list500_num]),\n",
    "      'url' : pd.Series(list500_url, index=[list500_num]),\n",
    "      'Readability' : pd.Series(readability, index=[list500_num]),\n",
    "      'Flesh_Mesaure' : pd.Series(flesh,index=[list500_num]),\n",
    "'Sentences' : pd.Series(sentence, index=[list500_num]),\n",
    "'Words' : pd.Series(word, index=[list500_num]),\n",
    "'Unique words' : pd.Series(unique_w, index=[list500_num])}\n",
    "fre = pd.DataFrame(d1)    \n",
    "fre.tail(3) #we see the first 3 in the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Retreiving the social media from each site\n",
    "#First create empty lists for the ones that \n",
    "#we will need to calculate\n",
    "sm_f = []\n",
    "sm_t = []\n",
    "sm_i = []\n",
    "sm_p = []\n",
    "sm_y = []\n",
    "sm_l = []   \n",
    "sm_nm = [] \n",
    "nm = []\n",
    "sm_url = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Then create a function that will feel in those \n",
    "#lists so as to make the data frame later on\n",
    "def socialmedia (list500_sites,list500_names,list500_url):\n",
    "    from time import time \n",
    "    # I used it to see how much time it does to run the function\n",
    "    start = time ()\n",
    "    for i in range(len(list500_names)):        \n",
    "            myHTML = list500_sites[i]\n",
    "            sm = ['facebook.com','twitter.com',\n",
    "                  'instagram.com','pinterest.com',\n",
    "                  'youtube.com','linkedin.com'] \n",
    "            if myHTML == 0:                \n",
    "                sm_nm.insert(i,list500_names[i]) \n",
    "                nm.insert(i,i)\n",
    "                sm_url.insert(i,list500_url[i])\n",
    "                sm_f.insert(i,'n/a')\n",
    "                sm_t.insert(i,'n/a')\n",
    "                sm_i.insert(i,'n/a')\n",
    "                sm_p.insert(i,'n/a')\n",
    "                sm_y.insert(i,'n/a')\n",
    "                sm_l.insert(i,'n/a')\n",
    "            else:\n",
    "                for index in range(len(sm)):\n",
    "                    x = sm[index]\n",
    "                    social = re.findall(x,myHTML)                                \n",
    "                    if (len(social) > 0):\n",
    "                        if x == 'facebook.com':\n",
    "                            answerf = 'TRUE'\n",
    "                        if x == 'twitter.com':\n",
    "                            answert = 'TRUE'\n",
    "                        if x == 'instagram.com':\n",
    "                            answeri = 'TRUE'\n",
    "                        if x == 'pinterest.com':\n",
    "                            answerp = 'TRUE'\n",
    "                        if x == 'youtube.com':\n",
    "                            answery = 'TRUE'\n",
    "                        if x =='linkedin.com':\n",
    "                            answerl = 'TRUE'                   \n",
    "                    else:\n",
    "                         if x == 'facebook.com':\n",
    "                            answerf = 'FALSE'\n",
    "                         if x == 'twitter.com':\n",
    "                            answert = 'FALSE'\n",
    "                         if x == 'instagram.com':\n",
    "                            answeri = 'FALSE'\n",
    "                         if x == 'pinterest.com':\n",
    "                            answerp = 'FALSE'\n",
    "                         if x == 'youtube.com':\n",
    "                            answery = 'FALSE'\n",
    "                         if x =='linkedin.com':\n",
    "                            answerl = 'FALSE'                \n",
    "                sm_nm.insert(i,list500_names[i]) \n",
    "                nm.insert(i,i)\n",
    "                sm_url.insert(i,list500_url[i])\n",
    "                sm_f.insert(i,answerf)\n",
    "                sm_t.insert(i,answert)\n",
    "                sm_i.insert(i,answeri)\n",
    "                sm_p.insert(i,answerp)\n",
    "                sm_y.insert(i,answery)\n",
    "                sm_l.insert(i,answerl)\n",
    "    end = time ()\n",
    "    duration = round (end - start, 3)\n",
    "    minutes = round (duration /60, 1)\n",
    "    print 'The lists are completed in ', minutes, ' minutes' \n",
    "    print 'The lists are ready in ', duration, ' seconds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Now we will run the function for the 25 first sites for starters\n",
    "socialmedia (list500_sites,list500_names,list500_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Finally we create the data frame with the elements we found            \n",
    "d2 = {'company' : pd.Series(sm_nm, index=[nm]),\n",
    "     'facebook' : pd.Series(sm_f, index=[nm]),\n",
    "      'twitter' : pd.Series(sm_t, index=[nm]),\n",
    "     'instagram' : pd.Series(sm_i, index=[nm]),\n",
    "      'pinterest' : pd.Series(sm_p, index=[nm]),\n",
    "     'youtube' : pd.Series(sm_y, index=[nm]),\n",
    "      'linkedin' : pd.Series(sm_l, index=[nm]),}\n",
    "social_media = pd.DataFrame(d2)    \n",
    "social_media.tail(3) #we see the first 3 in the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create the lists we will need for the data frame\n",
    "l_nm = []\n",
    "l_ex = []\n",
    "l_in = []\n",
    "l_t = []\n",
    "nm = []\n",
    "l_url = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create the function that will calculate the different type of links\n",
    "def links (list500_sites,list500_names,list500_url):\n",
    "    from time import time \n",
    "    # I used it to see how much time it does to run the function\n",
    "    start = time ()\n",
    "    for num in range(len(list500_names)):        \n",
    "            myHTML = list500_sites[num]\n",
    "            if myHTML == 0:\n",
    "                l_nm.insert(num,list500_names[num])            \n",
    "                l_ex.insert(num,'n/a')\n",
    "                l_t.insert(num,'n/a')\n",
    "                l_in.insert(num,'n/a')\n",
    "                nm.insert(num,num)                \n",
    "            else: \n",
    "                href = re.findall('href',myHTML)\n",
    "                external = re.findall('href=\"https:',myHTML)\n",
    "                ex = (len(external))\n",
    "                alllinks = (len(href))\n",
    "                internal =  (len(href) - len(external))\n",
    "                l_nm.insert(num,list500_names[num])            \n",
    "                l_ex.insert(num,ex)\n",
    "                l_t.insert(num,alllinks)\n",
    "                l_in.insert(num,internal)\n",
    "                nm.insert(num,num)                \n",
    "    end = time ()\n",
    "    duration = round (end - start, 3)\n",
    "    minutes = round (duration /60, 1)\n",
    "    print 'The lists are ready in ', minutes, ' minutes'\n",
    "    print 'The lists are ready in ', duration, ' seconds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Run the function in order to find the external, \n",
    "#internal and total links of each site\n",
    "#For now we are running for the first 25 sites only\n",
    "links (list500_sites,list500_names,list500_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create a dataframe so as to be able to see \n",
    "#the results of the function we run\n",
    "d3 = {'company' : pd.Series(l_nm, index=[nm]),\n",
    "      'external' : pd.Series(l_ex, index=[nm]),\n",
    "      'internal' : pd.Series(l_in, index=[nm]),\n",
    "     'total links' : pd.Series(l_t, index=[nm])}\n",
    "sites_links = pd.DataFrame(d3)    \n",
    "sites_links.tail(3) #we see the first 3 in the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The initial lists we will need in order \n",
    "#to calculate the loading time\n",
    "lt_nm = [] \n",
    "lt_time = []\n",
    "nm = []\n",
    "lt_url = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#the function that will calculate the loading time\n",
    "def loadtime (list_company_website,list500_names,list500_url):\n",
    "    from time import time\n",
    "    browser2 = urllib2.build_opener()\n",
    "    browser2.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
    "    for num in range(len(list500_names)):\n",
    "        lc = str(list_company_website[num])        \n",
    "        lc = lc.replace(\"'\",\"\")   \n",
    "        lc = lc.replace(\"[\",\"\")\n",
    "        lc = lc.replace(\"]\",\"\")\n",
    "        url2 = 'http://' + lc\n",
    "        if num == 118 or num == 464:\n",
    "            #The site 118(119) has a problem and the whole code \n",
    "            #is stacking when I run it so we will thing of this \n",
    "            #site as a not downloadable\n",
    "            lt_nm.insert(num,list500_names[num])            \n",
    "            lt_time.insert(num,'n/a')\n",
    "            nm.insert(num,num)\n",
    "            lt_url.insert(num,list500_url[num])            \n",
    "        else:\n",
    "            try:\n",
    "                response2 = browser2.open(url2)\n",
    "            except Exception:\n",
    "                lt_time.insert(num,'n/a')\n",
    "                lt_nm.insert(num,list500_names[num])  \n",
    "                nm.insert(num,num)\n",
    "                print (\"The site \" + str(num)+ \" has NOT been loaded!\")\n",
    "                continue     \n",
    "            start_time = time()\n",
    "            myHTML2 = response2.read()\n",
    "            end_time = time()\n",
    "            response2.close()\n",
    "            l_t = round(end_time-start_time, 3) \n",
    "            #in order to be more readable we rounded the time\n",
    "            loadt = str(l_t)\n",
    "            lt_nm.insert(num,list500_names[num])            \n",
    "            lt_time.insert(num,loadt)\n",
    "            nm.insert(num,num)\n",
    "            lt_url.insert(num,list500_url[num])\n",
    "            #print (\"The site \" + str(num) + \" has been loaded!\")\n",
    "    print \"The function is completed!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#running the function for the first 25 sites\n",
    "loadtime (list_company_website,list500_names,list500_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#creating the data frame with the loading times\n",
    "d4 = {'company' : pd.Series(lt_nm, index=[nm]),\n",
    "      'loading time' : pd.Series(lt_time, index=[nm])}\n",
    "loading_time = pd.DataFrame(d4)    \n",
    "loading_time.head(3) #we see the first 3 in the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Find out how many and what type of images each site has\n",
    "#first we create the initially empty lists\n",
    "p_p = []\n",
    "p_d = []\n",
    "p_jpg = []\n",
    "p_jpeg = []\n",
    "p_gif = []\n",
    "p_tif = []\n",
    "p_tiff = []\n",
    "p_bmp = []\n",
    "p_jpe = []\n",
    "p_nm = []\n",
    "p_tt =[]\n",
    "nm = []\n",
    "p_url = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Then we create the function that will explore \n",
    "#the html pages and search for the images\n",
    "def images (list500_sites,list500_names,list500_url):\n",
    "    from time import time # I used it to see \n",
    "    #how much time it does to run the function\n",
    "    start = time ()\n",
    "    for num in range(len(list500_names)):\n",
    "            myHTML = list500_sites[num] \n",
    "            image = ['.png','.dib','.jpg','.jpeg',\n",
    "                     '.bmp','.jpe','.gif','.tif','.tiff'] \n",
    "            totalnumber = 0 \n",
    "            if myHTML == 0:\n",
    "                p_nm.insert(num,list500_names[num])            \n",
    "                p_p.insert(num,'n/a')  \n",
    "                p_d.insert(num,'n/a')  \n",
    "                p_jpg.insert(num,'n/a')  \n",
    "                p_jpeg.insert(num,'n/a')  \n",
    "                p_gif.insert(num,'n/a')  \n",
    "                p_tif.insert(num,'n/a')  \n",
    "                p_tiff.insert(num,'n/a')  \n",
    "                p_bmp.insert(num,'n/a')  \n",
    "                p_jpe.insert(num,'n/a')  \n",
    "                p_tt.insert(num,'n/a')\n",
    "                nm.insert(num,num)\n",
    "                p_url.insert(num,list500_url[num])          \n",
    "            else: \n",
    "                for index in range(len(image)):\n",
    "                    x = image[index]\n",
    "                    photo = re.findall(x,myHTML)\n",
    "                    if x == '.png':\n",
    "                        p = str (len(photo))\n",
    "                    if x == '.dib':\n",
    "                        d = str (len(photo))\n",
    "                    if x == '.jpg':\n",
    "                        jpg = str (len(photo))\n",
    "                    if x == '.jpeg':\n",
    "                        jpeg = str (len(photo))\n",
    "                    if x == '.gif':\n",
    "                        gif = str (len(photo))\n",
    "                    if x == '.tif':\n",
    "                        tif = str (len(photo))\n",
    "                    if x == '.tiff':\n",
    "                        tiff = str (len(photo))\n",
    "                    if x == '.bmp':\n",
    "                        bmp = str (len(photo))\n",
    "                    if x == '.jpe':\n",
    "                        jpe = str (len(photo))\n",
    "                    totalnumber = len(photo) + totalnumber\n",
    "                total = str (totalnumber)\n",
    "                p_nm.insert(num,list500_names[num])            \n",
    "                p_p.insert(num,p)  \n",
    "                p_d.insert(num,d)  \n",
    "                p_jpg.insert(num,jpg)  \n",
    "                p_jpeg.insert(num,jpeg)  \n",
    "                p_gif.insert(num,gif)  \n",
    "                p_tif.insert(num,tif)  \n",
    "                p_tiff.insert(num,tiff)  \n",
    "                p_bmp.insert(num,bmp)  \n",
    "                p_jpe.insert(num,jpe)  \n",
    "                p_tt.insert(num,total)\n",
    "                nm.insert(num,num)\n",
    "                p_url.insert(num,list500_url[num])\n",
    "    end = time ()\n",
    "    duration = round (end - start, 3)\n",
    "    minutes = round (duration /60, 1)\n",
    "    print 'The lists are ready in ', minutes, ' minutes'\n",
    "    print 'The lists are ready in ', duration, ' seconds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Then we run the function for the first 20 sites for now\n",
    "images (list500_sites,list500_names,list500_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Finally we create a dataframe in order to see the results of the function\n",
    "d5 = {'company' : pd.Series(p_nm, index=[nm]),\n",
    "      '.png' : pd.Series(p_p, index=[nm]),\n",
    "      '.dib' : pd.Series(p_d, index=[nm]),\n",
    "      '.jpg' : pd.Series(p_jpg, index=[nm]),\n",
    "      '.jpeg' : pd.Series(p_jpeg, index=[nm]),\n",
    "      '.bmp' : pd.Series(p_bmp, index=[nm]),\n",
    "      '.jpe' : pd.Series(p_jpe, index=[nm]),\n",
    "      '.gif' : pd.Series(p_gif, index=[nm]),\n",
    "      '.tif' : pd.Series(p_tif, index=[nm]),\n",
    "      '.tiff' : pd.Series(p_tiff, index=[nm]), \n",
    "      'total images' : pd.Series(p_tt, index=[nm])}\n",
    "images_types = pd.DataFrame(d5)    \n",
    "images_types.head(3) #we see the first 3 in the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now we will find the different dimensions that each site uses\n",
    "#initially we create the empty lists we will need\n",
    "nm = []\n",
    "s_comp = []\n",
    "s_dimensions = []\n",
    "s_times = []\n",
    "s_tt_dif_dim = []\n",
    "ht = [] #list of different heights in each case\n",
    "wt = [] #list of different widths in each case\n",
    "h_w = [] # combinations of height and width\n",
    "dif_size = []  \n",
    "un_size = [] \n",
    "s_url = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#With the below function we will gather \n",
    "#in a variable all the different dimensions \n",
    "#and in another one all the times that each \n",
    "#dimension occures for each html code\n",
    "def find_dif_sizes (list_company_website,list500_names,list500_url):\n",
    "    from time import time # I used it to see how much time it does to run the function\n",
    "    start = time ()\n",
    "    for num in range(len(list500_names)):\n",
    "            nm.insert(num,num)                  \n",
    "            s_comp.insert(num,list500_names[num])\n",
    "            s_url.insert(num,list500_url[num])\n",
    "            myHTML = list500_sites[num] \n",
    "            if myHTML == 0:\n",
    "                s_dimensions.insert(num,0)\n",
    "                s_times.insert(num,0)    \n",
    "            else: \n",
    "                soup = BeautifulSoup(myHTML, \"lxml\")\n",
    "                # we create 2 local variables so as to gather the \n",
    "                #different dimensions and occurencies  of each page seperately\n",
    "                s_dimensions_local = []\n",
    "                s_times_local = []\n",
    "                hw = 0 \n",
    "                # we use it for the lists of height and width\n",
    "                # find all the img in the first site html.Since in some \n",
    "                #cases either the height or the width is missing we would \n",
    "                #like to keep only the ones that have both dimensions\n",
    "                for tag in soup.find_all('img'):\n",
    "                    h = tag.attrs.get('height', None)\n",
    "                    w = tag.attrs.get('width', None)\n",
    "                    #we use if to check which ones have both \n",
    "                    if h != None:\n",
    "                        if w != None:\n",
    "                            ht.insert(hw,h)\n",
    "                            wt.insert(hw,w)\n",
    "                            hw = hw + 1                        \n",
    "                hw2 = 0\n",
    "                for l in range(len(ht)):\n",
    "                    h_w_c = ht[l] + 'x' + wt[l]    \n",
    "                    #we create a str with the form (300x300) \n",
    "                    #so as to be more easily to read later on \n",
    "                    h_w.insert(hw2,h_w_c)  \n",
    "                    #we put it in a new list\n",
    "                    hw2 = hw2 + 1    \n",
    "                if h_w == []:#we check if there are not any dimensions available\n",
    "                    nm.insert(num,num)                  \n",
    "                    s_comp.insert(num,list500_names[num])\n",
    "                    s_dimensions.insert(num,0)\n",
    "                    s_times.insert(num,0)    \n",
    "                if h_w != []:#now we continue with the cases \n",
    "                    #where the dimensions are indeed available             \n",
    "                    from collections import Counter\n",
    "                    hw_unique = Counter(h_w)\n",
    "                    hw_unique2 = str(hw_unique) \n",
    "                    #the unique different dimensions for the specific site\n",
    "                    #Due to the fact that we are talking about \n",
    "                    #a list we have to split the parts we need \n",
    "                    split1 = hw_unique2.split('{')\n",
    "                    a = split1[1]\n",
    "                    split2 = a.split('}')\n",
    "                    b = split2[0]\n",
    "                    split3 = b.split(',')\n",
    "                    finalsplit = []\n",
    "                    fs = []\n",
    "                    z = 0\n",
    "                    m = 1\n",
    "                    j = 0\n",
    "                    z1 = 0\n",
    "                    m1 = 1\n",
    "                    #each of the items in split3 has a form '300x300 : 15'\n",
    "                    #and in order to create the dataframe we have \n",
    "                    #to split this form and keep the informations in different list\n",
    "                    for numb in split3:                \n",
    "                        oldstring = numb\n",
    "                        newstring = oldstring.replace(\"'\", \"\")\n",
    "                        new = newstring.replace(\"'\",\"\")\n",
    "                        string = new.replace(\" \",\"\")\n",
    "                        finalstring = string.split(':')\n",
    "                        #the finalstring is a list that contains the dimensions\n",
    "                        #and the occurencies in order toseperate in different\n",
    "                        #lists we create an additional loop\n",
    "                        for xx in range(len(finalstring)):\n",
    "                            ax = finalstring[xx]\n",
    "                            if 'x' in ax:\n",
    "                                s_dimensions_local.insert(z1,finalstring[xx])\n",
    "                                z1 = z1 + 1\n",
    "                            else:\n",
    "                                s_times_local.insert(m1,finalstring[xx])\n",
    "                                m1 = m1 + 1  \n",
    "                    #Now we can add to the lists the parts we created so as\n",
    "                    #to have them all gathered together             \n",
    "                    s_dimensions.insert(num,s_dimensions_local)\n",
    "                    s_times.insert(num,s_times_local)                \n",
    "    end = time ()\n",
    "    duration = round (end - start, 3)\n",
    "    minutes = round (duration /60, 1)\n",
    "    print 'The lists are ready in ', minutes, ' minutes'\n",
    "    print 'The lists are ready in ', duration, ' seconds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Run the function for the first 20 sites\n",
    "find_dif_sizes (list500_sites,list500_names,list500_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Find the unique different image dimensions and put them on a list\n",
    "def unique_dif_sizes (s_dimensions,list500_names):\n",
    "    ds = 0\n",
    "    for num in range(len(list500_names)):\n",
    "        asw = s_dimensions[num]\n",
    "        if asw != 0 :\n",
    "            for s in range(len(asw)):\n",
    "                ss = asw[s]\n",
    "                dif_size.insert(ds,ss)\n",
    "                ds = ds + 1    \n",
    "    dsu = 0\n",
    "    for i in dif_size:\n",
    "        if i not in un_size:\n",
    "            un_size.insert(dsu,i)\n",
    "            dsu = dsu + 1             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Run the function unique_dif_sizes\n",
    "unique_dif_sizes (s_dimensions,list500_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The lists we will need for the next function\n",
    "t_f_s = []\n",
    "ttf = []\n",
    "nm = []\n",
    "com = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function in order to check whether or not each \n",
    "#company has these dimensions\n",
    "def dimensions_per_company (un_size,list500_names):\n",
    "    from time import time \n",
    "    # I used it to see how much time it does to run the function\n",
    "    start = time ()\n",
    "    #t_f_s.insert(0,un_size)\n",
    "    #ttf.insert(0,t_f_s)\n",
    "    for num in range(len(list500_names)): \n",
    "        #print(str(num))\n",
    "        s1a = s_dimensions[num] \n",
    "        #dimensions of site num\n",
    "        where = [] #empty list\n",
    "        wh = 0\n",
    "        haveornot = []\n",
    "        for er in range (len(un_size)):\n",
    "            if s1a != 0 :\n",
    "                for sizea in s1a:\n",
    "                    if sizea == un_size[er]:\n",
    "                        where.insert(wh,str(er))\n",
    "                        wh = wh +1\n",
    "                        break\n",
    "            if str(er) in where:\n",
    "                haveornot.insert(er,True)                    \n",
    "            else:\n",
    "                haveornot.insert(er,False)\n",
    "                    \n",
    "        t_f_s.insert(num,haveornot)\n",
    "        ttf.insert(num,t_f_s)\n",
    "        nm.insert(num,num)\n",
    "        com.insert(num,list500_names[num])\n",
    "    end = time ()\n",
    "    duration = round (end - start, 3)\n",
    "    minutes = round (duration /60, 1)\n",
    "    print 'The lists are ready in ', minutes, ' minutes'\n",
    "    print 'The lists are ready in ', duration, ' seconds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Run the function dimensions_per_company\n",
    "dimensions_per_company (un_size,list500_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create an initial dataframe where we will add the sizes later on\n",
    "d6 = {'company' : pd.Series(com, index=[nm])}\n",
    "sizess = pd.DataFrame(d6)    \n",
    "sizess.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Now we want to break the variable t_f_s \n",
    "#in order to add the columns to the dataframe                  \n",
    "#Finally we create the data frame with the elements we found \n",
    "def final_dimensions_dataframe (un_size,t_f_s,list500_names):\n",
    "    from time import time \n",
    "    # I used it to see how much time it does to run the function\n",
    "    start = time ()\n",
    "    for q in range(len(un_size)):\n",
    "        names = un_size[q]\n",
    "        var = []\n",
    "        for num in range(len(list500_names)):\n",
    "            a = t_f_s[num]\n",
    "            var.insert(num,a[q])\n",
    "        sizess[names] = pd.Series(var, index=sizess.index) \n",
    "    end = time ()\n",
    "    duration = round (end - start, 3)\n",
    "    minutes = round (duration /60, 1)\n",
    "    print 'The lists are ready in ', minutes, ' minutes'\n",
    "    print 'The lists are ready in ', duration, ' seconds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Run the function final_dimensions_dataframe\n",
    "final_dimensions_dataframe (un_size,t_f_s,list500_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sizess.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#In order to validate the html code we will use the w3 validator\n",
    "#We will validate each url and then we will open the url of the validation page\n",
    "#so as to extract the errors, the info warnings and the non-document-error io informations \n",
    "#First we create the empty lists we would use later on\n",
    "num_errors = []\n",
    "num_info_warnings = []\n",
    "num_non_doc = [] \n",
    "nm = []\n",
    "num_open_page = []\n",
    "empty = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Then we create the function that will pull the informations we want\n",
    "def html_validation (list500_url,list500_names):\n",
    "    from time import time # I used it to see how much time it does to run the function\n",
    "    start = time ()\n",
    "    for num in range(len(list500_names)):\n",
    "        line = list500_url[num] \n",
    "        url_check = \"https://validator.w3.org/nu/?doc=https://\" + line\n",
    "        browser = urllib2.build_opener()\n",
    "        browser.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
    "        response = browser.open(url_check)\n",
    "        html_check = response.read()\n",
    "        html_check\n",
    "        check = str(html_check)\n",
    "        er = 0\n",
    "        err = 0\n",
    "        errr = 0\n",
    "        e = False\n",
    "        if check != empty:\n",
    "            e = True\n",
    "            soup = BeautifulSoup(check,\"lxml\")\n",
    "            o = 0\n",
    "            keyf = []\n",
    "            for row in soup.html.body.findAll('div'):\n",
    "                keyf.insert(o,row)\n",
    "                o = o + 1\n",
    "            #print(len(keyf),list500_url[num], \"site number: \", str(num))        \n",
    "            if len(keyf) != 0:       \n",
    "                    keyfin = str(keyf[2]) \n",
    "                    #the elements we need is in the 2nd div of the code\n",
    "                    dol= re.findall('class=\"error\"',keyfin)            \n",
    "                    er = er + len(dol)\n",
    "                    doll= re.findall('class=\"info warning\"'\n",
    "                                     ,keyfin)            \n",
    "                    err = err + len(doll)\n",
    "                    dolll= re.findall('class=\"non-document-error io\"'\n",
    "                                      ,keyfin)            \n",
    "                    errr = errr + len(dolll)\n",
    "        num_errors.insert(num,er)\n",
    "        num_info_warnings.insert(num,err)\n",
    "        num_non_doc.insert(num,errr)  \n",
    "        nm.insert(num,num) \n",
    "        num_open_page.insert(num,e)\n",
    "    end = time ()\n",
    "    duration = round (end - start, 3)\n",
    "    minutes = round (duration /60, 1)\n",
    "    print 'The lists are ready in ', minutes, ' minutes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Now we will run the function we created\n",
    "html_validation (list500_url,list500_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#After the checks we will create the dataframe with the informations we want\n",
    "d8 = {'company' : pd.Series(list500_names, index=[nm]),\n",
    "      'The_page_opened' : pd.Series(num_open_page, index=[nm])\n",
    "      ,'number_of_errors' : pd.Series(num_errors, index=[nm]),\n",
    "      'number_of_warning' : pd.Series(num_info_warnings, index=[nm])\n",
    "      ,'non-document-error' : pd.Series(num_non_doc, index=[nm])}\n",
    "html_val = pd.DataFrame(d8)    \n",
    "html_val.head(3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The next step is to take some informations from the fortune 500 site for each company\n",
    "#In order to achieve that we should open the pages for each one of the sites seperately\n",
    "#Since there is a pattern in the way the pages are named it shouldn't be difficult\n",
    "#Firstly we should create the pattern with which we will download the pages\n",
    "#By running the code we can see that the names of each comany are not \n",
    "#written exactly as we have saved them\n",
    "#So we do need to alter the names first in order for the below function to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creating a new list with alterations in order for the names\n",
    "#to match the ones that fortune 500 uses so that we can download the html page\n",
    "list_company_name_new = []\n",
    "for num in range (0,500):\n",
    "    cn = list_company_name[num]\n",
    "    cn = cn.replace(\" \", \"-\")\n",
    "    cn = cn.replace(\"&\", \"\")\n",
    "    cn = cn.replace(\"\", \"\")\n",
    "    cn = cn.replace(\".\", \"-\")\n",
    "    cn = cn.replace(\"amp;\", \"\")    \n",
    "    company = cn.lower()\n",
    "    list_company_name_new.insert(num,cn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fortune_pages = []\n",
    "def fortune500 (list_company_name_new):\n",
    "    from time import time # I used it to see how much time it does to run the function\n",
    "    start = time ()\n",
    "    for num3 in range (0,500):\n",
    "        i = str (num3 +1)    \n",
    "        companyname =  list_company_name_new[num3]\n",
    "        browser = urllib2.build_opener() \n",
    "        #because i work from different computers with different \n",
    "        #pyhton version some commands are not recognizable in each version\n",
    "        browser.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
    "        site_fortune = \"http://beta.fortune.com/fortune500/\"+companyname+\"-\"+ i    \n",
    "        page_fortune = browser.open(site_fortune)\n",
    "        html_fortune = page_fortune.read()    \n",
    "        #print(\"fortune page for company: \", list_company_name_new[num3],i)\n",
    "        fortune_pages.insert(num3, html_fortune)\n",
    "    end = time ()\n",
    "    duration = round (end - start, 3)\n",
    "    minutes = round (duration /60, 1)\n",
    "    print 'The lists are ready in ', minutes, ' minutes'\n",
    "    print 'The lists are ready in ', duration, ' seconds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Run the function we created\n",
    "fortune500 (list_company_name_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now that we have opened the url we are going to extract \n",
    "#some informations that we need from them\n",
    "#In order to do that initially we have to create \n",
    "#the variables we will need\n",
    "keyf =[]\n",
    "per =[]\n",
    "rev_dol = []\n",
    "rev_per = []\n",
    "prof_dol = []\n",
    "prof_per = []\n",
    "assets_dol = []\n",
    "assets_per = []\n",
    "tse_dol = []\n",
    "tse_per = []\n",
    "mar_dol = []\n",
    "mar_per = []\n",
    "market = []\n",
    "nm = []\n",
    "ln = []\n",
    "urln = []\n",
    "empty = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fortune_metrics (list_company_name,list_company_website):\n",
    "    x = 0\n",
    "    for n in range (0,500):   #we put 25 for testing\n",
    "        nm.insert(x,x)\n",
    "        ln.insert(x,list_company_name[n])\n",
    "        urln.insert(x,list_company_website[n])\n",
    "        files = fortune_pages[x]\n",
    "        soup = BeautifulSoup(files,\"lxml\")\n",
    "        o=0\n",
    "        for row in soup.html.body.findAll('tbody'):\n",
    "            keyf.insert(o,row)\n",
    "            o=o+1\n",
    "        keyfin = keyf[0] \n",
    "        #the elements we need is in the first tbody of the code\n",
    "        data = keyfin.findAll('td')\n",
    "\n",
    "        one = str(data[0]) \n",
    "        # revenue\n",
    "        two = str(data[1]) \n",
    "        # revenue in dollars we need to extract this\n",
    "        revdol= re.findall('>\\$(.+?)</td>',two) \n",
    "        #we keep only the numbers\n",
    "        if revdol[0] != empty:\n",
    "            w = revdol[0]\n",
    "            a = w.replace(\"[\", \"\")\n",
    "            r = a.replace(\"]\",\"\")\n",
    "            rev_dol.insert(x,r)\n",
    "        else:\n",
    "            rev_dol.insert(x,'not available')\n",
    "        tria = str(data[2])\n",
    "        # revenue in percentage we need to extract this as well\n",
    "        revper= re.findall('>(.+?)%</td>',tria) \n",
    "        #we keep only the numbers\n",
    "        if revper != empty:    \n",
    "            w = revper[0]\n",
    "            a = w.replace(\"[\", \"\")\n",
    "            r1 = a.replace(\"]\",\"\")    \n",
    "            rev_per.insert(x,r1) \n",
    "        else:\n",
    "            rev_per.insert(x,'not available')\n",
    "        four = str(data[3])   # profit     \n",
    "        five = str(data[4])   \n",
    "        # profit in dollars we need to extract this   \n",
    "        profdol= re.findall('>\\$(.+?)</td>',five) \n",
    "        #we keep only the numbers\n",
    "        if profdol != empty:\n",
    "            w = profdol[0]\n",
    "            a = w.replace(\"[\", \"\")\n",
    "            p = a.replace(\"]\",\"\")\n",
    "            prof_dol.insert(x,p)\n",
    "        else:\n",
    "            prof_dol.insert(x,'not available')\n",
    "        six = str(data[5])    \n",
    "        # profit in percentage we need to extract this as well   \n",
    "        profper = re.findall('>(.+?)%</td>',six) \n",
    "        #we keep only the numbers\n",
    "        if profper != empty:\n",
    "            w = profper[0]\n",
    "            a = w.replace(\"[\", \"\")\n",
    "            p1 = a.replace(\"]\",\"\")    \n",
    "            prof_per.insert(x,p1)\n",
    "        else:\n",
    "            prof_per.insert(x,'not available')\n",
    "        seven = str(data[6]) #assets\n",
    "        eight = str(data[7]) #assets in dollars we need to extract this\n",
    "        assetsdol= re.findall('>\\$(.+?)</td>',eight) \n",
    "        #we keep only the numbers\n",
    "        if assetsdol != empty:\n",
    "            w = assetsdol[0]\n",
    "            a = w.replace(\"[\", \"\")\n",
    "            ass = a.replace(\"]\",\"\")\n",
    "            assets_dol.insert(x,ass)\n",
    "        else:\n",
    "            assets_dol.insert(x,'not available')\n",
    "        ten = str(data[9]) #Total Stockholder Equity ($M)    \n",
    "        eleven = str(data[10]) \n",
    "        #Total Stockholder Equity ($M) in dollars we need to extract this\n",
    "        tsedol= re.findall('>\\$(.+?)</td>',eleven) \n",
    "        #we keep only the numbers\n",
    "        if tsedol != empty:\n",
    "            w = tsedol[0]\n",
    "            a = w.replace(\"[\", \"\")\n",
    "            ts = a.replace(\"]\",\"\")\n",
    "            tse_dol.insert(x,ts)\n",
    "        else:\n",
    "            tse_dol.insert(x,'not available')\n",
    "        thirteen = str(data[12]) # market value\n",
    "        fourteen = str(data[13]) \n",
    "        # market value in dollars we need to extract this\n",
    "        mardol= re.findall('>\\$(.+?)</td>',fourteen) \n",
    "        #we keep only the numbers\n",
    "        if mardol != empty:\n",
    "            w = mardol[0]\n",
    "            a = w.replace(\"[\", \"\")\n",
    "            mar = a.replace(\"]\",\"\")\n",
    "            mar_dol.insert(x,mar)\n",
    "        else:\n",
    "            mar_dol.insert(x,'not available')\n",
    "        x = x + 1\n",
    "    print \"The function is complete!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fortune_metrics (list_company_name,list_company_website)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d9 = {'company' : pd.Series(ln, index=[nm]),\n",
    "      'Revenues $' : pd.Series(rev_dol, index=[nm]),\n",
    "      'Revenues %' : pd.Series(rev_per, index=[nm]),\n",
    "      'Assets $' : pd.Series(assets_dol, index=[nm]),\n",
    "      'Total Stockholder Equity $' : pd.Series(tse_dol, index=[nm]),\n",
    "      'Market value $' : pd.Series(mar_dol, index=[nm])}\n",
    "fort500 = pd.DataFrame(d9)    \n",
    "fort500.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = pd.merge(fort500, html_val, how='inner', on=['company', 'company'])\n",
    "result2 = pd.merge(social_media, fre, how='inner', on=['company', 'company'])\n",
    "result3 = pd.merge(sites_links, sizess, how='inner', on=['company', 'company'])\n",
    "result4 = pd.merge(images_types, loading_time, how='inner', on=['company', 'company'])\n",
    "result5 = pd.merge(result,result2 , how='inner', on=['company', 'company'])\n",
    "result6 = pd.merge(result3, result4, how='inner', on=['company', 'company'])\n",
    "final = pd.merge(result5, result6, how='inner', on=['company', 'company'])\n",
    "final.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final.to_csv('total_500_new.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data500 = pd.read_csv(\"total_500_new.csv\", sep=';') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data500.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
