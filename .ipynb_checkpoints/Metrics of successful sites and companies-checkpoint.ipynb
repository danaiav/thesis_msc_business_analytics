{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#First we import the libraries we will need\n",
    "import urllib\n",
    "import urllib.request\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#First of all we need to find all the name of the sites that belong to fortune 500. This can happen if we seperate\n",
    "#The information needed from the below link\n",
    "url = \"http://www.zyxware.com/articles/4344/list-of-fortune-500-companies-and-their-websites\"\n",
    "list_company_number =[]\n",
    "list_company_name = []\n",
    "list_company_website = []\n",
    "list500_sites = []\n",
    "list500_names = []\n",
    "list500_num = []\n",
    "list500_url = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#In order to extract the needed informations we will create 3 lists. The first one will contain the rank of each site, the\n",
    "#second one will contain the name of the company and the 3rd one will contain the actual link of the company's site.\n",
    "#For achieving this purpose we will create a funstion that will in its turn create those three list.\n",
    "#In order to know if the function worked we will ask it to return the first element of each list along with a sentence.\n",
    "def websites (url):\n",
    "    browser = urllib.request.build_opener()\n",
    "    browser.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
    "    response = browser.open(url)# this might throw an exception if something goes wrong.\n",
    "    myHTML = response.read()\n",
    "    soup = BeautifulSoup(myHTML,\"lxml\")    \n",
    "    o = 0\n",
    "    td_list =[]\n",
    "    for row2 in soup.html.body.findAll('td'):\n",
    "        td_list.insert(o, row2)\n",
    "        o = o + 1\n",
    "    a = 0\n",
    "    b = 1\n",
    "    c = 2\n",
    "    list_numbering = 0\n",
    "    for i in range (0,500):        \n",
    "        num = str(td_list[a])\n",
    "        company = str(td_list[b])\n",
    "        site = str(td_list[c])\n",
    "        c_num = re.findall('>(.+?)</td>',num)  \n",
    "        c_num = str(c_num[0])\n",
    "        c_name = re.findall('>(.+?)</td>',company)\n",
    "        c_name = str(c_name[0])\n",
    "        c_site = re.findall('\">(.+?)</a>',site)\n",
    "        c_site = str(c_site[0])        \n",
    "        list_company_number.insert(list_numbering,c_num)\n",
    "        list_company_name.insert(list_numbering,c_name)\n",
    "        list_company_website.insert(list_numbering,c_site)\n",
    "        a = a + 3\n",
    "        b = b + 3\n",
    "        c = c + 3\n",
    "        list_numbering =  list_numbering + 1 \n",
    "    print ('The three lists are ready')\n",
    "    print (list_company_number[0])\n",
    "    print (list_company_name[0])\n",
    "    print (list_company_website[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The three lists are ready\n",
      "1\n",
      "Walmart\n",
      "www.walmart.com\n"
     ]
    }
   ],
   "source": [
    "# After creating the function we should now test that it actually works correctly\n",
    "websites (url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## The next step is to download and save in a new list the 500 sites.\n",
    "## We will create a new function that will do this by reading the list \"list_company_websites\" that we created \n",
    "## with the previous function\n",
    "\n",
    "def list_company_HTML (list_company_website,list_company_name,start,end):\n",
    "    import time\n",
    "    browser2 = urllib.request.build_opener()\n",
    "    browser2.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
    "    for i in range (start,end):\n",
    "        k = str(i+1)\n",
    "        lc = str(list_company_website[i])        \n",
    "        lc = lc.replace(\"'\",\"\")   \n",
    "        lc = lc.replace(\"[\",\"\")\n",
    "        lc = lc.replace(\"]\",\"\")\n",
    "        lcn = str(list_company_name[i])        \n",
    "        lcn = lcn.replace(\"'\",\"\")   \n",
    "        lcn = lcn.replace(\"[\",\"\")\n",
    "        lcn = lcn.replace(\"]\",\"\")\n",
    "        url2= 'http://'+lc\n",
    "        #an exception might be thrown, so the code should be in a try-except block\n",
    "        try:\n",
    "            #use the browser to get the url.\n",
    "            response2=browser2.open(url2)# this might throw an exception if something goes wrong.\n",
    "        except Exception: # this describes what to do if an exception is thrown\n",
    "             continue#ignore this page.      \n",
    "        #read the response in html format. This is essentially a long piece of text\n",
    "        myHTML2=response2.read()\n",
    "        list500_sites.insert(i,myHTML2)\n",
    "        list500_names.insert(i,lcn)\n",
    "        list500_url.insert(i,lc)\n",
    "        list500_num.insert(i,k)\n",
    "        #wait for 2 seconds\n",
    "        time.sleep(2)\n",
    "        #print (\"The site \" + k + \" has been downloaded!\")\n",
    "    print (\"We downloaded: \",len(list500_sites),\" sites!\")\n",
    "    #print (len(list500_names),list500_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "25\n",
      "We downloaded:  25  sites!\n"
     ]
    }
   ],
   "source": [
    "start = 0\n",
    "stop = 25\n",
    "for i in range (0,1): #i am not downloading all the pages for the time being for reasons of running faster the process during the tests\n",
    "    print (start)\n",
    "    print (stop)    \n",
    "    list_company_HTML (list_company_website,list_company_name,start,stop)\n",
    "    start = stop\n",
    "    stop = stop + 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Retreiving the social media from each site\n",
    "#First create empty lists for the ones that we will need to calculate\n",
    "sm_f = []\n",
    "sm_t = []\n",
    "sm_i = []\n",
    "sm_p = []\n",
    "sm_y = []\n",
    "sm_l = []   \n",
    "sm_nm = [] \n",
    "nm = []\n",
    "sm_url = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Then create a function that will feel in those lists so as to make the data frame later on\n",
    "\n",
    "def socialmedia (list500_sites,list500_names,list500_url,start,end):\n",
    "    for num in range(start,end):\n",
    "        if list500_sites[num] != \"\":\n",
    "            hand = str(list500_sites[num])\n",
    "            sm = ['facebook.com','twitter.com','instagram.com','pinterest.com','youtube.com','linkedin.com'] \n",
    "            number = 0 \n",
    "            for index in range(len(sm)):\n",
    "                x = sm[index]\n",
    "                photo = re.findall(x,hand)                                \n",
    "                if (len(photo)>0):\n",
    "                    if x == 'facebook.com':\n",
    "                        answerf = 'yes'\n",
    "                    if x == 'twitter.com':\n",
    "                        answert = 'yes'\n",
    "                    if x == 'instagram.com':\n",
    "                        answeri = 'yes'\n",
    "                    if x == 'pinterest.com':\n",
    "                        answerp = 'yes'\n",
    "                    if x == 'youtube.com':\n",
    "                        answery = 'yes'\n",
    "                    if x =='linkedin.com':\n",
    "                        answerl = 'yes'                   \n",
    "                else:\n",
    "                     if x == 'facebook.com':\n",
    "                        answerf = 'no'\n",
    "                     if x == 'twitter.com':\n",
    "                        answert = 'no'\n",
    "                     if x == 'instagram.com':\n",
    "                        answeri = 'no'\n",
    "                     if x == 'pinterest.com':\n",
    "                        answerp = 'no'\n",
    "                     if x == 'youtube.com':\n",
    "                        answery = 'no'\n",
    "                     if x =='linkedin.com':\n",
    "                        answerl = 'no'                 \n",
    "            sm_nm.insert(num,list500_names[num]) \n",
    "            nm.insert(num,num)\n",
    "            sm_url.insert(num,list500_url[num])\n",
    "            sm_f.insert(num,answerf)\n",
    "            sm_t.insert(num,answert)\n",
    "            sm_i.insert(num,answeri)\n",
    "            sm_p.insert(num,answerp)\n",
    "            sm_y.insert(num,answery)\n",
    "            sm_l.insert(num,answerl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Now we will run the function for the 20 first sites for starters\n",
    "socialmedia (list500_sites,list500_names,list500_url,0,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>facebook</th>\n",
       "      <th>instagram</th>\n",
       "      <th>linkedin</th>\n",
       "      <th>pinterest</th>\n",
       "      <th>twitter</th>\n",
       "      <th>url</th>\n",
       "      <th>youtube</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Walmart</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>www.walmart.com</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Exxon Mobil</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>www.exxonmobil.com</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Apple</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>www.apple.com</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       company facebook instagram linkedin pinterest twitter  \\\n",
       "0      Walmart      yes       yes       no       yes     yes   \n",
       "1  Exxon Mobil       no        no       no        no      no   \n",
       "2        Apple       no        no      yes        no      no   \n",
       "\n",
       "                  url youtube  \n",
       "0     www.walmart.com     yes  \n",
       "1  www.exxonmobil.com      no  \n",
       "2       www.apple.com     yes  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finally we create the data frame with the elements we found            \n",
    "d = {'company' : pd.Series(sm_nm, index=[nm]),'url' : pd.Series(sm_url, index=[nm]),\n",
    "     'facebook' : pd.Series(sm_f, index=[nm]),'twitter' : pd.Series(sm_t, index=[nm]),\n",
    "     'instagram' : pd.Series(sm_i, index=[nm]),'pinterest' : pd.Series(sm_p, index=[nm]),\n",
    "     'youtube' : pd.Series(sm_y, index=[nm]),'linkedin' : pd.Series(sm_l, index=[nm]),}\n",
    "social_media = pd.DataFrame(d)    \n",
    "social_media.head(3) #we see the first 3 in the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create the lists we will need for the data frame\n",
    "l_nm = []\n",
    "l_ex = []\n",
    "l_in = []\n",
    "l_t = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create the function that will calculate the different type of links\n",
    "def links (list500_sites,list500_names,start,end):\n",
    "    for num in range(start,end):\n",
    "        line = str(list500_sites[num])\n",
    "        href = re.findall('href',line)\n",
    "        external = re.findall('href=\"https:',line)\n",
    "        ex = (len(external))\n",
    "        alllinks = (len(href))\n",
    "        internal =  (len(href) - len(external))\n",
    "        l_nm.insert(num,list500_names[num])            \n",
    "        l_ex.insert(num,ex)\n",
    "        l_t.insert(num,alllinks)\n",
    "        l_in.insert(num,internal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Run the function in order to find the external, internal and total links of each site\n",
    "#For now we are running for the first 20 sites only\n",
    "links (list500_sites,list500_names,0,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>external</th>\n",
       "      <th>internal</th>\n",
       "      <th>total links</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Walmart</th>\n",
       "      <td>67</td>\n",
       "      <td>98</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Exxon Mobil</th>\n",
       "      <td>23</td>\n",
       "      <td>783</td>\n",
       "      <td>806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Apple</th>\n",
       "      <td>5</td>\n",
       "      <td>277</td>\n",
       "      <td>282</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            external internal total links\n",
       "Walmart           67       98         165\n",
       "Exxon Mobil       23      783         806\n",
       "Apple              5      277         282"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a dataframe so as to be able to see the results of the function we run\n",
    "d2 = {'external' : pd.Series(l_ex, index=[l_nm]),'internal' : pd.Series(l_in, index=[l_nm]),\n",
    "     'total links' : pd.Series(l_t, index=[l_nm])}\n",
    "sites_links = pd.DataFrame(d2)    \n",
    "sites_links.head(3) #we see the first 3 in the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The initial lists we will need in order to calculate the loading time\n",
    "lt_nm = [] \n",
    "lt_time = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#the function that will calculate the loading time\n",
    "def loadtime (list_company_website,list500_names,start,end):\n",
    "    from time import time\n",
    "    browser2 = urllib.request.build_opener()\n",
    "    browser2.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
    "    for num in range(start,end):\n",
    "        lc = str(list_company_website[num])        \n",
    "        lc = lc.replace(\"'\",\"\")   \n",
    "        lc = lc.replace(\"[\",\"\")\n",
    "        lc = lc.replace(\"]\",\"\")\n",
    "        url2= 'http://'+lc        \n",
    "        try:\n",
    "            response2=browser2.open(url2)\n",
    "        except Exception: \n",
    "             continue#     \n",
    "        start_time = time()\n",
    "        myHTML2=response2.read()\n",
    "        end_time = time()\n",
    "        response2.close()\n",
    "        l_t = round(end_time-start_time, 3) #in order to be more redable we rounded the time\n",
    "        loadt = str(l_t)\n",
    "        lt_nm.insert(num,list500_names[num])            \n",
    "        lt_time.insert(num,loadt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#running the function for the first 20 sites\n",
    "loadtime (list_company_website,list500_names,0,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loading time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Walmart</th>\n",
       "      <td>0.508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Exxon Mobil</th>\n",
       "      <td>3.829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Apple</th>\n",
       "      <td>0.075</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            loading time\n",
       "Walmart            0.508\n",
       "Exxon Mobil        3.829\n",
       "Apple              0.075"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating the data frame with the loading times\n",
    "d3 = {'loading time' : pd.Series(lt_time, index=[lt_nm])}\n",
    "loading_time = pd.DataFrame(d3)    \n",
    "loading_time.head(3) #we see the first 3 in the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Find out how many and what type of images each site has\n",
    "#first we create the initially empty lists\n",
    "p_p = []\n",
    "p_d = []\n",
    "p_jpg = []\n",
    "p_jpeg = []\n",
    "p_gif = []\n",
    "p_tif = []\n",
    "p_tiff = []\n",
    "p_bmp = []\n",
    "p_jpe = []\n",
    "p_nm = []\n",
    "p_tt =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Then we create the function that will explore the html pages and search for the images\n",
    "def images (list500_sites,list500_names,start,end):\n",
    "    for num in range(start,end):\n",
    "        line = str(list500_sites[num])\n",
    "        image = ['.png','.dib','.jpg','.jpeg','.bmp','.jpe','.gif','.tif','.tiff'] \n",
    "        totalnumber = 0 \n",
    "        for index in range(len(image)):\n",
    "            x = image[index]\n",
    "            photo = re.findall(x,line)\n",
    "            if x == '.png':\n",
    "                p = str (len(photo))\n",
    "            if x == '.dib':\n",
    "                d = str (len(photo))\n",
    "            if x == '.jpg':\n",
    "                jpg = str (len(photo))\n",
    "            if x == '.jpeg':\n",
    "                jpeg = str (len(photo))\n",
    "            if x == '.gif':\n",
    "                gif = str (len(photo))\n",
    "            if x == '.tif':\n",
    "                tif = str (len(photo))\n",
    "            if x == '.tiff':\n",
    "                tiff = str (len(photo))\n",
    "            if x == '.bmp':\n",
    "                bmp = str (len(photo))\n",
    "            if x == '.jpe':\n",
    "                jpe = str (len(photo))\n",
    "            totalnumber = len(photo) + totalnumber\n",
    "        total = str (totalnumber)\n",
    "        p_nm.insert(num,list500_names[num])            \n",
    "        p_p.insert(num,p)  \n",
    "        p_d.insert(num,d)  \n",
    "        p_jpg.insert(num,jpg)  \n",
    "        p_jpeg.insert(num,jpeg)  \n",
    "        p_gif.insert(num,gif)  \n",
    "        p_tif.insert(num,tif)  \n",
    "        p_tiff.insert(num,tiff)  \n",
    "        p_bmp.insert(num,bmp)  \n",
    "        p_jpe.insert(num,jpe)  \n",
    "        p_tt.insert(num,total)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Then we run the function for the first 20 sites for now\n",
    "images (list500_sites,list500_names,0,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>.bmp</th>\n",
       "      <th>.dib</th>\n",
       "      <th>.gif</th>\n",
       "      <th>.jpe</th>\n",
       "      <th>.jpeg</th>\n",
       "      <th>.jpg</th>\n",
       "      <th>.png</th>\n",
       "      <th>.tif</th>\n",
       "      <th>.tiff</th>\n",
       "      <th>total images</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Walmart</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>103</td>\n",
       "      <td>103</td>\n",
       "      <td>79</td>\n",
       "      <td>46</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Exxon Mobil</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Apple</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            .bmp .dib .gif .jpe .jpeg .jpg .png .tif .tiff total images\n",
       "Walmart        0    0   22  103   103   79   46    6     0          359\n",
       "Exxon Mobil    0    0    1    0     0   16    2    4     0           23\n",
       "Apple          0    0    1    0     0    0    2    0     0            3"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finally we create a dataframe in order to see the results of the function\n",
    "d4 = {'.png' : pd.Series(p_p, index=[p_nm]),'.dib' : pd.Series(p_d, index=[p_nm]),\n",
    "'.jpg' : pd.Series(p_jpg, index=[p_nm]),'.jpeg' : pd.Series(p_jpeg, index=[p_nm]),\n",
    "'.bmp' : pd.Series(p_bmp, index=[p_nm]),'.jpe' : pd.Series(p_jpe, index=[p_nm]),\n",
    "'.gif' : pd.Series(p_gif, index=[p_nm]),'.tif' : pd.Series(p_tif, index=[p_nm]),\n",
    "'.tiff' : pd.Series(p_tiff, index=[p_nm]), 'total images' : pd.Series(p_tt, index=[p_nm])}\n",
    "images_types = pd.DataFrame(d4)    \n",
    "images_types.head(3) #we see the first 3 in the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now we will find the different dimensions that each site uses\n",
    "#initially we create the empty lists we will need\n",
    "nm = []\n",
    "s_comp = []\n",
    "s_dimensions = []\n",
    "s_times = []\n",
    "s_tt_dif_dim = []\n",
    "ht = [] #list of different heights in each case\n",
    "wt = [] #list of different widths in each case\n",
    "h_w = [] # combinations of height and width\n",
    "dif_size = []  \n",
    "un_size = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#With the below function we will gather in a variable all the different dimensions \n",
    "#and in another one all the times that each dimension occures for each html code\n",
    "def find_dif_sizes (list500_sites,list500_names,start,end):\n",
    "    for num in range(start,end):     \n",
    "        line = str(list500_sites[num]) #read each line of the list500_sites that contains each html code\n",
    "        soup = BeautifulSoup(line, \"lxml\")\n",
    "        # we create 2 local variables so as to gather the different dimensions and occurencies  of each page seperately\n",
    "        s_dimensions_local = []\n",
    "        s_times_local = []\n",
    "        hw = 0 # we use it for the lists of height and width\n",
    "        for tag in soup.find_all('img'): # find all the img in the first site html\n",
    "             print (tag.attrs.get('height', None), tag.attrs.get('width', None))\n",
    "            #Since in some cases either the height or the width is missing we would like to keep only the ones that have both dimensions\n",
    "             h = tag.attrs.get('height', None)\n",
    "             w = tag.attrs.get('width', None)\n",
    "             #we use if to check which ones have both        \n",
    "             if h != None:\n",
    "                 if w != None:\n",
    "                     ht.insert(hw,h)\n",
    "                     wt.insert(hw,w)\n",
    "                     hw = hw+1                        \n",
    "        hw2 = 0\n",
    "        for l in range(len(ht)):\n",
    "             h_w_c = ht[l] + 'x' + wt[l]    #we create a str with the form (300x300) so as to be more easily to read later on \n",
    "             h_w.insert(hw2,h_w_c)  #we put it in a new list\n",
    "             hw2 = hw2 + 1    \n",
    "        if h_w == []:#we check if there are not any dimensions available\n",
    "             nm.insert(num,num)                  \n",
    "             s_comp.insert(num,list500_names[num])\n",
    "             s_dimensions.insert(num,0)\n",
    "             s_times.insert(num,0)\n",
    "             print(\"No photos for \"+ list500_names[num]+\" so we go to the next site\")    \n",
    "        if h_w != []:#now we continue with the cases where the dimensions are indeed available             \n",
    "             from collections import Counter\n",
    "             hw_unique = Counter(h_w)\n",
    "             hw_unique2 = str(hw_unique) #the unique different dimensions for the specific site\n",
    "            #Due to the fact that we are talking about a list we have to split the parts we need \n",
    "             split1 = hw_unique2.split('{')\n",
    "             a=split1[1]\n",
    "             split2 =a.split('}')\n",
    "             b=split2[0]\n",
    "             split3 = b.split(',')\n",
    "             print(split3)\n",
    "             finalsplit=[]\n",
    "             fs = []\n",
    "             z=0\n",
    "             m=1\n",
    "             j = 0\n",
    "             z1=0\n",
    "             m1=1\n",
    "            #each of the items in split3 has a form '300x300 : 15' and in order to create the dataframe we have \n",
    "            #to split this form and keep the informations in different list\n",
    "             for numb in split3: \n",
    "                 print(numb)\n",
    "                 oldstring = numb\n",
    "                 newstring = oldstring.replace(\"'\", \"\")\n",
    "                 new = newstring.replace(\"'\",\"\")\n",
    "                 string = new.replace(\" \",\"\")\n",
    "                 finalstring = string.split(':')\n",
    "                    #the finalstring is a list that contains the dimensions and the occurencies\n",
    "                    #in order toseperate in different lists we create an additional loop\n",
    "                 for xx in range(len(finalstring)):\n",
    "                     ax = finalstring[xx]\n",
    "                     if 'x' in ax:\n",
    "                         print(\"We got in \", str(xx), finalstring[xx] )\n",
    "                         s_dimensions_local.insert(z1,finalstring[xx])\n",
    "                         z1 = z1 +1\n",
    "                     else:\n",
    "                         print(\"We got in \", str(xx), finalstring[xx] )\n",
    "                         s_times_local.insert(m1,finalstring[xx])\n",
    "                         m1 = m1 +1  \n",
    "            #Now we can add to the lists the parts we created so as to have them all gathered together\n",
    "             nm.insert(num,num)                  \n",
    "             s_comp.insert(num,list500_names[num])\n",
    "             s_dimensions.insert(num,s_dimensions_local)\n",
    "             s_times.insert(num,s_times_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'list500_sites' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-552cfd25d238>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Run the function for the first 20 sites\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfind_dif_sizes\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist500_sites\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlist500_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'list500_sites' is not defined"
     ]
    }
   ],
   "source": [
    "#Run the function for the first 20 sites\n",
    "find_dif_sizes (list500_sites,list500_names,0,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
