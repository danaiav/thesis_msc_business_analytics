{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 10,
=======
   "execution_count": 1,
>>>>>>> origin/master
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#First we import the libraries we will need\n",
    "import urllib2\n",
    "#import urllib.request\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 11,
=======
   "execution_count": 2,
>>>>>>> origin/master
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#First of all we need to find all the name of the sites that belong to fortune 500. This can happen if we seperate\n",
    "#The information needed from the below link\n",
    "url = \"http://www.zyxware.com/articles/4344/list-of-fortune-500-companies-and-their-websites\"\n",
    "list_company_number =[]\n",
    "list_company_name = []\n",
    "list_company_website = []\n",
    "list500_sites = []\n",
    "list500_names = []\n",
    "list500_num = []\n",
    "list500_url = []"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 12,
=======
   "execution_count": 3,
>>>>>>> origin/master
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#In order to extract the needed informations we will create 3 lists. The first one will contain the rank of each site, the\n",
    "#second one will contain the name of the company and the 3rd one will contain the actual link of the company's site.\n",
    "#For achieving this purpose we will create a funstion that will in its turn create those three list.\n",
    "#In order to know if the function worked we will ask it to return the first element of each list along with a sentence.\n",
    "def websites (url):\n",
    "    browser = urllib2.build_opener()\n",
    "    browser.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
    "    response = browser.open(url)# this might throw an exception if something goes wrong.\n",
    "    myHTML = response.read()\n",
    "    soup = BeautifulSoup(myHTML,\"lxml\")    \n",
    "    o = 0\n",
    "    td_list =[]\n",
    "    for row2 in soup.html.body.findAll('td'):\n",
    "        td_list.insert(o, row2)\n",
    "        o = o + 1\n",
    "    a = 0\n",
    "    b = 1\n",
    "    c = 2\n",
    "    list_numbering = 0\n",
    "    for i in range (0,500):        \n",
    "        num = str(td_list[a])\n",
    "        company = str(td_list[b])\n",
    "        site = str(td_list[c])\n",
    "        c_num = re.findall('>(.+?)</td>',num)  \n",
    "        c_num = str(c_num[0])\n",
    "        c_name = re.findall('>(.+?)</td>',company)\n",
    "        c_name = str(c_name[0])\n",
    "        c_site = re.findall('\">(.+?)</a>',site)\n",
    "        c_site = str(c_site[0])        \n",
    "        list_company_number.insert(list_numbering,c_num)\n",
    "        list_company_name.insert(list_numbering,c_name)\n",
    "        list_company_website.insert(list_numbering,c_site)\n",
    "        a = a + 3\n",
    "        b = b + 3\n",
    "        c = c + 3\n",
    "        list_numbering =  list_numbering + 1 \n",
    "    print ('The three lists are ready')\n",
    "    print (list_company_number[0])\n",
    "    print (list_company_name[0])\n",
    "    print (list_company_website[0])"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 13,
=======
   "execution_count": 4,
>>>>>>> origin/master
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The three lists are ready\n",
      "1\n",
      "Walmart\n",
      "www.walmart.com\n"
     ]
    }
   ],
   "source": [
    "# After creating the function we should now test that it actually works correctly\n",
    "websites (url)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 14,
=======
   "execution_count": 5,
>>>>>>> origin/master
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## The next step is to download and save in a new list the 500 sites.\n",
    "## We will create a new function that will do this by reading the list \"list_company_websites\" that we created \n",
    "## with the previous function\n",
    "\n",
    "def list_company_HTML (list_company_website,list_company_name,start,end):\n",
    "    import time\n",
    "    browser2 = urllib2.build_opener()\n",
    "    browser2.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
    "    for i in range (start,end):\n",
    "        k = str(i+1)\n",
    "        lc = str(list_company_website[i])        \n",
    "        lc = lc.replace(\"'\",\"\")   \n",
    "        lc = lc.replace(\"[\",\"\")\n",
    "        lc = lc.replace(\"]\",\"\")\n",
    "        lcn = str(list_company_name[i])        \n",
    "        lcn = lcn.replace(\"'\",\"\")   \n",
    "        lcn = lcn.replace(\"[\",\"\")\n",
    "        lcn = lcn.replace(\"]\",\"\")\n",
    "        url2= 'http://'+lc\n",
    "        #an exception might be thrown, so the code should be in a try-except block\n",
    "        try:\n",
    "            #use the browser to get the url.\n",
    "            response2=browser2.open(url2)# this might throw an exception if something goes wrong.\n",
    "        except Exception: # this describes what to do if an exception is thrown\n",
    "             continue#ignore this page.      \n",
    "        #read the response in html format. This is essentially a long piece of text\n",
    "        myHTML2=response2.read()\n",
    "        list500_sites.insert(i,myHTML2)\n",
    "        list500_names.insert(i,lcn)\n",
    "        list500_url.insert(i,lc)\n",
    "        list500_num.insert(i,k)\n",
    "        #wait for 2 seconds\n",
    "        time.sleep(2)\n",
    "        #print (\"The site \" + k + \" has been downloaded!\")\n",
    "    print (\"We downloaded: \",len(list500_sites),\" sites!\")\n",
    "    #print (len(list500_names),list500_names)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 15,
=======
   "execution_count": 6,
>>>>>>> origin/master
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "25\n",
<<<<<<< HEAD
      "('We downloaded: ', 24, ' sites!')\n"
=======
      "We downloaded:  22  sites!\n"
>>>>>>> origin/master
     ]
    }
   ],
   "source": [
    "start = 0\n",
    "stop = 25\n",
    "for i in range (0,1): #i am not downloading all the pages for the time being for reasons of running faster the process during the tests\n",
    "    print (start)\n",
    "    print (stop)    \n",
    "    list_company_HTML (list_company_website,list_company_name,start,stop)\n",
    "    start = stop\n",
    "    stop = stop + 25"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 16,
=======
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#As we can see there is one site that hasn't been downloaded in order to keep track of the sites that we could not download\n",
    "#we will create a new list that we will keep them all together there\n",
    "not_d = []\n",
    "def not_downloadables (list500_names,list_company_name):\n",
    "    met = 0\n",
    "    for i in range(0,25):#normally it would be till 500 but now we test for the first 25 we have downloaded\n",
    "        ct = list_company_name[i]\n",
    "        if ct not in list500_names:\n",
    "            not_d.insert(met,ct)\n",
    "    print(not_d)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Kroger', 'Fannie Mae', 'CVS Health']\n"
     ]
    }
   ],
   "source": [
    "#Now we will run the function to see which sites haven;t been downloaded\n",
    "not_downloadables (list500_names,list_company_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
>>>>>>> origin/master
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Retreiving the social media from each site\n",
    "#First create empty lists for the ones that we will need to calculate\n",
    "sm_f = []\n",
    "sm_t = []\n",
    "sm_i = []\n",
    "sm_p = []\n",
    "sm_y = []\n",
    "sm_l = []   \n",
    "sm_nm = [] \n",
    "nm = []\n",
    "sm_url = []"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 20,
=======
   "execution_count": 10,
>>>>>>> origin/master
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Then create a function that will feel in those lists so as to make the data frame later on\n",
    "\n",
    "def socialmedia (list500_sites,list500_names,list500_url):\n",
    "    for num in range(len(list500_names)):\n",
    "        if list500_sites[num] != \"\":\n",
    "            hand = str(list500_sites[num])\n",
    "            sm = ['facebook.com','twitter.com','instagram.com','pinterest.com','youtube.com','linkedin.com'] \n",
    "            number = 0 \n",
    "            for index in range(len(sm)):\n",
    "                x = sm[index]\n",
    "                photo = re.findall(x,hand)                                \n",
    "                if (len(photo)>0):\n",
    "                    if x == 'facebook.com':\n",
    "                        answerf = True\n",
    "                    if x == 'twitter.com':\n",
    "                        answert = True\n",
    "                    if x == 'instagram.com':\n",
    "                        answeri = True\n",
    "                    if x == 'pinterest.com':\n",
    "                        answerp = True\n",
    "                    if x == 'youtube.com':\n",
    "                        answery = True\n",
    "                    if x =='linkedin.com':\n",
    "                        answerl = True                   \n",
    "                else:\n",
    "                     if x == 'facebook.com':\n",
    "                        answerf = False\n",
    "                     if x == 'twitter.com':\n",
    "                        answert = False\n",
    "                     if x == 'instagram.com':\n",
    "                        answeri = False\n",
    "                     if x == 'pinterest.com':\n",
    "                        answerp = False\n",
    "                     if x == 'youtube.com':\n",
    "                        answery = False\n",
    "                     if x =='linkedin.com':\n",
    "                        answerl = False                \n",
    "            sm_nm.insert(num,list500_names[num]) \n",
    "            nm.insert(num,num)\n",
    "            sm_url.insert(num,list500_url[num])\n",
    "            sm_f.insert(num,answerf)\n",
    "            sm_t.insert(num,answert)\n",
    "            sm_i.insert(num,answeri)\n",
    "            sm_p.insert(num,answerp)\n",
    "            sm_y.insert(num,answery)\n",
    "            sm_l.insert(num,answerl)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 21,
=======
   "execution_count": 11,
>>>>>>> origin/master
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Now we will run the function for the 25 first sites for starters\n",
    "socialmedia (list500_sites,list500_names,list500_url)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 22,
=======
   "execution_count": 12,
>>>>>>> origin/master
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>facebook</th>\n",
       "      <th>instagram</th>\n",
       "      <th>linkedin</th>\n",
       "      <th>pinterest</th>\n",
       "      <th>twitter</th>\n",
       "      <th>url</th>\n",
       "      <th>youtube</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Walmart</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>www.walmart.com</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Exxon Mobil</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>www.exxonmobil.com</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Apple</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>www.apple.com</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       company facebook instagram linkedin pinterest twitter  \\\n",
       "0      Walmart     True      True    False      True    True   \n",
       "1  Exxon Mobil    False     False    False     False   False   \n",
       "2        Apple    False     False     True     False   False   \n",
       "\n",
       "                  url youtube  \n",
       "0     www.walmart.com    True  \n",
       "1  www.exxonmobil.com   False  \n",
       "2       www.apple.com    True  "
      ]
     },
<<<<<<< HEAD
     "execution_count": 22,
=======
     "execution_count": 12,
>>>>>>> origin/master
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finally we create the data frame with the elements we found            \n",
    "d = {'company' : pd.Series(sm_nm, index=[nm]),'url' : pd.Series(sm_url, index=[nm]),\n",
    "     'facebook' : pd.Series(sm_f, index=[nm]),'twitter' : pd.Series(sm_t, index=[nm]),\n",
    "     'instagram' : pd.Series(sm_i, index=[nm]),'pinterest' : pd.Series(sm_p, index=[nm]),\n",
    "     'youtube' : pd.Series(sm_y, index=[nm]),'linkedin' : pd.Series(sm_l, index=[nm]),}\n",
    "social_media = pd.DataFrame(d)    \n",
    "social_media.head(3) #we see the first 3 in the data frame"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 23,
=======
   "execution_count": 13,
>>>>>>> origin/master
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create the lists we will need for the data frame\n",
    "l_nm = []\n",
    "l_ex = []\n",
    "l_in = []\n",
    "l_t = []\n",
    "nm = []\n",
    "l_url = []"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 24,
=======
   "execution_count": 14,
>>>>>>> origin/master
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create the function that will calculate the different type of links\n",
<<<<<<< HEAD
    "def links (list500_sites,list500_names,list500_url,start,end):\n",
    "    for num in range(start,end):\n",
=======
    "def links (list500_sites,list500_names,list500_url):\n",
    "    for num in range(len(list500_names)):\n",
>>>>>>> origin/master
    "        line = str(list500_sites[num])\n",
    "        href = re.findall('href',line)\n",
    "        external = re.findall('href=\"https:',line)\n",
    "        ex = (len(external))\n",
    "        alllinks = (len(href))\n",
    "        internal =  (len(href) - len(external))\n",
    "        l_nm.insert(num,list500_names[num])            \n",
    "        l_ex.insert(num,ex)\n",
    "        l_t.insert(num,alllinks)\n",
    "        l_in.insert(num,internal)\n",
    "        nm.insert(num,num)\n",
    "        l_url.insert(num,list500_url[num])"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 25,
=======
   "execution_count": 15,
>>>>>>> origin/master
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Run the function in order to find the external, internal and total links of each site\n",
<<<<<<< HEAD
    "#For now we are running for the first 20 sites only\n",
    "links (list500_sites,list500_names,list500_url,0,20)"
=======
    "#For now we are running for the first 25 sites only\n",
    "links (list500_sites,list500_names,list500_url)"
>>>>>>> origin/master
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 26,
=======
   "execution_count": 16,
>>>>>>> origin/master
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>external</th>\n",
       "      <th>internal</th>\n",
       "      <th>total links</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Walmart</td>\n",
       "      <td>67</td>\n",
       "      <td>98</td>\n",
       "      <td>165</td>\n",
       "      <td>www.walmart.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Exxon Mobil</td>\n",
       "      <td>23</td>\n",
       "      <td>783</td>\n",
       "      <td>806</td>\n",
       "      <td>www.exxonmobil.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Apple</td>\n",
       "      <td>5</td>\n",
       "      <td>277</td>\n",
       "      <td>282</td>\n",
       "      <td>www.apple.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       company  external  internal  total links                 url\n",
       "0      Walmart        67        98          165     www.walmart.com\n",
       "1  Exxon Mobil        23       783          806  www.exxonmobil.com\n",
       "2        Apple         5       277          282       www.apple.com"
      ]
     },
<<<<<<< HEAD
     "execution_count": 26,
=======
     "execution_count": 16,
>>>>>>> origin/master
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a dataframe so as to be able to see the results of the function we run\n",
    "d2 = {'company' : pd.Series(l_nm, index=[nm]),'url' : pd.Series(l_url, index=[nm]),\n",
    "      'external' : pd.Series(l_ex, index=[nm]),'internal' : pd.Series(l_in, index=[nm]),\n",
    "     'total links' : pd.Series(l_t, index=[nm])}\n",
    "sites_links = pd.DataFrame(d2)    \n",
    "sites_links.head(3) #we see the first 3 in the data frame"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 27,
=======
   "execution_count": 17,
>>>>>>> origin/master
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The initial lists we will need in order to calculate the loading time\n",
    "lt_nm = [] \n",
    "lt_time = []\n",
    "nm = []\n",
    "lt_url = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#the function that will calculate the loading time\n",
    "def loadtime (list_company_website,list500_names,list500_url):\n",
    "    from time import time\n",
    "    browser2 = urllib2.build_opener()\n",
    "    browser2.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
    "    for num in range(len(list500_names)):\n",
    "        lc = str(list_company_website[num])        \n",
    "        lc = lc.replace(\"'\",\"\")   \n",
    "        lc = lc.replace(\"[\",\"\")\n",
    "        lc = lc.replace(\"]\",\"\")\n",
    "        url2= 'http://'+lc\n",
    "        try:\n",
    "            response2=browser2.open(url2)# this might throw an exception if something goes wrong.\n",
    "        except Exception: # this describes what to do if an exception is thrown\n",
    "             continue   \n",
    "        start_time = time()\n",
    "        myHTML2=response2.read()\n",
    "        end_time = time()\n",
    "        response2.close()\n",
    "        l_t = round(end_time-start_time, 3)\n",
    "        loadt = str(l_t)\n",
    "        lt_nm.insert(num,list500_names[num])            \n",
    "        lt_time.insert(num,loadt)\n",
    "        nm.insert(num,num)\n",
<<<<<<< HEAD
    "        lt_url.insert(num,lc)"
=======
    "        lt_url.insert(num,list500_url[num])"
>>>>>>> origin/master
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#running the function for the first 25 sites\n",
    "loadtime (list_company_website,list500_names,list500_url)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 43,
=======
   "execution_count": 20,
>>>>>>> origin/master
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>loading time</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Walmart</td>\n",
<<<<<<< HEAD
       "      <td>0.422</td>\n",
       "      <td>www.walmart.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Exxon Mobil</td>\n",
       "      <td>5.943</td>\n",
       "      <td>www.exxonmobil.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Apple</td>\n",
       "      <td>0.094</td>\n",
       "      <td>www.apple.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Berkshire Hathaway</td>\n",
       "      <td>0.0</td>\n",
       "      <td>www.berkshirehathaway.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>McKesson</td>\n",
       "      <td>0.156</td>\n",
       "      <td>www.mckesson.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>UnitedHealth Group</td>\n",
       "      <td>0.886</td>\n",
       "      <td>www.unitedhealthgroup.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CVS Health</td>\n",
       "      <td>0.078</td>\n",
       "      <td>www.cvshealth.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>General Motors</td>\n",
       "      <td>0.047</td>\n",
       "      <td>www.gm.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Ford Motor</td>\n",
       "      <td>0.11</td>\n",
       "      <td>www.ford.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AT&amp;amp;T</td>\n",
       "      <td>1.531</td>\n",
       "      <td>www.att.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>General Electric</td>\n",
       "      <td>0.031</td>\n",
       "      <td>www.ge.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>AmerisourceBergen</td>\n",
       "      <td>0.0</td>\n",
       "      <td>www.amerisourcebergen.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Verizon</td>\n",
       "      <td>0.141</td>\n",
       "      <td>www.verizon.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Chevron</td>\n",
       "      <td>0.093</td>\n",
       "      <td>www.chevron.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Costco</td>\n",
       "      <td>0.125</td>\n",
       "      <td>www.costco.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Amazon.com</td>\n",
       "      <td>0.312</td>\n",
       "      <td>www.thekrogerco.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Walgreens Boots Alliance</td>\n",
       "      <td>0.533</td>\n",
       "      <td>www.amazon.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>HP</td>\n",
       "      <td>0.616</td>\n",
       "      <td>www.walgreensbootsalliance.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Cardinal Health</td>\n",
       "      <td>0.203</td>\n",
       "      <td>www.hp.com</td>\n",
=======
       "      <td>0.926</td>\n",
       "      <td>www.walmart.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Exxon Mobil</td>\n",
       "      <td>7.681</td>\n",
       "      <td>www.exxonmobil.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Apple</td>\n",
       "      <td>0.161</td>\n",
       "      <td>www.apple.com</td>\n",
>>>>>>> origin/master
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
<<<<<<< HEAD
       "                     company loading time                             url\n",
       "0                    Walmart        0.422                 www.walmart.com\n",
       "1                Exxon Mobil        5.943              www.exxonmobil.com\n",
       "2                      Apple        0.094                   www.apple.com\n",
       "3         Berkshire Hathaway          0.0       www.berkshirehathaway.com\n",
       "4                   McKesson        0.156                www.mckesson.com\n",
       "5         UnitedHealth Group        0.886       www.unitedhealthgroup.com\n",
       "6                 CVS Health        0.078               www.cvshealth.com\n",
       "7             General Motors        0.047                      www.gm.com\n",
       "8                 Ford Motor         0.11                    www.ford.com\n",
       "9                   AT&amp;T        1.531                     www.att.com\n",
       "10          General Electric        0.031                      www.ge.com\n",
       "11         AmerisourceBergen          0.0       www.amerisourcebergen.com\n",
       "12                   Verizon        0.141                 www.verizon.com\n",
       "13                   Chevron        0.093                 www.chevron.com\n",
       "14                    Costco        0.125                  www.costco.com\n",
       "16                Amazon.com        0.312             www.thekrogerco.com\n",
       "17  Walgreens Boots Alliance        0.533                  www.amazon.com\n",
       "18                        HP        0.616  www.walgreensbootsalliance.com\n",
       "19           Cardinal Health        0.203                      www.hp.com"
      ]
     },
     "execution_count": 43,
=======
       "       company loading time                 url\n",
       "0      Walmart        0.926     www.walmart.com\n",
       "1  Exxon Mobil        7.681  www.exxonmobil.com\n",
       "2        Apple        0.161       www.apple.com"
      ]
     },
     "execution_count": 20,
>>>>>>> origin/master
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating the data frame with the loading times\n",
    "d3 = {'company' : pd.Series(lt_nm, index=[nm]),'url' : pd.Series(lt_url, index=[nm]),\n",
    "      'loading time' : pd.Series(lt_time, index=[nm])}\n",
    "loading_time = pd.DataFrame(d3)    \n",
    "loading_time #we see the first 3 in the data frame"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 58,
=======
   "execution_count": 21,
>>>>>>> origin/master
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Find out how many and what type of images each site has\n",
    "#first we create the initially empty lists\n",
    "p_p = []\n",
    "p_d = []\n",
    "p_jpg = []\n",
    "p_jpeg = []\n",
    "p_gif = []\n",
    "p_tif = []\n",
    "p_tiff = []\n",
    "p_bmp = []\n",
    "p_jpe = []\n",
    "p_nm = []\n",
<<<<<<< HEAD
    "p_tt = []\n",
=======
    "p_tt =[]\n",
>>>>>>> origin/master
    "nm = []\n",
    "p_url = []"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 59,
=======
   "execution_count": 22,
>>>>>>> origin/master
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Then we create the function that will explore the html pages and search for the images\n",
<<<<<<< HEAD
    "def images (list500_sites,list500_names,list500_url,start,end):\n",
    "    for num in range(start,end):\n",
=======
    "def images (list500_sites,list500_names,list500_url):\n",
    "    for num in range(len(list500_names)):\n",
>>>>>>> origin/master
    "        line = str(list500_sites[num])\n",
    "        image = ['.png','.dib','.jpg','.jpeg','.bmp','.jpe','.gif','.tif','.tiff'] \n",
    "        totalnumber = 0 \n",
    "        for index in range(len(image)):\n",
    "            x = image[index]\n",
    "            photo = re.findall(x,line)\n",
    "            if x == '.png':\n",
    "                p = str (len(photo))\n",
    "            if x == '.dib':\n",
    "                d = str (len(photo))\n",
    "            if x == '.jpg':\n",
    "                jpg = str (len(photo))\n",
    "            if x == '.jpeg':\n",
    "                jpeg = str (len(photo))\n",
    "            if x == '.gif':\n",
    "                gif = str (len(photo))\n",
    "            if x == '.tif':\n",
    "                tif = str (len(photo))\n",
    "            if x == '.tiff':\n",
    "                tiff = str (len(photo))\n",
    "            if x == '.bmp':\n",
    "                bmp = str (len(photo))\n",
    "            if x == '.jpe':\n",
    "                jpe = str (len(photo))\n",
    "            totalnumber = len(photo) + totalnumber\n",
    "        total = str (totalnumber)\n",
    "        p_nm.insert(num,list500_names[num])            \n",
    "        p_p.insert(num,p)  \n",
    "        p_d.insert(num,d)  \n",
    "        p_jpg.insert(num,jpg)  \n",
    "        p_jpeg.insert(num,jpeg)  \n",
    "        p_gif.insert(num,gif)  \n",
    "        p_tif.insert(num,tif)  \n",
    "        p_tiff.insert(num,tiff)  \n",
    "        p_bmp.insert(num,bmp)  \n",
    "        p_jpe.insert(num,jpe)  \n",
<<<<<<< HEAD
    "        p_tt.insert(num,total)  \n",
=======
    "        p_tt.insert(num,total)\n",
>>>>>>> origin/master
    "        nm.insert(num,num)\n",
    "        p_url.insert(num,list500_url[num])"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 60,
=======
   "execution_count": 23,
>>>>>>> origin/master
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Then we run the function for the first 20 sites for now\n",
<<<<<<< HEAD
    "images (list500_sites,list500_names,list500_url,0,20)"
=======
    "images (list500_sites,list500_names,list500_url)"
>>>>>>> origin/master
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 65,
=======
   "execution_count": 24,
>>>>>>> origin/master
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>.bmp</th>\n",
       "      <th>.dib</th>\n",
       "      <th>.gif</th>\n",
       "      <th>.jpe</th>\n",
       "      <th>.jpeg</th>\n",
       "      <th>.jpg</th>\n",
       "      <th>.png</th>\n",
       "      <th>.tif</th>\n",
       "      <th>.tiff</th>\n",
       "      <th>company</th>\n",
       "      <th>total images</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>103</td>\n",
       "      <td>103</td>\n",
       "      <td>76</td>\n",
       "      <td>43</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>Walmart</td>\n",
       "      <td>353</td>\n",
       "      <td>www.walmart.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Exxon Mobil</td>\n",
       "      <td>23</td>\n",
       "      <td>www.exxonmobil.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Apple</td>\n",
       "      <td>3</td>\n",
       "      <td>www.apple.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  .bmp .dib .gif .jpe .jpeg .jpg .png .tif .tiff      company total images  \\\n",
       "0    0    0   22  103   103   76   43    6     0      Walmart          353   \n",
       "1    0    0    1    0     0   16    2    4     0  Exxon Mobil           23   \n",
       "2    0    0    1    0     0    0    2    0     0        Apple            3   \n",
       "\n",
       "                  url  \n",
       "0     www.walmart.com  \n",
       "1  www.exxonmobil.com  \n",
       "2       www.apple.com  "
      ]
     },
<<<<<<< HEAD
     "execution_count": 65,
=======
     "execution_count": 24,
>>>>>>> origin/master
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finally we create a dataframe in order to see the results of the function\n",
<<<<<<< HEAD
    "d4 = {'company' : pd.Series(p_nm, index=[nm]),'url' : pd.Series(p_url, index=[nm]),'total images' : pd.Series(p_tt, index=[nm]),\n",
    "      '.png' : pd.Series(p_p, index=[nm]),'.dib' : pd.Series(p_d, index=[nm]),\n",
    "      '.jpg' : pd.Series(p_jpg, index=[nm]),'.jpeg' : pd.Series(p_jpeg, index=[nm]),\n",
    "      '.bmp' : pd.Series(p_bmp, index=[nm]),'.jpe' : pd.Series(p_jpe, index=[nm]),\n",
    "      '.gif' : pd.Series(p_gif, index=[nm]),'.tif' : pd.Series(p_tif, index=[nm]),\n",
    "      '.tiff' : pd.Series(p_tiff, index=[nm])}\n",
=======
    "d4 = {'company' : pd.Series(p_nm, index=[nm]),'url' : pd.Series(p_url, index=[nm]),\n",
    "      '.png' : pd.Series(p_p, index=[nm]),'.dib' : pd.Series(p_d, index=[nm]),\n",
    "'.jpg' : pd.Series(p_jpg, index=[nm]),'.jpeg' : pd.Series(p_jpeg, index=[nm]),\n",
    "'.bmp' : pd.Series(p_bmp, index=[nm]),'.jpe' : pd.Series(p_jpe, index=[nm]),\n",
    "'.gif' : pd.Series(p_gif, index=[nm]),'.tif' : pd.Series(p_tif, index=[nm]),\n",
    "'.tiff' : pd.Series(p_tiff, index=[nm]), 'total images' : pd.Series(p_tt, index=[nm])}\n",
>>>>>>> origin/master
    "images_types = pd.DataFrame(d4)    \n",
    "images_types.head(3) #we see the first 3 in the data frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now we will find the different dimensions that each site uses\n",
    "#initially we create the empty lists we will need\n",
    "nm = []\n",
    "s_comp = []\n",
    "s_dimensions = []\n",
    "s_times = []\n",
    "s_tt_dif_dim = []\n",
    "ht = [] #list of different heights in each case\n",
    "wt = [] #list of different widths in each case\n",
    "h_w = [] # combinations of height and width\n",
    "dif_size = []  \n",
    "un_size = [] \n",
    "s_url = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#With the below function we will gather in a variable all the different dimensions \n",
    "#and in another one all the times that each dimension occures for each html code\n",
    "def find_dif_sizes (list500_sites,list500_names,list500_url):\n",
    "    for num in range(len(list500_names)):\n",
    "        nm.insert(num,num)                  \n",
    "        s_comp.insert(num,list500_names[num])\n",
    "        s_url.insert(num,list500_url[num])\n",
    "        line = str(list500_sites[num]) #read each line of the list500_sites that contains each html code\n",
    "        soup = BeautifulSoup(line, \"lxml\")\n",
    "        # we create 2 local variables so as to gather the different dimensions and occurencies  of each page seperately\n",
    "        s_dimensions_local = []\n",
    "        s_times_local = []\n",
    "        hw = 0 # we use it for the lists of height and width\n",
    "        for tag in soup.find_all('img'): # find all the img in the first site html\n",
    "            #Since in some cases either the height or the width is missing we would like to keep only the ones that have both dimensions\n",
    "             h = tag.attrs.get('height', None)\n",
    "             w = tag.attrs.get('width', None)\n",
    "             #we use if to check which ones have both        \n",
    "             if h != None:\n",
    "                 if w != None:\n",
    "                     ht.insert(hw,h)\n",
    "                     wt.insert(hw,w)\n",
    "                     hw = hw+1                        \n",
    "        hw2 = 0\n",
    "        for l in range(len(ht)):\n",
    "             h_w_c = ht[l] + 'x' + wt[l]    #we create a str with the form (300x300) so as to be more easily to read later on \n",
    "             h_w.insert(hw2,h_w_c)  #we put it in a new list\n",
    "             hw2 = hw2 + 1    \n",
    "        if h_w == []:#we check if there are not any dimensions available\n",
    "             nm.insert(num,num)                  \n",
    "             s_comp.insert(num,list500_names[num])\n",
    "             s_dimensions.insert(num,0)\n",
    "             s_times.insert(num,0)    \n",
    "        if h_w != []:#now we continue with the cases where the dimensions are indeed available             \n",
    "             from collections import Counter\n",
    "             hw_unique = Counter(h_w)\n",
    "             hw_unique2 = str(hw_unique) #the unique different dimensions for the specific site\n",
    "            #Due to the fact that we are talking about a list we have to split the parts we need \n",
    "             split1 = hw_unique2.split('{')\n",
    "             a=split1[1]\n",
    "             split2 =a.split('}')\n",
    "             b=split2[0]\n",
    "             split3 = b.split(',')\n",
    "             finalsplit=[]\n",
    "             fs = []\n",
    "             z=0\n",
    "             m=1\n",
    "             j = 0\n",
    "             z1=0\n",
    "             m1=1\n",
    "            #each of the items in split3 has a form '300x300 : 15' and in order to create the dataframe we have \n",
    "            #to split this form and keep the informations in different list\n",
    "             for numb in split3:                \n",
    "                 oldstring = numb\n",
    "                 newstring = oldstring.replace(\"'\", \"\")\n",
    "                 new = newstring.replace(\"'\",\"\")\n",
    "                 string = new.replace(\" \",\"\")\n",
    "                 finalstring = string.split(':')\n",
    "                    #the finalstring is a list that contains the dimensions and the occurencies\n",
    "                    #in order toseperate in different lists we create an additional loop\n",
    "                 for xx in range(len(finalstring)):\n",
    "                     ax = finalstring[xx]\n",
    "                     if 'x' in ax:\n",
    "                         s_dimensions_local.insert(z1,finalstring[xx])\n",
    "                         z1 = z1 +1\n",
    "                     else:\n",
    "                         s_times_local.insert(m1,finalstring[xx])\n",
    "                         m1 = m1 +1  \n",
    "            #Now we can add to the lists the parts we created so as to have them all gathered together             \n",
    "             s_dimensions.insert(num,s_dimensions_local)\n",
    "             s_times.insert(num,s_times_local)                \n",
    "    print(\"Completed\")\n",
    "    print (s_comp,nm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "['Walmart', 'Exxon Mobil', 'Apple', 'Berkshire Hathaway', 'McKesson', 'UnitedHealth Group', 'General Motors', 'Ford Motor', 'AT&amp;T', 'General Electric', 'AmerisourceBergen', 'Verizon', 'Chevron', 'Costco', 'Amazon.com', 'Walgreens Boots Alliance', 'HP', 'Cardinal Health', 'Express Scripts Holding', 'J.P. Morgan Chase', 'Boeing', 'Microsoft'] [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n"
     ]
    }
   ],
   "source": [
    "#Run the function for the first 20 sites\n",
    "find_dif_sizes (list500_sites,list500_names,list500_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Find the unique different image dimensions and put them on a list\n",
    "def unique_dif_sizes (s_dimensions,list500_names):\n",
    "    ds=0\n",
    "    for num in range(len(list500_names)):\n",
    "        for s in s_dimensions[num]:\n",
    "            dif_size.insert(ds,s)\n",
    "            ds=ds+1\n",
    "    dsu = 0\n",
    "    for i in dif_size:\n",
    "        if i not in un_size:\n",
    "            un_size.insert(dsu,i)\n",
    "            dsu = dsu + 1\n",
    "    print(un_size)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['144x144', '15x75', '24pxx133px', '21pxx173px', '70x125', '1x1', '50x45', '400x300', '292pxx292px', '200pxx200px', '1279pxx984px', '100pxx500px', '300pxx1500px', '29x29', '304x540', '720x1920']\n"
     ]
    }
   ],
   "source": [
    "#Run the finction unique_dif_sizes\n",
    "unique_dif_sizes (s_dimensions,list500_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The lists we will need for the next function\n",
    "t_f_s = []\n",
    "ttf = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function in order to check whether or not each company has these dimensions\n",
    "def dimensions_per_company (un_size,list500_names):\n",
    "    t_f_s.insert(0,un_size)\n",
    "    ttf.insert(0,t_f_s)\n",
    "    for num in range(len(list500_names)):\n",
    "        s1a = s_dimensions[num] #dimensions of site num\n",
    "        where =[] #empty list\n",
    "        wh=0\n",
    "        haveornot = []\n",
    "        for er in range (len(un_size)):\n",
    "            for sizea in s1a:\n",
    "                if sizea == un_size[er]:\n",
    "                    where.insert(wh,str(er))\n",
    "                    break\n",
    "            if str(er) in where:\n",
    "               haveornot.insert(er,True)                    \n",
    "            else:\n",
    "               haveornot.insert(er,False)\n",
    "                    \n",
    "        t_f_s.insert(num+1,haveornot)\n",
    "        ttf.insert(num+1,t_f_s)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Run the function dimensions_per_company\n",
    "dimensions_per_company (un_size,list500_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Walmart', 'Exxon Mobil', 'Apple', 'Berkshire Hathaway', 'McKesson', 'UnitedHealth Group', 'General Motors', 'Ford Motor', 'AT&amp;T', 'General Electric', 'AmerisourceBergen', 'Verizon', 'Chevron', 'Costco', 'Amazon.com', 'Walgreens Boots Alliance', 'HP', 'Cardinal Health', 'Express Scripts Holding', 'J.P. Morgan Chase', 'Boeing', 'Microsoft']\n"
     ]
    }
   ],
   "source": [
    "print(s_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Walmart</td>\n",
       "      <td>www.walmart.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Exxon Mobil</td>\n",
       "      <td>www.exxonmobil.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Apple</td>\n",
       "      <td>www.apple.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       company                 url\n",
       "0      Walmart     www.walmart.com\n",
       "1  Exxon Mobil  www.exxonmobil.com\n",
       "2        Apple       www.apple.com"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create an initial dataframe where we will add the sizes later on\n",
    "d7 = {'company' : pd.Series(s_comp, index=[nm]),'url' : pd.Series(s_url, index=[nm])}\n",
    "sizess = pd.DataFrame(d7)    \n",
    "sizess.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Now we want to break the variable t_f_s in order to add the columns to the dataframe                  \n",
    "#Finally we create the data frame with the elements we found \n",
    "def final_dimensions_dataframe (un_size,t_f_s,list500_names):\n",
    "    for q in range(len(un_size)):\n",
    "        names = un_size[q]\n",
    "        var = []\n",
    "        for num in range(len(list500_names)):\n",
    "            a = t_f_s[num+1]\n",
    "            var.insert(num,a[q])\n",
    "        sizess[names] = pd.Series(var, index=sizess.index)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>url</th>\n",
       "      <th>144x144</th>\n",
       "      <th>15x75</th>\n",
       "      <th>24pxx133px</th>\n",
       "      <th>21pxx173px</th>\n",
       "      <th>70x125</th>\n",
       "      <th>1x1</th>\n",
       "      <th>50x45</th>\n",
       "      <th>400x300</th>\n",
       "      <th>292pxx292px</th>\n",
       "      <th>200pxx200px</th>\n",
       "      <th>1279pxx984px</th>\n",
       "      <th>100pxx500px</th>\n",
       "      <th>300pxx1500px</th>\n",
       "      <th>29x29</th>\n",
       "      <th>304x540</th>\n",
       "      <th>720x1920</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Walmart</td>\n",
       "      <td>www.walmart.com</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Exxon Mobil</td>\n",
       "      <td>www.exxonmobil.com</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Apple</td>\n",
       "      <td>www.apple.com</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       company                 url 144x144  15x75 24pxx133px 21pxx173px  \\\n",
       "0      Walmart     www.walmart.com    True  False      False      False   \n",
       "1  Exxon Mobil  www.exxonmobil.com    True  False      False      False   \n",
       "2        Apple       www.apple.com    True  False      False      False   \n",
       "\n",
       "  70x125    1x1  50x45 400x300 292pxx292px 200pxx200px 1279pxx984px  \\\n",
       "0  False  False  False   False       False       False        False   \n",
       "1  False  False  False   False       False       False        False   \n",
       "2  False  False  False   False       False       False        False   \n",
       "\n",
       "  100pxx500px 300pxx1500px  29x29 304x540 720x1920  \n",
       "0       False        False  False   False    False  \n",
       "1       False        False  False   False    False  \n",
       "2       False        False  False   False    False  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Run the function final_dimensions_dataframe\n",
    "final_dimensions_dataframe (un_size,t_f_s,list500_names)\n",
    "sizess.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now we would like to find the words in the text and the unique words of each html page\n",
    "#First of all we need to have a dictionary with which we would check if the word we found truly exists\n",
    "#The dictionary is available in the internet from a github acount from where we are going to read it\n",
    "url_dictionary = \"https://raw.githubusercontent.com/dwyl/english-words/master/words.txt\"\n",
    "browser = urllib.request.build_opener()\n",
    "browser.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
    "response = browser.open(url_dictionary)\n",
    "html_dictionary = response.read()\n",
    "html_dictionary\n",
    "dicti = str(html_dictionary)\n",
    "dict_new = dicti.split(\"\\\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aa'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_new[49] #the first 49 parts are not words so we have to remove them from the list\n",
    "dict_final = []\n",
    "df = 0\n",
    "for i in range (50,len(dict_new)):\n",
    "    forfinal = dict_new[i]\n",
    "    forfinal = forfinal.replace(\"'\",\"\")\n",
    "    dict_final.insert(df,forfinal)\n",
    "    df =df +1\n",
    "dict_final[0] #This is the original dictionary with which we will check each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#In order not to check each word in all cases we will divide them based on the letter they start with\n",
    "alphabet =[\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\",\"j\",\"k\",\"l\",\"m\",\"n\",\"o\",\"p\",\"q\",\"r\",\"s\",\"t\",\"u\",\"v\",\"w\",\"x\",\"y\",\"z\"]\n",
    "start_end =[]\n",
    "start_end.insert(0,0) #since the letter a will begin in the start of the list\n",
    "for iii in range(len(alphabet)):\n",
    "    count = 0\n",
    "    letter = alphabet[iii]\n",
    "    for ii in range(len(dict_final)):\n",
    "        wordathand = dict_final[ii]\n",
    "        if wordathand != \"\":\n",
    "            letters = list(wordathand)\n",
    "            if letters[0] == letter:\n",
    "                count = count + 1\n",
    "    stpoint = iii + 1        \n",
    "    start_end.insert(stpoint,count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
