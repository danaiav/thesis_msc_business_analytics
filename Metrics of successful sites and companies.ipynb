{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#First we import the libraries we will need\n",
    "import urllib\n",
    "import urllib.request\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#First of all we need to find all the name of the sites that belong to fortune 500. This can happen if we seperate\n",
    "#The information needed from the below link\n",
    "url = \"http://www.zyxware.com/articles/4344/list-of-fortune-500-companies-and-their-websites\"\n",
    "list_company_number =[]\n",
    "list_company_name = []\n",
    "list_company_website = []\n",
    "list500_sites = []\n",
    "list500_names = []\n",
    "list500_num = []\n",
    "list500_url = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#In order to extract the needed informations we will create 3 lists. The first one will contain the rank of each site, the\n",
    "#second one will contain the name of the company and the 3rd one will contain the actual link of the company's site.\n",
    "#For achieving this purpose we will create a funstion that will in its turn create those three list.\n",
    "#In order to know if the function worked we will ask it to return the first element of each list along with a sentence.\n",
    "def websites (url):\n",
    "    browser = urllib.request.build_opener()\n",
    "    browser.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
    "    response = browser.open(url)# this might throw an exception if something goes wrong.\n",
    "    myHTML = response.read()\n",
    "    soup = BeautifulSoup(myHTML,\"lxml\")    \n",
    "    o = 0\n",
    "    td_list =[]\n",
    "    for row2 in soup.html.body.findAll('td'):\n",
    "        td_list.insert(o, row2)\n",
    "        o = o + 1\n",
    "    a = 0\n",
    "    b = 1\n",
    "    c = 2\n",
    "    list_numbering = 0\n",
    "    for i in range (0,500):        \n",
    "        num = str(td_list[a])\n",
    "        company = str(td_list[b])\n",
    "        site = str(td_list[c])\n",
    "        c_num = re.findall('>(.+?)</td>',num)  \n",
    "        c_num = str(c_num[0])\n",
    "        c_name = re.findall('>(.+?)</td>',company)\n",
    "        c_name = str(c_name[0])\n",
    "        c_site = re.findall('\">(.+?)</a>',site)\n",
    "        c_site = str(c_site[0])        \n",
    "        list_company_number.insert(list_numbering,c_num)\n",
    "        list_company_name.insert(list_numbering,c_name)\n",
    "        list_company_website.insert(list_numbering,c_site)\n",
    "        a = a + 3\n",
    "        b = b + 3\n",
    "        c = c + 3\n",
    "        list_numbering =  list_numbering + 1 \n",
    "    print ('The three lists are ready')\n",
    "    print (list_company_number[0])\n",
    "    print (list_company_name[0])\n",
    "    print (list_company_website[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The three lists are ready\n",
      "1\n",
      "Walmart\n",
      "www.walmart.com\n"
     ]
    }
   ],
   "source": [
    "# After creating the function we should now test that it actually works correctly\n",
    "websites (url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## The next step is to download and save in a new list the 500 sites.\n",
    "## We will create a new function that will do this by reading the list \"list_company_websites\" that we created \n",
    "## with the previous function\n",
    "\n",
    "def list_company_HTML (list_company_website,list_company_name,start,end):\n",
    "    import time\n",
    "    browser2 = urllib.request.build_opener()\n",
    "    browser2.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
    "    for i in range (start,end):\n",
    "        k = str(i+1)\n",
    "        lc = str(list_company_website[i])        \n",
    "        lc = lc.replace(\"'\",\"\")   \n",
    "        lc = lc.replace(\"[\",\"\")\n",
    "        lc = lc.replace(\"]\",\"\")\n",
    "        lcn = str(list_company_name[i])        \n",
    "        lcn = lcn.replace(\"'\",\"\")   \n",
    "        lcn = lcn.replace(\"[\",\"\")\n",
    "        lcn = lcn.replace(\"]\",\"\")\n",
    "        url2= 'http://'+lc\n",
    "        #an exception might be thrown, so the code should be in a try-except block\n",
    "        try:\n",
    "            #use the browser to get the url.\n",
    "            response2=browser2.open(url2)# this might throw an exception if something goes wrong.\n",
    "        except Exception: # this describes what to do if an exception is thrown\n",
    "             continue#ignore this page.      \n",
    "        #read the response in html format. This is essentially a long piece of text\n",
    "        myHTML2=response2.read()\n",
    "        list500_sites.insert(i,myHTML2)\n",
    "        list500_names.insert(i,lcn)\n",
    "        list500_url.insert(i,lc)\n",
    "        list500_num.insert(i,k)\n",
    "        #wait for 2 seconds\n",
    "        time.sleep(2)\n",
    "        #print (\"The site \" + k + \" has been downloaded!\")\n",
    "    print (\"We downloaded: \",len(list500_sites),\" sites!\")\n",
    "    #print (len(list500_names),list500_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "25\n",
      "We downloaded:  24  sites!\n"
     ]
    }
   ],
   "source": [
    "start = 0\n",
    "stop = 25\n",
    "for i in range (0,1): #i am not downloading all the pages for the time being for reasons of running faster the process during the tests\n",
    "    print (start)\n",
    "    print (stop)    \n",
    "    list_company_HTML (list_company_website,list_company_name,start,stop)\n",
    "    start = stop\n",
    "    stop = stop + 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Retreiving the social media from each site\n",
    "#First create empty lists for the ones that we will need to calculate\n",
    "sm_f = []\n",
    "sm_t = []\n",
    "sm_i = []\n",
    "sm_p = []\n",
    "sm_y = []\n",
    "sm_l = []   \n",
    "sm_nm = [] \n",
    "nm = []\n",
    "sm_url = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Then create a function that will feel in those lists so as to make the data frame later on\n",
    "\n",
    "def socialmedia (list500_sites,list500_names,list500_url,start,end):\n",
    "    for num in range(start,end):\n",
    "        if list500_sites[num] != \"\":\n",
    "            hand = str(list500_sites[num])\n",
    "            sm = ['facebook.com','twitter.com','instagram.com','pinterest.com','youtube.com','linkedin.com'] \n",
    "            number = 0 \n",
    "            for index in range(len(sm)):\n",
    "                x = sm[index]\n",
    "                photo = re.findall(x,hand)                                \n",
    "                if (len(photo)>0):\n",
    "                    if x == 'facebook.com':\n",
    "                        answerf = 'yes'\n",
    "                    if x == 'twitter.com':\n",
    "                        answert = 'yes'\n",
    "                    if x == 'instagram.com':\n",
    "                        answeri = 'yes'\n",
    "                    if x == 'pinterest.com':\n",
    "                        answerp = 'yes'\n",
    "                    if x == 'youtube.com':\n",
    "                        answery = 'yes'\n",
    "                    if x =='linkedin.com':\n",
    "                        answerl = 'yes'                   \n",
    "                else:\n",
    "                     if x == 'facebook.com':\n",
    "                        answerf = 'no'\n",
    "                     if x == 'twitter.com':\n",
    "                        answert = 'no'\n",
    "                     if x == 'instagram.com':\n",
    "                        answeri = 'no'\n",
    "                     if x == 'pinterest.com':\n",
    "                        answerp = 'no'\n",
    "                     if x == 'youtube.com':\n",
    "                        answery = 'no'\n",
    "                     if x =='linkedin.com':\n",
    "                        answerl = 'no'                 \n",
    "            sm_nm.insert(num,list500_names[num]) \n",
    "            nm.insert(num,num)\n",
    "            sm_url.insert(num,list500_url[num])\n",
    "            sm_f.insert(num,answerf)\n",
    "            sm_t.insert(num,answert)\n",
    "            sm_i.insert(num,answeri)\n",
    "            sm_p.insert(num,answerp)\n",
    "            sm_y.insert(num,answery)\n",
    "            sm_l.insert(num,answerl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Now we will run the function for the 20 first sites for starters\n",
    "socialmedia (list500_sites,list500_names,list500_url,0,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>facebook</th>\n",
       "      <th>instagram</th>\n",
       "      <th>linkedin</th>\n",
       "      <th>pinterest</th>\n",
       "      <th>twitter</th>\n",
       "      <th>url</th>\n",
       "      <th>youtube</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Walmart</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>www.walmart.com</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Exxon Mobil</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>www.exxonmobil.com</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Apple</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>www.apple.com</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       company facebook instagram linkedin pinterest twitter  \\\n",
       "0      Walmart      yes       yes       no       yes     yes   \n",
       "1  Exxon Mobil       no        no       no        no      no   \n",
       "2        Apple       no        no      yes        no      no   \n",
       "\n",
       "                  url youtube  \n",
       "0     www.walmart.com     yes  \n",
       "1  www.exxonmobil.com      no  \n",
       "2       www.apple.com     yes  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finally we create the data frame with the elements we found            \n",
    "d = {'company' : pd.Series(sm_nm, index=[nm]),'url' : pd.Series(sm_url, index=[nm]),\n",
    "     'facebook' : pd.Series(sm_f, index=[nm]),'twitter' : pd.Series(sm_t, index=[nm]),\n",
    "     'instagram' : pd.Series(sm_i, index=[nm]),'pinterest' : pd.Series(sm_p, index=[nm]),\n",
    "     'youtube' : pd.Series(sm_y, index=[nm]),'linkedin' : pd.Series(sm_l, index=[nm]),}\n",
    "social_media = pd.DataFrame(d)    \n",
    "social_media.head(3) #we see the first 3 in the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create the lists we will need for the data frame\n",
    "l_nm = []\n",
    "l_ex = []\n",
    "l_in = []\n",
    "l_t = []\n",
    "nm = []\n",
    "l_url = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create the function that will calculate the different type of links\n",
    "def links (list500_sites,list500_names,list500_url,start,end):\n",
    "    for num in range(start,end):\n",
    "        line = str(list500_sites[num])\n",
    "        href = re.findall('href',line)\n",
    "        external = re.findall('href=\"https:',line)\n",
    "        ex = (len(external))\n",
    "        alllinks = (len(href))\n",
    "        internal =  (len(href) - len(external))\n",
    "        l_nm.insert(num,list500_names[num])            \n",
    "        l_ex.insert(num,ex)\n",
    "        l_t.insert(num,alllinks)\n",
    "        l_in.insert(num,internal)\n",
    "        nm.insert(num,num)\n",
    "        l_url.insert(num,list500_url[num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Run the function in order to find the external, internal and total links of each site\n",
    "#For now we are running for the first 20 sites only\n",
    "links (list500_sites,list500_names,list500_url,0,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>external</th>\n",
       "      <th>internal</th>\n",
       "      <th>total links</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Walmart</td>\n",
       "      <td>67</td>\n",
       "      <td>82</td>\n",
       "      <td>149</td>\n",
       "      <td>www.walmart.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Exxon Mobil</td>\n",
       "      <td>19</td>\n",
       "      <td>784</td>\n",
       "      <td>803</td>\n",
       "      <td>www.exxonmobil.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Apple</td>\n",
       "      <td>5</td>\n",
       "      <td>277</td>\n",
       "      <td>282</td>\n",
       "      <td>www.apple.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       company  external  internal  total links                 url\n",
       "0      Walmart        67        82          149     www.walmart.com\n",
       "1  Exxon Mobil        19       784          803  www.exxonmobil.com\n",
       "2        Apple         5       277          282       www.apple.com"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a dataframe so as to be able to see the results of the function we run\n",
    "d2 = {'company' : pd.Series(l_nm, index=[nm]),'url' : pd.Series(l_url, index=[nm]),\n",
    "      'external' : pd.Series(l_ex, index=[nm]),'internal' : pd.Series(l_in, index=[nm]),\n",
    "     'total links' : pd.Series(l_t, index=[nm])}\n",
    "sites_links = pd.DataFrame(d2)    \n",
    "sites_links.head(3) #we see the first 3 in the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The initial lists we will need in order to calculate the loading time\n",
    "lt_nm = [] \n",
    "lt_time = []\n",
    "nm = []\n",
    "lt_url = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#the function that will calculate the loading time\n",
    "def loadtime (list_company_website,list500_names,list500_url,start,end):\n",
    "    from time import time\n",
    "    browser2 = urllib.request.build_opener()\n",
    "    browser2.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
    "    for num in range(start,end):\n",
    "        lc = str(list_company_website[num])        \n",
    "        lc = lc.replace(\"'\",\"\")   \n",
    "        lc = lc.replace(\"[\",\"\")\n",
    "        lc = lc.replace(\"]\",\"\")\n",
    "        url2= 'http://'+lc        \n",
    "        try:\n",
    "            response2=browser2.open(url2)\n",
    "        except Exception: \n",
    "             continue#     \n",
    "        start_time = time()\n",
    "        myHTML2=response2.read()\n",
    "        end_time = time()\n",
    "        response2.close()\n",
    "        l_t = round(end_time-start_time, 3) #in order to be more redable we rounded the time\n",
    "        loadt = str(l_t)\n",
    "        lt_nm.insert(num,list500_names[num])            \n",
    "        lt_time.insert(num,loadt)\n",
    "        nm.insert(num,num)\n",
    "        lt_url.insert(num,list500_url[num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#running the function for the first 20 sites\n",
    "loadtime (list_company_website,list500_names,list500_url,0,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>loading time</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Walmart</td>\n",
       "      <td>0.574</td>\n",
       "      <td>www.walmart.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Exxon Mobil</td>\n",
       "      <td>4.341</td>\n",
       "      <td>www.exxonmobil.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Apple</td>\n",
       "      <td>0.069</td>\n",
       "      <td>www.apple.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       company loading time                 url\n",
       "0      Walmart        0.574     www.walmart.com\n",
       "1  Exxon Mobil        4.341  www.exxonmobil.com\n",
       "2        Apple        0.069       www.apple.com"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating the data frame with the loading times\n",
    "d3 = {'company' : pd.Series(lt_nm, index=[nm]),'url' : pd.Series(lt_url, index=[nm]),\n",
    "      'loading time' : pd.Series(lt_time, index=[nm])}\n",
    "loading_time = pd.DataFrame(d3)    \n",
    "loading_time.head(3) #we see the first 3 in the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Find out how many and what type of images each site has\n",
    "#first we create the initially empty lists\n",
    "p_p = []\n",
    "p_d = []\n",
    "p_jpg = []\n",
    "p_jpeg = []\n",
    "p_gif = []\n",
    "p_tif = []\n",
    "p_tiff = []\n",
    "p_bmp = []\n",
    "p_jpe = []\n",
    "p_nm = []\n",
    "p_tt =[]\n",
    "nm = []\n",
    "p_url = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Then we create the function that will explore the html pages and search for the images\n",
    "def images (list500_sites,list500_names,list500_url,start,end):\n",
    "    for num in range(start,end):\n",
    "        line = str(list500_sites[num])\n",
    "        image = ['.png','.dib','.jpg','.jpeg','.bmp','.jpe','.gif','.tif','.tiff'] \n",
    "        totalnumber = 0 \n",
    "        for index in range(len(image)):\n",
    "            x = image[index]\n",
    "            photo = re.findall(x,line)\n",
    "            if x == '.png':\n",
    "                p = str (len(photo))\n",
    "            if x == '.dib':\n",
    "                d = str (len(photo))\n",
    "            if x == '.jpg':\n",
    "                jpg = str (len(photo))\n",
    "            if x == '.jpeg':\n",
    "                jpeg = str (len(photo))\n",
    "            if x == '.gif':\n",
    "                gif = str (len(photo))\n",
    "            if x == '.tif':\n",
    "                tif = str (len(photo))\n",
    "            if x == '.tiff':\n",
    "                tiff = str (len(photo))\n",
    "            if x == '.bmp':\n",
    "                bmp = str (len(photo))\n",
    "            if x == '.jpe':\n",
    "                jpe = str (len(photo))\n",
    "            totalnumber = len(photo) + totalnumber\n",
    "        total = str (totalnumber)\n",
    "        p_nm.insert(num,list500_names[num])            \n",
    "        p_p.insert(num,p)  \n",
    "        p_d.insert(num,d)  \n",
    "        p_jpg.insert(num,jpg)  \n",
    "        p_jpeg.insert(num,jpeg)  \n",
    "        p_gif.insert(num,gif)  \n",
    "        p_tif.insert(num,tif)  \n",
    "        p_tiff.insert(num,tiff)  \n",
    "        p_bmp.insert(num,bmp)  \n",
    "        p_jpe.insert(num,jpe)  \n",
    "        p_tt.insert(num,total)\n",
    "        nm.insert(num,num)\n",
    "        p_url.insert(num,list500_url[num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Then we run the function for the first 20 sites for now\n",
    "images (list500_sites,list500_names,list500_url,0,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>.bmp</th>\n",
       "      <th>.dib</th>\n",
       "      <th>.gif</th>\n",
       "      <th>.jpe</th>\n",
       "      <th>.jpeg</th>\n",
       "      <th>.jpg</th>\n",
       "      <th>.png</th>\n",
       "      <th>.tif</th>\n",
       "      <th>.tiff</th>\n",
       "      <th>company</th>\n",
       "      <th>total images</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>76</td>\n",
       "      <td>43</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>Walmart</td>\n",
       "      <td>297</td>\n",
       "      <td>www.walmart.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Exxon Mobil</td>\n",
       "      <td>23</td>\n",
       "      <td>www.exxonmobil.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Apple</td>\n",
       "      <td>3</td>\n",
       "      <td>www.apple.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  .bmp .dib .gif .jpe .jpeg .jpg .png .tif .tiff      company total images  \\\n",
       "0    0    0   22   75    75   76   43    6     0      Walmart          297   \n",
       "1    0    0    1    0     0   16    2    4     0  Exxon Mobil           23   \n",
       "2    0    0    1    0     0    0    2    0     0        Apple            3   \n",
       "\n",
       "                  url  \n",
       "0     www.walmart.com  \n",
       "1  www.exxonmobil.com  \n",
       "2       www.apple.com  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finally we create a dataframe in order to see the results of the function\n",
    "d4 = {'company' : pd.Series(p_nm, index=[nm]),'url' : pd.Series(p_url, index=[nm]),\n",
    "      '.png' : pd.Series(p_p, index=[nm]),'.dib' : pd.Series(p_d, index=[nm]),\n",
    "'.jpg' : pd.Series(p_jpg, index=[nm]),'.jpeg' : pd.Series(p_jpeg, index=[nm]),\n",
    "'.bmp' : pd.Series(p_bmp, index=[nm]),'.jpe' : pd.Series(p_jpe, index=[nm]),\n",
    "'.gif' : pd.Series(p_gif, index=[nm]),'.tif' : pd.Series(p_tif, index=[nm]),\n",
    "'.tiff' : pd.Series(p_tiff, index=[nm]), 'total images' : pd.Series(p_tt, index=[nm])}\n",
    "images_types = pd.DataFrame(d4)    \n",
    "images_types.head(3) #we see the first 3 in the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now we will find the different dimensions that each site uses\n",
    "#initially we create the empty lists we will need\n",
    "nm = []\n",
    "s_comp = []\n",
    "s_dimensions = []\n",
    "s_times = []\n",
    "s_tt_dif_dim = []\n",
    "ht = [] #list of different heights in each case\n",
    "wt = [] #list of different widths in each case\n",
    "h_w = [] # combinations of height and width\n",
    "dif_size = []  \n",
    "un_size = [] \n",
    "s_url = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#With the below function we will gather in a variable all the different dimensions \n",
    "#and in another one all the times that each dimension occures for each html code\n",
    "def find_dif_sizes (list500_sites,list500_names,list500_url,start,end):\n",
    "    for num in range(start,end):\n",
    "        nm.insert(num,num)                  \n",
    "        s_comp.insert(num,list500_names[num])\n",
    "        s_url.insert(num,list500_url[num])\n",
    "        line = str(list500_sites[num]) #read each line of the list500_sites that contains each html code\n",
    "        soup = BeautifulSoup(line, \"lxml\")\n",
    "        # we create 2 local variables so as to gather the different dimensions and occurencies  of each page seperately\n",
    "        s_dimensions_local = []\n",
    "        s_times_local = []\n",
    "        hw = 0 # we use it for the lists of height and width\n",
    "        for tag in soup.find_all('img'): # find all the img in the first site html\n",
    "            #Since in some cases either the height or the width is missing we would like to keep only the ones that have both dimensions\n",
    "             h = tag.attrs.get('height', None)\n",
    "             w = tag.attrs.get('width', None)\n",
    "             #we use if to check which ones have both        \n",
    "             if h != None:\n",
    "                 if w != None:\n",
    "                     ht.insert(hw,h)\n",
    "                     wt.insert(hw,w)\n",
    "                     hw = hw+1                        \n",
    "        hw2 = 0\n",
    "        for l in range(len(ht)):\n",
    "             h_w_c = ht[l] + 'x' + wt[l]    #we create a str with the form (300x300) so as to be more easily to read later on \n",
    "             h_w.insert(hw2,h_w_c)  #we put it in a new list\n",
    "             hw2 = hw2 + 1    \n",
    "        if h_w == []:#we check if there are not any dimensions available\n",
    "             nm.insert(num,num)                  \n",
    "             s_comp.insert(num,list500_names[num])\n",
    "             s_dimensions.insert(num,0)\n",
    "             s_times.insert(num,0)    \n",
    "        if h_w != []:#now we continue with the cases where the dimensions are indeed available             \n",
    "             from collections import Counter\n",
    "             hw_unique = Counter(h_w)\n",
    "             hw_unique2 = str(hw_unique) #the unique different dimensions for the specific site\n",
    "            #Due to the fact that we are talking about a list we have to split the parts we need \n",
    "             split1 = hw_unique2.split('{')\n",
    "             a=split1[1]\n",
    "             split2 =a.split('}')\n",
    "             b=split2[0]\n",
    "             split3 = b.split(',')\n",
    "             finalsplit=[]\n",
    "             fs = []\n",
    "             z=0\n",
    "             m=1\n",
    "             j = 0\n",
    "             z1=0\n",
    "             m1=1\n",
    "            #each of the items in split3 has a form '300x300 : 15' and in order to create the dataframe we have \n",
    "            #to split this form and keep the informations in different list\n",
    "             for numb in split3:                \n",
    "                 oldstring = numb\n",
    "                 newstring = oldstring.replace(\"'\", \"\")\n",
    "                 new = newstring.replace(\"'\",\"\")\n",
    "                 string = new.replace(\" \",\"\")\n",
    "                 finalstring = string.split(':')\n",
    "                    #the finalstring is a list that contains the dimensions and the occurencies\n",
    "                    #in order toseperate in different lists we create an additional loop\n",
    "                 for xx in range(len(finalstring)):\n",
    "                     ax = finalstring[xx]\n",
    "                     if 'x' in ax:\n",
    "                         s_dimensions_local.insert(z1,finalstring[xx])\n",
    "                         z1 = z1 +1\n",
    "                     else:\n",
    "                         s_times_local.insert(m1,finalstring[xx])\n",
    "                         m1 = m1 +1  \n",
    "            #Now we can add to the lists the parts we created so as to have them all gathered together             \n",
    "             s_dimensions.insert(num,s_dimensions_local)\n",
    "             s_times.insert(num,s_times_local)                \n",
    "    print(\"Completed\")\n",
    "    print (s_comp,nm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "['Walmart', 'Exxon Mobil', 'Apple', 'Berkshire Hathaway', 'McKesson', 'UnitedHealth Group', 'CVS Health', 'General Motors', 'Ford Motor', 'AT&amp;T', 'General Electric', 'AmerisourceBergen', 'Verizon', 'Chevron', 'Costco', 'Kroger', 'Amazon.com', 'Walgreens Boots Alliance', 'HP', 'Cardinal Health'] [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n"
     ]
    }
   ],
   "source": [
    "#Run the function for the first 20 sites\n",
    "find_dif_sizes (list500_sites,list500_names,list500_url,0,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Find the unique different image dimensions and put them on a list\n",
    "def unique_dif_sizes (s_dimensions,start,end):\n",
    "    ds=0\n",
    "    for num in range(start,end):\n",
    "        for s in s_dimensions[num]:\n",
    "            dif_size.insert(ds,s)\n",
    "            ds=ds+1\n",
    "    dsu = 0\n",
    "    for i in dif_size:\n",
    "        if i not in un_size:\n",
    "            un_size.insert(dsu,i)\n",
    "            dsu = dsu + 1\n",
    "    print(un_size)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['144x144', '15x75', '800x1200', '21pxx173px', '24pxx133px', '70x125', '1x1', '50x45', '400x300', '292pxx292px', '200pxx200px', '1279pxx984px', '100pxx500px', '300pxx1500px', '29x29']\n"
     ]
    }
   ],
   "source": [
    "#Run the finction unique_dif_sizes\n",
    "unique_dif_sizes (s_dimensions,0,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The lists we will need for the next function\n",
    "t_f_s = []\n",
    "ttf = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function in order to check whether or not each company has these dimensions\n",
    "def dimensions_per_company (un_size,start,end):\n",
    "    t_f_s.insert(0,un_size)\n",
    "    ttf.insert(0,t_f_s)\n",
    "    for num in range(start,end):\n",
    "        s1a = s_dimensions[num] #dimensions of site num\n",
    "        where =[] #empty list\n",
    "        wh=0\n",
    "        haveornot = []\n",
    "        for er in range (len(un_size)):\n",
    "            for sizea in s1a:\n",
    "                if sizea == un_size[er]:\n",
    "                    where.insert(wh,str(er))\n",
    "                    break\n",
    "            if str(er) in where:\n",
    "               haveornot.insert(er,True)                    \n",
    "            else:\n",
    "               haveornot.insert(er,False)\n",
    "                    \n",
    "        t_f_s.insert(num+1,haveornot)\n",
    "        ttf.insert(num+1,t_f_s)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Run the function dimensions_per_company\n",
    "dimensions_per_company (un_size,0,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Walmart', 'Exxon Mobil', 'Apple', 'Berkshire Hathaway', 'McKesson', 'UnitedHealth Group', 'CVS Health', 'General Motors', 'Ford Motor', 'AT&amp;T', 'General Electric', 'AmerisourceBergen', 'Verizon', 'Chevron', 'Costco', 'Kroger', 'Amazon.com', 'Walgreens Boots Alliance', 'HP', 'Cardinal Health']\n"
     ]
    }
   ],
   "source": [
    "print(s_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Walmart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Exxon Mobil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Apple</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       company\n",
       "0      Walmart\n",
       "1  Exxon Mobil\n",
       "2        Apple"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create an initial dataframe where we will add the sizes later on\n",
    "d7 = {'company' : pd.Series(s_comp, index=[nm])}\n",
    "sizess = pd.DataFrame(d7)    \n",
    "sizess.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Now we want to break the variable t_f_s in order to add the columns to the dataframe                  \n",
    "#Finally we create the data frame with the elements we found \n",
    "def final_dimensions_dataframe (un_size,t_f_s,start,end):\n",
    "    for q in range(len(un_size)):\n",
    "        names = un_size[q]\n",
    "        var = []\n",
    "        for num in range(start,end):\n",
    "            a = t_f_s[num+1]\n",
    "            var.insert(num,a[q])\n",
    "        sizess[names] = pd.Series(var, index=sizess.index)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>144x144</th>\n",
       "      <th>15x75</th>\n",
       "      <th>800x1200</th>\n",
       "      <th>21pxx173px</th>\n",
       "      <th>24pxx133px</th>\n",
       "      <th>70x125</th>\n",
       "      <th>1x1</th>\n",
       "      <th>50x45</th>\n",
       "      <th>400x300</th>\n",
       "      <th>292pxx292px</th>\n",
       "      <th>200pxx200px</th>\n",
       "      <th>1279pxx984px</th>\n",
       "      <th>100pxx500px</th>\n",
       "      <th>300pxx1500px</th>\n",
       "      <th>29x29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Walmart</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Exxon Mobil</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Apple</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       company 144x144  15x75 800x1200 21pxx173px 24pxx133px 70x125    1x1  \\\n",
       "0      Walmart    True  False    False      False      False  False  False   \n",
       "1  Exxon Mobil    True  False    False      False      False  False  False   \n",
       "2        Apple    True  False    False      False      False  False  False   \n",
       "\n",
       "   50x45 400x300 292pxx292px 200pxx200px 1279pxx984px 100pxx500px  \\\n",
       "0  False   False       False       False        False       False   \n",
       "1  False   False       False       False        False       False   \n",
       "2  False   False       False       False        False       False   \n",
       "\n",
       "  300pxx1500px  29x29  \n",
       "0        False  False  \n",
       "1        False  False  \n",
       "2        False  False  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Run the function final_dimensions_dataframe\n",
    "final_dimensions_dataframe (un_size,t_f_s,0,len(nm))\n",
    "sizess.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
