{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#First we import the libraries we will need\n",
    "import urllib\n",
    "import urllib2\n",
    "import time\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#First of all we need to find all the name of the sites that belong to fortune 500. This can happen if we seperate\n",
    "#The information needed from the below link\n",
    "url = \"http://www.zyxware.com/articles/4344/list-of-fortune-500-companies-and-their-websites\"\n",
    "list_company_number =[]\n",
    "list_company_name = []\n",
    "list_company_website = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#In order to extract the needed informations we will create 3 lists. The first one will contain the rank of each site, the\n",
    "#second one will contain the name of the company and the 3rd one will contain the actual link of the company's site.\n",
    "#For achieving this purpose we will create a funstion that will in its turn create those three list.\n",
    "#In order to know if the function worked we will ask it to return the first element of each list along with a sentence.\n",
    "def websites (url): \n",
    "    from time import time # I used it to see how much time it does to run the function\n",
    "    start = time ()\n",
    "    browser = urllib2.build_opener() \n",
    "    browser.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
    "    response = browser.open(url)# this might throw an exception if something goes wrong.\n",
    "    myHTML = response.read()\n",
    "    soup = BeautifulSoup(myHTML,\"lxml\")    \n",
    "    o = 0\n",
    "    td_list =[]\n",
    "    for row2 in soup.html.body.findAll('td'):\n",
    "        td_list.insert(o, row2)\n",
    "        o = o + 1\n",
    "    a = 0\n",
    "    b = 1\n",
    "    c = 2\n",
    "    list_numbering = 0\n",
    "    for i in range (0,500):        \n",
    "        num = str(td_list[a])\n",
    "        company = str(td_list[b])\n",
    "        site = str(td_list[c])\n",
    "        c_num = re.findall('>(.+?)</td>',num)  \n",
    "        c_num = str(c_num[0])\n",
    "        c_name = re.findall('>(.+?)</td>',company)\n",
    "        c_name = str(c_name[0])\n",
    "        c_site = re.findall('\">(.+?)</a>',site)\n",
    "        c_site = str(c_site[0])        \n",
    "        list_company_number.insert(list_numbering,c_num)\n",
    "        list_company_name.insert(list_numbering,c_name)\n",
    "        list_company_website.insert(list_numbering,c_site)\n",
    "        a = a + 3\n",
    "        b = b + 3\n",
    "        c = c + 3\n",
    "        list_numbering =  list_numbering + 1 \n",
    "    end = time ()\n",
    "    duration = round (end - start, 1)\n",
    "    minutes = round (duration /60, 1)\n",
    "    print 'The lists are ready in ', duration, ' seconds'\n",
    "    print 'The lists are ready in ', minutes, ' minutes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lists are ready in  1.7  seconds\n",
      "The lists are ready in  0.0  minutes\n"
     ]
    }
   ],
   "source": [
    "# After creating the function we should now test that it actually works correctly\n",
    "websites (url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The validation is complete! There were 0 not valid pages\n"
     ]
    }
   ],
   "source": [
    "#Try to validate each page url #pip install validators\n",
    "import validators\n",
    "nv = 0\n",
    "for num in range(len(list_company_website)):\n",
    "    line = 'http://' + str(list_company_website[num])\n",
    "    x = validators.url(line)    \n",
    "    if x != True:\n",
    "        nv = nv +1\n",
    "print \"The validation is complete! There were\" , nv, \"not valid pages\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list500_sites = []\n",
    "list500_names = []\n",
    "list500_num = []\n",
    "list500_url = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The site 0 has been downloaded!\n",
      "The site 1 has been downloaded!\n",
      "The site 2 has been downloaded!\n",
      "The site 3 has been downloaded!\n",
      "The site 4 has been downloaded!\n",
      "The site 5 has been downloaded!\n",
      "The site 6 has been downloaded!\n",
      "The site 7 has been downloaded!\n",
      "The site 8 has been downloaded!\n",
      "The site 9 has been downloaded!\n",
      "The site 10 has been downloaded!\n",
      "The site 11 has been downloaded!\n",
      "The site 12 has been downloaded!\n",
      "The site 13 has been downloaded!\n",
      "The site 14 has been downloaded!\n",
      "The site 15 has NOT been downloaded from exception!\n",
      "The site 16 has been downloaded!\n",
      "The site 17 has been downloaded!\n",
      "The site 18 has been downloaded!\n",
      "The site 19 has been downloaded!\n",
      "The site 20 has been downloaded!\n",
      "The site 21 has been downloaded!\n",
      "The site 22 has been downloaded!\n",
      "The site 23 has been downloaded!\n",
      "The site 24 has been downloaded!\n",
      "The site 25 has been downloaded!\n",
      "The site 26 has been downloaded!\n",
      "The site 27 has been downloaded!\n",
      "The site 28 has been downloaded!\n",
      "The site 29 has been downloaded!\n",
      "The site 30 has been downloaded!\n",
      "The site 31 has been downloaded!\n",
      "The site 32 has been downloaded!\n",
      "The site 33 has been downloaded!\n",
      "The site 34 has been downloaded!\n",
      "The site 35 has been downloaded!\n",
      "The site 36 has been downloaded!\n",
      "The site 37 has been downloaded!\n",
      "The site 38 has been downloaded!\n",
      "The site 39 has been downloaded!\n",
      "The site 40 has been downloaded!\n",
      "The site 41 has been downloaded!\n",
      "The site 42 has been downloaded!\n",
      "The site 43 has been downloaded!\n",
      "The site 44 has been downloaded!\n",
      "The site 45 has been downloaded!\n",
      "The site 46 has been downloaded!\n",
      "The site 47 has been downloaded!\n",
      "The site 48 has been downloaded!\n",
      "The site 49 has been downloaded!\n",
      "The site 50 has been downloaded!\n",
      "The site 51 has NOT been downloaded from exception!\n",
      "The site 52 has been downloaded!\n",
      "The site 53 has been downloaded!\n",
      "The site 54 has been downloaded!\n",
      "The site 55 has been downloaded!\n",
      "The site 56 has been downloaded!\n",
      "The site 57 has been downloaded!\n",
      "The site 58 has been downloaded!\n",
      "The site 59 has been downloaded!\n",
      "The site 60 has been downloaded!\n",
      "The site 61 has been downloaded!\n",
      "The site 62 has NOT been downloaded from exception!\n",
      "The site 63 has been downloaded!\n",
      "The site 64 has been downloaded!\n",
      "The site 65 has been downloaded!\n",
      "The site 66 has been downloaded!\n",
      "The site 67 has been downloaded!\n",
      "The site 68 has been downloaded!\n",
      "The site 69 has been downloaded!\n",
      "The site 70 has NOT been downloaded!\n",
      "The site 71 has been downloaded!\n",
      "The site 72 has been downloaded!\n",
      "The site 73 has been downloaded!\n",
      "The site 74 has been downloaded!\n",
      "The site 75 has been downloaded!\n",
      "The site 76 has been downloaded!\n",
      "The site 77 has been downloaded!\n",
      "The site 78 has been downloaded!\n",
      "The site 79 has been downloaded!\n",
      "The site 80 has been downloaded!\n",
      "The site 81 has been downloaded!\n",
      "The site 82 has been downloaded!\n",
      "The site 83 has been downloaded!\n",
      "The site 84 has been downloaded!\n",
      "The site 85 has been downloaded!\n",
      "The site 86 has been downloaded!\n",
      "The site 87 has been downloaded!\n",
      "The site 88 has been downloaded!\n",
      "The site 89 has been downloaded!\n",
      "The site 90 has NOT been downloaded from exception!\n",
      "The site 91 has been downloaded!\n",
      "The site 92 has been downloaded!\n",
      "The site 93 has been downloaded!\n",
      "The site 94 has been downloaded!\n",
      "The site 95 has been downloaded!\n",
      "The site 96 has been downloaded!\n",
      "The site 97 has NOT been downloaded from exception!\n",
      "The site 98 has been downloaded!\n",
      "The site 99 has been downloaded!\n",
      "The site 100 has been downloaded!\n",
      "The site 101 has been downloaded!\n",
      "The site 102 has been downloaded!\n",
      "The site 103 has been downloaded!\n",
      "The site 104 has been downloaded!\n",
      "The site 105 has been downloaded!\n",
      "The site 106 has been downloaded!\n",
      "The site 107 has been downloaded!\n",
      "The site 108 has been downloaded!\n",
      "The site 109 has been downloaded!\n",
      "The site 110 has been downloaded!\n",
      "The site 111 has been downloaded!\n",
      "The site 112 has been downloaded!\n",
      "The site 113 has been downloaded!\n",
      "The site 114 has been downloaded!\n",
      "The site 115 has been downloaded!\n",
      "The site 116 has been downloaded!\n",
      "The site 117 has been downloaded!\n",
      "The site 118 has NOT been downloaded!\n",
      "The site 119 has been downloaded!\n",
      "The site 120 has been downloaded!\n",
      "The site 121 has been downloaded!\n",
      "The site 122 has been downloaded!\n",
      "The site 123 has been downloaded!\n",
      "The site 124 has been downloaded!\n",
      "The site 125 has been downloaded!\n",
      "The site 126 has been downloaded!\n",
      "The site 127 has been downloaded!\n",
      "The site 128 has been downloaded!\n",
      "The site 129 has been downloaded!\n",
      "The site 130 has been downloaded!\n",
      "The site 131 has been downloaded!\n",
      "The site 132 has been downloaded!\n",
      "The site 133 has been downloaded!\n",
      "The site 134 has been downloaded!\n",
      "The site 135 has NOT been downloaded from exception!\n",
      "The site 136 has been downloaded!\n",
      "The site 137 has been downloaded!\n",
      "The site 138 has been downloaded!\n",
      "The site 139 has been downloaded!\n",
      "The site 140 has been downloaded!\n",
      "The site 141 has NOT been downloaded from exception!\n",
      "The site 142 has been downloaded!\n",
      "The site 143 has been downloaded!\n",
      "The site 144 has NOT been downloaded from exception!\n",
      "The site 145 has been downloaded!\n",
      "The site 146 has been downloaded!\n",
      "The site 147 has been downloaded!\n",
      "The site 148 has been downloaded!\n",
      "The site 149 has been downloaded!\n",
      "The site 150 has been downloaded!\n",
      "The site 151 has been downloaded!\n",
      "The site 152 has been downloaded!\n",
      "The site 153 has been downloaded!\n",
      "The site 154 has been downloaded!\n",
      "The site 155 has been downloaded!\n",
      "The site 156 has been downloaded!\n",
      "The site 157 has been downloaded!\n",
      "The site 158 has been downloaded!\n",
      "The site 159 has been downloaded!\n",
      "The site 160 has been downloaded!\n",
      "The site 161 has been downloaded!\n",
      "The site 162 has been downloaded!\n",
      "The site 163 has been downloaded!\n",
      "The site 164 has NOT been downloaded from exception!\n",
      "The site 165 has been downloaded!\n",
      "The site 166 has been downloaded!\n",
      "The site 167 has been downloaded!\n",
      "The site 168 has been downloaded!\n",
      "The site 169 has been downloaded!\n",
      "The site 170 has been downloaded!\n",
      "The site 171 has been downloaded!\n",
      "The site 172 has been downloaded!\n",
      "The site 173 has been downloaded!\n",
      "The site 174 has been downloaded!\n",
      "The site 175 has been downloaded!\n",
      "The site 176 has been downloaded!\n",
      "The site 177 has been downloaded!\n",
      "The site 178 has been downloaded!\n",
      "The site 179 has been downloaded!\n",
      "The site 180 has been downloaded!\n",
      "The site 181 has been downloaded!\n",
      "The site 182 has been downloaded!\n",
      "The site 183 has been downloaded!\n",
      "The site 184 has been downloaded!\n",
      "The site 185 has been downloaded!\n",
      "The site 186 has been downloaded!\n",
      "The site 187 has been downloaded!\n",
      "The site 188 has been downloaded!\n",
      "The site 189 has been downloaded!\n",
      "The site 190 has been downloaded!\n",
      "The site 191 has been downloaded!\n",
      "The site 192 has been downloaded!\n",
      "The site 193 has been downloaded!\n",
      "The site 194 has been downloaded!\n",
      "The site 195 has NOT been downloaded from exception!\n",
      "The site 196 has been downloaded!\n",
      "The site 197 has been downloaded!\n",
      "The site 198 has been downloaded!\n",
      "The site 199 has been downloaded!\n",
      "The site 200 has been downloaded!\n",
      "The site 201 has been downloaded!\n",
      "The site 202 has been downloaded!\n",
      "The site 203 has been downloaded!\n",
      "The site 204 has been downloaded!\n",
      "The site 205 has been downloaded!\n",
      "The site 206 has been downloaded!\n",
      "The site 207 has been downloaded!\n",
      "The site 208 has been downloaded!\n",
      "The site 209 has been downloaded!\n",
      "The site 210 has been downloaded!\n",
      "The site 211 has been downloaded!\n",
      "The site 212 has been downloaded!\n",
      "The site 213 has been downloaded!\n",
      "The site 214 has been downloaded!\n",
      "The site 215 has been downloaded!\n",
      "The site 216 has NOT been downloaded from exception!\n",
      "The site 217 has been downloaded!\n",
      "The site 218 has been downloaded!\n",
      "The site 219 has been downloaded!\n",
      "The site 220 has been downloaded!\n",
      "The site 221 has been downloaded!\n",
      "The site 222 has been downloaded!\n",
      "The site 223 has been downloaded!\n",
      "The site 224 has been downloaded!\n",
      "The site 225 has been downloaded!\n",
      "The site 226 has been downloaded!\n",
      "The site 227 has been downloaded!\n",
      "The site 228 has NOT been downloaded from exception!\n",
      "The site 229 has been downloaded!\n",
      "The site 230 has been downloaded!\n",
      "The site 231 has been downloaded!\n",
      "The site 232 has been downloaded!\n",
      "The site 233 has been downloaded!\n",
      "The site 234 has been downloaded!\n",
      "The site 235 has been downloaded!\n",
      "The site 236 has been downloaded!\n",
      "The site 237 has been downloaded!\n",
      "The site 238 has been downloaded!\n",
      "The site 239 has been downloaded!\n",
      "The site 240 has been downloaded!\n",
      "The site 241 has been downloaded!\n",
      "The site 242 has NOT been downloaded from exception!\n",
      "The site 243 has been downloaded!\n",
      "The site 244 has NOT been downloaded from exception!\n",
      "The site 245 has been downloaded!\n",
      "The site 246 has been downloaded!\n",
      "The site 247 has been downloaded!\n",
      "The site 248 has been downloaded!\n",
      "The site 249 has been downloaded!\n",
      "The site 250 has been downloaded!\n",
      "The site 251 has been downloaded!\n",
      "The site 252 has been downloaded!\n",
      "The site 253 has been downloaded!\n",
      "The site 254 has been downloaded!\n",
      "The site 255 has been downloaded!\n",
      "The site 256 has been downloaded!\n",
      "The site 257 has been downloaded!\n",
      "The site 258 has been downloaded!\n",
      "The site 259 has been downloaded!\n",
      "The site 260 has been downloaded!\n",
      "The site 261 has been downloaded!\n",
      "The site 262 has been downloaded!\n",
      "The site 263 has been downloaded!\n",
      "The site 264 has been downloaded!\n",
      "The site 265 has been downloaded!\n",
      "The site 266 has been downloaded!\n",
      "The site 267 has been downloaded!\n",
      "The site 268 has NOT been downloaded!\n",
      "The site 269 has been downloaded!\n",
      "The site 270 has been downloaded!\n",
      "The site 271 has been downloaded!\n",
      "The site 272 has been downloaded!\n",
      "The site 273 has been downloaded!\n",
      "The site 274 has been downloaded!\n",
      "The site 275 has NOT been downloaded from exception!\n",
      "The site 276 has been downloaded!\n",
      "The site 277 has been downloaded!\n",
      "The site 278 has been downloaded!\n",
      "The site 279 has been downloaded!\n",
      "The site 280 has been downloaded!\n",
      "The site 281 has been downloaded!\n",
      "The site 282 has NOT been downloaded from exception!\n",
      "The site 283 has been downloaded!\n",
      "The site 284 has been downloaded!\n",
      "The site 285 has been downloaded!\n",
      "The site 286 has been downloaded!\n",
      "The site 287 has been downloaded!\n",
      "The site 288 has been downloaded!\n",
      "The site 289 has been downloaded!\n",
      "The site 290 has been downloaded!\n",
      "The site 291 has been downloaded!\n",
      "The site 292 has been downloaded!\n",
      "The site 293 has been downloaded!\n",
      "The site 294 has been downloaded!\n",
      "The site 295 has been downloaded!\n",
      "The site 296 has been downloaded!\n",
      "The site 297 has been downloaded!\n",
      "The site 298 has been downloaded!\n",
      "The site 299 has been downloaded!\n",
      "The site 300 has been downloaded!\n",
      "The site 301 has been downloaded!\n",
      "The site 302 has been downloaded!\n",
      "The site 303 has been downloaded!\n",
      "The site 304 has been downloaded!\n",
      "The site 305 has been downloaded!\n",
      "The site 306 has NOT been downloaded from exception!\n",
      "The site 307 has been downloaded!\n",
      "The site 308 has been downloaded!\n",
      "The site 309 has been downloaded!\n",
      "The site 310 has been downloaded!\n",
      "The site 311 has NOT been downloaded from exception!\n",
      "The site 312 has been downloaded!\n",
      "The site 313 has been downloaded!\n",
      "The site 314 has been downloaded!\n",
      "The site 315 has been downloaded!\n",
      "The site 316 has been downloaded!\n",
      "The site 317 has been downloaded!\n",
      "The site 318 has been downloaded!\n",
      "The site 319 has been downloaded!\n",
      "The site 320 has been downloaded!\n",
      "The site 321 has been downloaded!\n",
      "The site 322 has been downloaded!\n",
      "The site 323 has been downloaded!\n",
      "The site 324 has been downloaded!\n",
      "The site 325 has been downloaded!\n",
      "The site 326 has NOT been downloaded from exception!\n",
      "The site 327 has been downloaded!\n",
      "The site 328 has been downloaded!\n",
      "The site 329 has been downloaded!\n",
      "The site 330 has been downloaded!\n",
      "The site 331 has been downloaded!\n",
      "The site 332 has been downloaded!\n",
      "The site 333 has been downloaded!\n",
      "The site 334 has been downloaded!\n",
      "The site 335 has been downloaded!\n",
      "The site 336 has been downloaded!\n",
      "The site 337 has been downloaded!\n",
      "The site 338 has been downloaded!\n",
      "The site 339 has been downloaded!\n",
      "The site 340 has been downloaded!\n",
      "The site 341 has been downloaded!\n",
      "The site 342 has been downloaded!\n",
      "The site 343 has been downloaded!\n",
      "The site 344 has been downloaded!\n",
      "The site 345 has been downloaded!\n",
      "The site 346 has been downloaded!\n",
      "The site 347 has been downloaded!\n",
      "The site 348 has been downloaded!\n",
      "The site 349 has been downloaded!\n",
      "The site 350 has been downloaded!\n",
      "The site 351 has been downloaded!\n",
      "The site 352 has been downloaded!\n",
      "The site 353 has been downloaded!\n",
      "The site 354 has been downloaded!\n",
      "The site 355 has been downloaded!\n",
      "The site 356 has been downloaded!\n",
      "The site 357 has been downloaded!\n",
      "The site 358 has been downloaded!\n",
      "The site 359 has been downloaded!\n",
      "The site 360 has been downloaded!\n",
      "The site 361 has been downloaded!\n",
      "The site 362 has been downloaded!\n",
      "The site 363 has NOT been downloaded from exception!\n",
      "The site 364 has been downloaded!\n",
      "The site 365 has been downloaded!\n",
      "The site 366 has been downloaded!\n",
      "The site 367 has been downloaded!\n",
      "The site 368 has been downloaded!\n",
      "The site 369 has been downloaded!\n",
      "The site 370 has been downloaded!\n",
      "The site 371 has been downloaded!\n",
      "The site 372 has been downloaded!\n",
      "The site 373 has been downloaded!\n",
      "The site 374 has been downloaded!\n",
      "The site 375 has been downloaded!\n",
      "The site 376 has been downloaded!\n",
      "The site 377 has been downloaded!\n",
      "The site 378 has been downloaded!\n",
      "The site 379 has been downloaded!\n",
      "The site 380 has been downloaded!\n",
      "The site 381 has been downloaded!\n",
      "The site 382 has been downloaded!\n",
      "The site 383 has been downloaded!\n",
      "The site 384 has been downloaded!\n",
      "The site 385 has been downloaded!\n",
      "The site 386 has been downloaded!\n",
      "The site 387 has been downloaded!\n",
      "The site 388 has been downloaded!\n",
      "The site 389 has been downloaded!\n",
      "The site 390 has been downloaded!\n",
      "The site 391 has been downloaded!\n",
      "The site 392 has been downloaded!\n",
      "The site 393 has been downloaded!\n",
      "The site 394 has been downloaded!\n",
      "The site 395 has been downloaded!\n",
      "The site 396 has been downloaded!\n",
      "The site 397 has NOT been downloaded from exception!\n",
      "The site 398 has been downloaded!\n",
      "The site 399 has been downloaded!\n",
      "The site 400 has been downloaded!\n",
      "The site 401 has been downloaded!\n",
      "The site 402 has been downloaded!\n",
      "The site 403 has been downloaded!\n",
      "The site 404 has been downloaded!\n",
      "The site 405 has been downloaded!\n",
      "The site 406 has been downloaded!\n",
      "The site 407 has been downloaded!\n",
      "The site 408 has been downloaded!\n",
      "The site 409 has been downloaded!\n",
      "The site 410 has been downloaded!\n",
      "The site 411 has been downloaded!\n",
      "The site 412 has been downloaded!\n",
      "The site 413 has been downloaded!\n",
      "The site 414 has NOT been downloaded from exception!\n",
      "The site 415 has been downloaded!\n",
      "The site 416 has been downloaded!\n",
      "The site 417 has been downloaded!\n",
      "The site 418 has been downloaded!\n",
      "The site 419 has been downloaded!\n",
      "The site 420 has been downloaded!\n",
      "The site 421 has been downloaded!\n",
      "The site 422 has been downloaded!\n",
      "The site 423 has been downloaded!\n",
      "The site 424 has been downloaded!\n",
      "The site 425 has been downloaded!\n",
      "The site 426 has been downloaded!\n",
      "The site 427 has been downloaded!\n",
      "The site 428 has been downloaded!\n",
      "The site 429 has been downloaded!\n",
      "The site 430 has been downloaded!\n",
      "The site 431 has been downloaded!\n",
      "The site 432 has been downloaded!\n",
      "The site 433 has been downloaded!\n",
      "The site 434 has been downloaded!\n",
      "The site 435 has been downloaded!\n",
      "The site 436 has been downloaded!\n",
      "The site 437 has been downloaded!\n",
      "The site 438 has been downloaded!\n",
      "The site 439 has been downloaded!\n",
      "The site 440 has been downloaded!\n",
      "The site 441 has NOT been downloaded from exception!\n",
      "The site 442 has been downloaded!\n",
      "The site 443 has been downloaded!\n",
      "The site 444 has been downloaded!\n",
      "The site 445 has been downloaded!\n",
      "The site 446 has been downloaded!\n",
      "The site 447 has been downloaded!\n",
      "The site 448 has been downloaded!\n",
      "The site 449 has been downloaded!\n",
      "The site 450 has been downloaded!\n",
      "The site 451 has been downloaded!\n",
      "The site 452 has NOT been downloaded from exception!\n",
      "The site 453 has been downloaded!\n",
      "The site 454 has been downloaded!\n",
      "The site 455 has been downloaded!\n",
      "The site 456 has been downloaded!\n",
      "The site 457 has been downloaded!\n",
      "The site 458 has been downloaded!\n",
      "The site 459 has been downloaded!\n",
      "The site 460 has been downloaded!\n",
      "The site 461 has been downloaded!\n",
      "The site 462 has been downloaded!\n",
      "The site 463 has been downloaded!\n",
      "The site 464 has NOT been downloaded!\n",
      "The site 465 has NOT been downloaded from exception!\n",
      "The site 466 has been downloaded!\n",
      "The site 467 has been downloaded!\n",
      "The site 468 has been downloaded!\n",
      "The site 469 has been downloaded!\n",
      "The site 470 has been downloaded!\n",
      "The site 471 has NOT been downloaded from exception!\n",
      "The site 472 has been downloaded!\n",
      "The site 473 has been downloaded!\n",
      "The site 474 has been downloaded!\n",
      "The site 475 has been downloaded!\n",
      "The site 476 has been downloaded!\n",
      "The site 477 has been downloaded!\n",
      "The site 478 has been downloaded!\n",
      "The site 479 has been downloaded!\n",
      "The site 480 has been downloaded!\n",
      "The site 481 has been downloaded!\n",
      "The site 482 has been downloaded!\n",
      "The site 483 has been downloaded!\n",
      "The site 484 has been downloaded!\n",
      "The site 485 has been downloaded!\n",
      "The site 486 has been downloaded!\n",
      "The site 487 has been downloaded!\n",
      "The site 488 has been downloaded!\n",
      "The site 489 has been downloaded!\n",
      "The site 490 has been downloaded!\n",
      "The site 491 has been downloaded!\n",
      "The site 492 has been downloaded!\n",
      "The site 493 has been downloaded!\n",
      "The site 494 has been downloaded!\n",
      "The site 495 has been downloaded!\n",
      "The site 496 has been downloaded!\n",
      "The site 497 has been downloaded!\n",
      "The site 498 has been downloaded!\n",
      "The site 499 has been downloaded!\n"
     ]
    }
   ],
   "source": [
    "#def list_company_HTML (list_company_website,list_company_name,start,end):\n",
    "import time\n",
    "browser2 = urllib2.build_opener()\n",
    "browser2.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
    "for i in range (0,500):\n",
    "    k = str(i + 1)\n",
    "    lc = str(list_company_website[i])\n",
    "    lc = lc.replace(\"'\",\"\")\n",
    "    lc = lc.replace(\"[\",\"\")\n",
    "    lc = lc.replace(\"]\",\"\")\n",
    "    lcn = str(list_company_name[i])\n",
    "    lcn = lcn.replace(\"'\",\"\")\n",
    "    lcn = lcn.replace(\"[\",\"\")\n",
    "    lcn = lcn.replace(\"]\",\"\")\n",
    "    url2= 'http://' + lc\n",
    "    list500_names.insert(i,lcn)\n",
    "    list500_url.insert(i,lc)\n",
    "    list500_num.insert(i,k)\n",
    "    if i == 118 or i == 464 or i == 268 or i == 70:\n",
    "        #These sites have a problem and the whole code is stacking \n",
    "        #when I run it so we will thing of this site as a not downloadable\n",
    "        list500_sites.insert(i,0)  \n",
    "        print (\"The site \" + str(i) + \" has NOT been downloaded!\")\n",
    "    else:\n",
    "        #an exception might be thrown, so the code should be in a try-except block\n",
    "        try:\n",
    "            response2=browser2.open(url2)\n",
    "            print (\"The site \" + str(i) + \" has been downloaded!\")\n",
    "        except Exception: # this describes what to do if an exception is thrown\n",
    "            list500_sites.insert(i,0)\n",
    "            print (\"The site \" + str(i) + \" has NOT been downloaded from exception!\")           \n",
    "            continue     \n",
    "            #if it goes into to exception it does not continue below\n",
    "        myHTML2=response2.read()\n",
    "        list500_sites.insert(i,myHTML2) \n",
    "        #wait for 2 seconds\n",
    "        time.sleep(2)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#As we can see there is one site that hasn't been downloaded in order\n",
    "#to keep track of the sites that we could not download\n",
    "#we will create a new list that we will keep them all together there\n",
    "not_d = []\n",
    "not_d_n = []\n",
    "num = []\n",
    "def not_downloadables (list500_names,list500_sites):\n",
    "    met = 0       \n",
    "    for i in range(len(list500_names)):       \n",
    "        if list500_sites[i] == 0:\n",
    "            ct = list500_names[i]\n",
    "            not_d.insert(met,ct)\n",
    "            not_d_n.insert(met,str(i))\n",
    "            num.insert(met,met)\n",
    "            met = met + 1\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fannie Mae</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Humana</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HCA Holdings</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Best Buy</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nike</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Tesoro</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Arrow Electronics</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AutoNation</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Southwest Airlines</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Kohl’s</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>American Electric Power</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Office Depot</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>PBF Energy</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Consolidated Edison</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Dominion Resources</td>\n",
       "      <td>242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>L-3 Communications</td>\n",
       "      <td>244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Hertz Global Holdings</td>\n",
       "      <td>268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Global Partners</td>\n",
       "      <td>275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Discover Financial Services</td>\n",
       "      <td>282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>PayPal Holdings</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Republic Services</td>\n",
       "      <td>311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>News Corp.</td>\n",
       "      <td>326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Williams</td>\n",
       "      <td>363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Auto-Owners Insurance</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Tractor Supply</td>\n",
       "      <td>414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Old Republic International</td>\n",
       "      <td>441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Regions Financial</td>\n",
       "      <td>452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>St. Jude Medical</td>\n",
       "      <td>464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Wyndham Worldwide</td>\n",
       "      <td>465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Host Hotels &amp;amp; Resorts</td>\n",
       "      <td>471</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        company number\n",
       "0                    Fannie Mae     15\n",
       "1                        Humana     51\n",
       "2                  HCA Holdings     62\n",
       "3                      Best Buy     70\n",
       "4                          Nike     90\n",
       "5                        Tesoro     97\n",
       "6             Arrow Electronics    118\n",
       "7                    AutoNation    135\n",
       "8            Southwest Airlines    141\n",
       "9                        Kohl’s    144\n",
       "10      American Electric Power    164\n",
       "11                 Office Depot    195\n",
       "12                   PBF Energy    216\n",
       "13          Consolidated Edison    228\n",
       "14           Dominion Resources    242\n",
       "15           L-3 Communications    244\n",
       "16        Hertz Global Holdings    268\n",
       "17              Global Partners    275\n",
       "18  Discover Financial Services    282\n",
       "19              PayPal Holdings    306\n",
       "20            Republic Services    311\n",
       "21                   News Corp.    326\n",
       "22                     Williams    363\n",
       "23        Auto-Owners Insurance    397\n",
       "24               Tractor Supply    414\n",
       "25   Old Republic International    441\n",
       "26            Regions Financial    452\n",
       "27             St. Jude Medical    464\n",
       "28            Wyndham Worldwide    465\n",
       "29    Host Hotels &amp; Resorts    471"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now we will run the function to see which sites havent been downloaded\n",
    "not_downloadables (list500_names,list500_sites)\n",
    "d = {'company' : pd.Series(not_d, index=[num]),\n",
    "     'number' : pd.Series(not_d_n, index=[num])}\n",
    "nd = pd.DataFrame(d)    \n",
    "nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "empty=[]\n",
    "keyf = []\n",
    "flesch = []\n",
    "sentence =[] \n",
    "word = []\n",
    "unique_w = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Site', '0', 'is not validated from check 2')\n",
      "('Site', '1', 'is not validated from check 2')\n",
      "('Site', '11', 'is not validated from check 2')\n",
      "('Site', '15', 'is not validated from sites')\n",
      "('Site', '33', 'is not validated from check 2')\n",
      "('Site', '37', 'is not validated from check 2')\n",
      "('Site', '45', 'is not validated from check')\n",
      "('Site', '51', 'is not validated from sites')\n",
      "('Site', '58', 'is not validated from check 2')\n",
      "('Site', '62', 'is not validated from sites')\n",
      "('Site', '67', 'is not validated from check 2')\n",
      "('Site', '70', 'is not validated from sites')\n",
      "('Site', '82', 'is not validated from check 2')\n",
      "('Site', '90', 'is not validated from sites')\n",
      "('Site', '97', 'is not validated from sites')\n",
      "('Site', '106', 'is not validated from check')\n",
      "('Site', '107', 'is not validated from sites')\n",
      "('Site', '118', 'is not validated from sites')\n",
      "('Site', '125', 'is not validated from check 2')\n",
      "('Site', '135', 'is not validated from sites')\n",
      "('Site', '141', 'is not validated from sites')\n",
      "('Site', '144', 'is not validated from sites')\n",
      "('Site', '147', 'is not validated from check 2')\n",
      "('Site', '155', 'is not validated from check 2')\n",
      "('Site', '164', 'is not validated from sites')\n",
      "('Site', '166', 'is not validated from check 2')\n",
      "('Site', '171', 'is not validated from check 2')\n",
      "('Site', '179', 'is not validated from check 2')\n",
      "('Site', '195', 'is not validated from sites')\n",
      "('Site', '209', 'is not validated from check 2')\n",
      "('Site', '211', 'is not validated from check 2')\n",
      "('Site', '216', 'is not validated from sites')\n",
      "('Site', '225', 'is not validated from check 2')\n",
      "('Site', '228', 'is not validated from sites')\n",
      "('Site', '242', 'is not validated from sites')\n",
      "('Site', '244', 'is not validated from sites')\n",
      "('Site', '268', 'is not validated from sites')\n",
      "('Site', '272', 'is not validated from check 2')\n",
      "('Site', '275', 'is not validated from sites')\n",
      "('Site', '282', 'is not validated from sites')\n",
      "('Site', '297', 'is not validated from check 2')\n"
     ]
    }
   ],
   "source": [
    "import time # I used it to see how much time it does to run the function\n",
    "for num in range(0,500):\n",
    "    site = list500_sites[num]\n",
    "    line = list500_url[num] \n",
    "    url_check = \"http://www.webpagefx.com/tools/read-able/check.php?tab=Test+By+Url&uri=http://\" + line\n",
    "    browser = urllib2.build_opener()\n",
    "    browser.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
    "    if site == 0 or num == 107:\n",
    "        print(\"Site\", str(num), \"is not validated from sites\")\n",
    "        flesch.insert(num,\"n/a\")\n",
    "        sentence.insert(num,\"n/a\")\n",
    "        word.insert(num,\"n/a\")\n",
    "        unique_w.insert(num,\"n/a\")  \n",
    "    else:\n",
    "        try:\n",
    "            response = browser.open(url_check)\n",
    "        except Exception: \n",
    "            flesch.insert(num,\"n/a\")\n",
    "            sentence.insert(num,\"n/a\")\n",
    "            word.insert(num,\"n/a\")\n",
    "            unique_w.insert(num,\"n/a\")\n",
    "            print(\"Site\", str(num), \"is not validated from check\")\n",
    "            continue        \n",
    "        html_r = response.read()\n",
    "        check = str(html_r)       \n",
    "        if check != empty:                \n",
    "                soup = BeautifulSoup(check,\"lxml\")\n",
    "                o = 0\n",
    "                keyf = []\n",
    "                for row in soup.html.body.findAll('tr'):\n",
    "                    keyf.insert(o,row)\n",
    "                    o = o + 1\n",
    "                if keyf != empty:                        \n",
    "                        #print(\"Site\", str(num), \"is validated\")\n",
    "                        #Flesh measurement\n",
    "                        if keyf[0] != empty:\n",
    "                            readability = str(keyf[0])\n",
    "                            split1 = readability.split('>')\n",
    "                            readability2 = str(split1[4])\n",
    "                            split2 = readability2.split('<')\n",
    "                            readability3 = str(split2[0])\n",
    "                            flesch.insert(num,readability3)\n",
    "                        else:\n",
    "                            flesch.insert(num,\"n/a\")\n",
    "                            sentence.insert(num,\"n/a\")\n",
    "                            word.insert(num,\"n/a\")\n",
    "                            unique_w.insert(num,\"n/a\")   \n",
    "                        #Number of sentences   \n",
    "                        if keyf[6] != empty:\n",
    "                            sentences = str(keyf[6])\n",
    "                            spli1 = sentences.split('>')\n",
    "                            sentences2 = str(spli1[4])\n",
    "                            spli2 = sentences2.split('<')\n",
    "                            sentences3 = str(spli2[0])\n",
    "                            sentence.insert(num,sentences3)\n",
    "                        else:\n",
    "                            flesch.insert(num,\"n/a\")\n",
    "                            sentence.insert(num,\"n/a\")\n",
    "                            word.insert(num,\"n/a\")\n",
    "                            unique_w.insert(num,\"n/a\")  \n",
    "                        #Number of words\n",
    "                        if keyf[7] != empty:\n",
    "                            words = str(keyf[7])\n",
    "                            spl1 = words.split('>')\n",
    "                            words2 = str(spl1[4])\n",
    "                            spl2 = words2.split('<')\n",
    "                            words3 = str(spl2[0])\n",
    "                            word.insert(num,words3)\n",
    "                        else:\n",
    "                            flesch.insert(num,\"n/a\")\n",
    "                            sentence.insert(num,\"n/a\")\n",
    "                            word.insert(num,\"n/a\")\n",
    "                            unique_w.insert(num,\"n/a\")  \n",
    "                        #No. of complex words\n",
    "                        if keyf[7] != empty:\n",
    "                            unique_ws = str(keyf[8])\n",
    "                            sp1 = unique_ws.split('>')\n",
    "                            unique_ws2 = str(sp1[4])\n",
    "                            sp2 = unique_ws2.split('<')\n",
    "                            unique_ws3 = str(sp2[0])\n",
    "                            unique_w.insert(num,unique_ws3)\n",
    "                        else:\n",
    "                            flesch.insert(num,\"n/a\")\n",
    "                            sentence.insert(num,\"n/a\")\n",
    "                            word.insert(num,\"n/a\")\n",
    "                            unique_w.insert(num,\"n/a\")  \n",
    "                else:\n",
    "                        print(\"Site\", str(num), \"is not validated from check 2\")\n",
    "                        flesch.insert(num,\"n/a\")\n",
    "                        sentence.insert(num,\"n/a\")\n",
    "                        word.insert(num,\"n/a\")\n",
    "                        unique_w.insert(num,\"n/a\")            \n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "readability = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readable (flesch):\n",
    "    for i in range (len(flesch)):\n",
    "            f_n = flesch[i]\n",
    "            if f_n == \"n/a\":\n",
    "                readability.insert(i,\"n/a\")                \n",
    "            else:\n",
    "                a = int(float(f_n))\n",
    "                if a > 90:    \n",
    "                    readability.insert(i,\"Very easy\")                    \n",
    "                elif a > 80:\n",
    "                    readability.insert(i,\"Easy\")\n",
    "                elif a > 70:\n",
    "                    readability.insert(i,\"Fairly easy\")\n",
    "                elif a > 60:\n",
    "                    readability.insert(i,\"Standard\")\n",
    "                elif a > 50:\n",
    "                    readability.insert(i,\"Fairly difficult\")\n",
    "                elif a > 30:\n",
    "                    readability.insert(i,\"Difficult\")\n",
    "                else:\n",
    "                    readability.insert(i,\"Very Confusing\")                    \n",
    "    print \"The function is completed!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "readable (flesch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d1 = {'company' : pd.Series(list500_names, index=[list500_num]),\n",
    "      'url' : pd.Series(list500_url, index=[list500_num]),\n",
    "      'Readability' : pd.Series(readability, index=[list500_num]),\n",
    "      'Flesh_Mesaure' : pd.Series(flesch,index=[list500_num]),\n",
    "'Sentences' : pd.Series(sentence, index=[list500_num]),\n",
    "'Words' : pd.Series(word, index=[list500_num]),\n",
    "'Unique words' : pd.Series(unique_w, index=[list500_num])}\n",
    "fre = pd.DataFrame(d1)    \n",
    "fre.head(3) #we see the first 3 in the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Retreiving the social media from each site\n",
    "#First create empty lists for the ones that \n",
    "#we will need to calculate\n",
    "sm_f = []\n",
    "sm_t = []\n",
    "sm_i = []\n",
    "sm_p = []\n",
    "sm_y = []\n",
    "sm_l = []   \n",
    "sm_nm = [] \n",
    "nm = []\n",
    "sm_url = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Then create a function that will feel in those \n",
    "#lists so as to make the data frame later on\n",
    "def socialmedia (list500_sites,list500_names,list500_url):\n",
    "    from time import time \n",
    "    # I used it to see how much time it does to run the function\n",
    "    start = time ()\n",
    "    for i in range(len(list500_names)):        \n",
    "            myHTML = list500_sites[i]\n",
    "            sm = ['facebook.com','twitter.com',\n",
    "                  'instagram.com','pinterest.com',\n",
    "                  'youtube.com','linkedin.com'] \n",
    "            if myHTML == 0:                \n",
    "                sm_nm.insert(i,list500_names[i]) \n",
    "                nm.insert(i,i)\n",
    "                sm_url.insert(i,list500_url[i])\n",
    "                sm_f.insert(i,'n/a')\n",
    "                sm_t.insert(i,'n/a')\n",
    "                sm_i.insert(i,'n/a')\n",
    "                sm_p.insert(i,'n/a')\n",
    "                sm_y.insert(i,'n/a')\n",
    "                sm_l.insert(i,'n/a')\n",
    "            else:\n",
    "                for index in range(len(sm)):\n",
    "                    x = sm[index]\n",
    "                    social = re.findall(x,myHTML)                                \n",
    "                    if (len(social) > 0):\n",
    "                        if x == 'facebook.com':\n",
    "                            answerf = 'TRUE'\n",
    "                        if x == 'twitter.com':\n",
    "                            answert = 'TRUE'\n",
    "                        if x == 'instagram.com':\n",
    "                            answeri = 'TRUE'\n",
    "                        if x == 'pinterest.com':\n",
    "                            answerp = 'TRUE'\n",
    "                        if x == 'youtube.com':\n",
    "                            answery = 'TRUE'\n",
    "                        if x =='linkedin.com':\n",
    "                            answerl = 'TRUE'                   \n",
    "                    else:\n",
    "                         if x == 'facebook.com':\n",
    "                            answerf = 'FALSE'\n",
    "                         if x == 'twitter.com':\n",
    "                            answert = 'FALSE'\n",
    "                         if x == 'instagram.com':\n",
    "                            answeri = 'FALSE'\n",
    "                         if x == 'pinterest.com':\n",
    "                            answerp = 'FALSE'\n",
    "                         if x == 'youtube.com':\n",
    "                            answery = 'FALSE'\n",
    "                         if x =='linkedin.com':\n",
    "                            answerl = 'FALSE'                \n",
    "                sm_nm.insert(i,list500_names[i]) \n",
    "                nm.insert(i,i)\n",
    "                sm_url.insert(i,list500_url[i])\n",
    "                sm_f.insert(i,answerf)\n",
    "                sm_t.insert(i,answert)\n",
    "                sm_i.insert(i,answeri)\n",
    "                sm_p.insert(i,answerp)\n",
    "                sm_y.insert(i,answery)\n",
    "                sm_l.insert(i,answerl)\n",
    "    end = time ()\n",
    "    duration = round (end - start, 3)\n",
    "    minutes = round (duration /60, 1)\n",
    "    print 'The lists are completed in ', minutes, ' minutes' \n",
    "    print 'The lists are ready in ', duration, ' seconds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Now we will run the function for the 25 first sites for starters\n",
    "socialmedia (list500_sites,list500_names,list500_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Finally we create the data frame with the elements we found            \n",
    "d2 = {'company' : pd.Series(sm_nm, index=[nm]),\n",
    "     'facebook' : pd.Series(sm_f, index=[nm]),\n",
    "      'twitter' : pd.Series(sm_t, index=[nm]),\n",
    "     'instagram' : pd.Series(sm_i, index=[nm]),\n",
    "      'pinterest' : pd.Series(sm_p, index=[nm]),\n",
    "     'youtube' : pd.Series(sm_y, index=[nm]),\n",
    "      'linkedin' : pd.Series(sm_l, index=[nm]),}\n",
    "social_media = pd.DataFrame(d2)    \n",
    "social_media.tail(3) #we see the first 3 in the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create the lists we will need for the data frame\n",
    "l_nm = []\n",
    "l_ex = []\n",
    "l_in = []\n",
    "l_t = []\n",
    "nm = []\n",
    "l_url = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create the function that will calculate the different type of links\n",
    "def links (list500_sites,list500_names,list500_url):\n",
    "    from time import time \n",
    "    # I used it to see how much time it does to run the function\n",
    "    start = time ()\n",
    "    for num in range(len(list500_names)):        \n",
    "            myHTML = list500_sites[num]\n",
    "            if myHTML == 0:\n",
    "                l_nm.insert(num,list500_names[num])            \n",
    "                l_ex.insert(num,'n/a')\n",
    "                l_t.insert(num,'n/a')\n",
    "                l_in.insert(num,'n/a')\n",
    "                nm.insert(num,num)                \n",
    "            else: \n",
    "                href = re.findall('href',myHTML)\n",
    "                external = re.findall('href=\"https:',myHTML)\n",
    "                ex = (len(external))\n",
    "                alllinks = (len(href))\n",
    "                internal =  (len(href) - len(external))\n",
    "                l_nm.insert(num,list500_names[num])            \n",
    "                l_ex.insert(num,ex)\n",
    "                l_t.insert(num,alllinks)\n",
    "                l_in.insert(num,internal)\n",
    "                nm.insert(num,num)                \n",
    "    end = time ()\n",
    "    duration = round (end - start, 3)\n",
    "    minutes = round (duration /60, 1)\n",
    "    print 'The lists are ready in ', minutes, ' minutes'\n",
    "    print 'The lists are ready in ', duration, ' seconds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Run the function in order to find the external, \n",
    "#internal and total links of each site\n",
    "#For now we are running for the first 25 sites only\n",
    "links (list500_sites,list500_names,list500_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create a dataframe so as to be able to see \n",
    "#the results of the function we run\n",
    "d3 = {'company' : pd.Series(l_nm, index=[nm]),\n",
    "      'external' : pd.Series(l_ex, index=[nm]),\n",
    "      'internal' : pd.Series(l_in, index=[nm]),\n",
    "     'total links' : pd.Series(l_t, index=[nm])}\n",
    "sites_links = pd.DataFrame(d3)    \n",
    "sites_links.tail(3) #we see the first 3 in the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The initial lists we will need in order \n",
    "#to calculate the loading time\n",
    "lt_nm = [] \n",
    "lt_time = []\n",
    "nm = []\n",
    "lt_url = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#the function that will calculate the loading time\n",
    "def loadtime (list_company_website,list500_names,list500_url):\n",
    "    from time import time\n",
    "    browser2 = urllib2.build_opener()\n",
    "    browser2.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
    "    for num in range(len(list500_names)):\n",
    "        lc = str(list_company_website[num])        \n",
    "        lc = lc.replace(\"'\",\"\")   \n",
    "        lc = lc.replace(\"[\",\"\")\n",
    "        lc = lc.replace(\"]\",\"\")\n",
    "        url2 = 'http://' + lc\n",
    "        if num == 118 or num == 464 or num == 70:\n",
    "            #The site 118(119) has a problem and the whole code \n",
    "            #is stacking when I run it so we will thing of this \n",
    "            #site as a not downloadable\n",
    "            lt_nm.insert(num,list500_names[num])            \n",
    "            lt_time.insert(num,'n/a')\n",
    "            nm.insert(num,num)\n",
    "            lt_url.insert(num,list500_url[num])            \n",
    "        else:\n",
    "            try:\n",
    "                response2 = browser2.open(url2)\n",
    "            except Exception:\n",
    "                lt_time.insert(num,'n/a')\n",
    "                lt_nm.insert(num,list500_names[num])  \n",
    "                nm.insert(num,num)\n",
    "                print (\"The site \" + str(num)+ \" has NOT been loaded!\")\n",
    "                continue     \n",
    "            start_time = time()\n",
    "            myHTML2 = response2.read()\n",
    "            end_time = time()\n",
    "            response2.close()\n",
    "            l_t = round(end_time-start_time, 3) \n",
    "            #in order to be more readable we rounded the time\n",
    "            loadt = str(l_t)\n",
    "            lt_nm.insert(num,list500_names[num])            \n",
    "            lt_time.insert(num,loadt)\n",
    "            nm.insert(num,num)\n",
    "            lt_url.insert(num,list500_url[num])\n",
    "            #print (\"The site \" + str(num) + \" has been loaded!\")\n",
    "    print \"The function is completed!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#running the function for the first 25 sites\n",
    "loadtime (list_company_website,list500_names,list500_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#creating the data frame with the loading times\n",
    "d4 = {'company' : pd.Series(lt_nm, index=[nm]),\n",
    "      'loading time' : pd.Series(lt_time, index=[nm])}\n",
    "loading_time = pd.DataFrame(d4)    \n",
    "loading_time.head(3) #we see the first 3 in the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Find out how many and what type of images each site has\n",
    "#first we create the initially empty lists\n",
    "p_p = []\n",
    "p_d = []\n",
    "p_jpg = []\n",
    "p_jpeg = []\n",
    "p_gif = []\n",
    "p_tif = []\n",
    "p_tiff = []\n",
    "p_bmp = []\n",
    "p_jpe = []\n",
    "p_nm = []\n",
    "p_tt =[]\n",
    "nm = []\n",
    "p_url = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Then we create the function that will explore \n",
    "#the html pages and search for the images\n",
    "def images (list500_sites,list500_names,list500_url):\n",
    "    from time import time # I used it to see \n",
    "    #how much time it does to run the function\n",
    "    start = time ()\n",
    "    for num in range(len(list500_names)):\n",
    "            myHTML = list500_sites[num] \n",
    "            image = ['.png','.dib','.jpg','.jpeg',\n",
    "                     '.bmp','.jpe','.gif','.tif','.tiff'] \n",
    "            totalnumber = 0 \n",
    "            if myHTML == 0:\n",
    "                p_nm.insert(num,list500_names[num])            \n",
    "                p_p.insert(num,'n/a')  \n",
    "                p_d.insert(num,'n/a')  \n",
    "                p_jpg.insert(num,'n/a')  \n",
    "                p_jpeg.insert(num,'n/a')  \n",
    "                p_gif.insert(num,'n/a')  \n",
    "                p_tif.insert(num,'n/a')  \n",
    "                p_tiff.insert(num,'n/a')  \n",
    "                p_bmp.insert(num,'n/a')  \n",
    "                p_jpe.insert(num,'n/a')  \n",
    "                p_tt.insert(num,'n/a')\n",
    "                nm.insert(num,num)\n",
    "                p_url.insert(num,list500_url[num])          \n",
    "            else: \n",
    "                for index in range(len(image)):\n",
    "                    x = image[index]\n",
    "                    photo = re.findall(x,myHTML)\n",
    "                    if x == '.png':\n",
    "                        p = str (len(photo))\n",
    "                    if x == '.dib':\n",
    "                        d = str (len(photo))\n",
    "                    if x == '.jpg':\n",
    "                        jpg = str (len(photo))\n",
    "                    if x == '.jpeg':\n",
    "                        jpeg = str (len(photo))\n",
    "                    if x == '.gif':\n",
    "                        gif = str (len(photo))\n",
    "                    if x == '.tif':\n",
    "                        tif = str (len(photo))\n",
    "                    if x == '.tiff':\n",
    "                        tiff = str (len(photo))\n",
    "                    if x == '.bmp':\n",
    "                        bmp = str (len(photo))\n",
    "                    if x == '.jpe':\n",
    "                        jpe = str (len(photo))\n",
    "                    totalnumber = len(photo) + totalnumber\n",
    "                total = str (totalnumber)\n",
    "                p_nm.insert(num,list500_names[num])            \n",
    "                p_p.insert(num,p)  \n",
    "                p_d.insert(num,d)  \n",
    "                p_jpg.insert(num,jpg)  \n",
    "                p_jpeg.insert(num,jpeg)  \n",
    "                p_gif.insert(num,gif)  \n",
    "                p_tif.insert(num,tif)  \n",
    "                p_tiff.insert(num,tiff)  \n",
    "                p_bmp.insert(num,bmp)  \n",
    "                p_jpe.insert(num,jpe)  \n",
    "                p_tt.insert(num,total)\n",
    "                nm.insert(num,num)\n",
    "                p_url.insert(num,list500_url[num])\n",
    "    end = time ()\n",
    "    duration = round (end - start, 3)\n",
    "    minutes = round (duration /60, 1)\n",
    "    print 'The lists are ready in ', minutes, ' minutes'\n",
    "    print 'The lists are ready in ', duration, ' seconds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Then we run the function for the first 20 sites for now\n",
    "images (list500_sites,list500_names,list500_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Finally we create a dataframe in order to see the results of the function\n",
    "d5 = {'company' : pd.Series(p_nm, index=[nm]),\n",
    "      '.png' : pd.Series(p_p, index=[nm]),\n",
    "      '.dib' : pd.Series(p_d, index=[nm]),\n",
    "      '.jpg' : pd.Series(p_jpg, index=[nm]),\n",
    "      '.jpeg' : pd.Series(p_jpeg, index=[nm]),\n",
    "      '.bmp' : pd.Series(p_bmp, index=[nm]),\n",
    "      '.jpe' : pd.Series(p_jpe, index=[nm]),\n",
    "      '.gif' : pd.Series(p_gif, index=[nm]),\n",
    "      '.tif' : pd.Series(p_tif, index=[nm]),\n",
    "      '.tiff' : pd.Series(p_tiff, index=[nm]), \n",
    "      'total images' : pd.Series(p_tt, index=[nm])}\n",
    "images_types = pd.DataFrame(d5)    \n",
    "images_types.head(3) #we see the first 3 in the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now we will gather the total number of pixels that exist\n",
    "#in each web page under examination\n",
    "#with this we will have a grasp of the images that each site is using\n",
    "im_nm = []\n",
    "im_pix = []    \n",
    "import time\n",
    "def url_to_image(list500_names, list500_url):\n",
    "    import numpy as np\n",
    "    import urllib\n",
    "    for num in range(len(list500_names)):\n",
    "        url= 'http://' + list500_url[num]\n",
    "        if num == 45 or num == 118:\n",
    "            print (str(num), \"Exception\")\n",
    "            im_pix.insert(num,\"n/a\")\n",
    "            im_nm.insert(num,list500_names[num]) \n",
    "        else:\n",
    "            try: # download the image, convert it to a NumPy array \n",
    "                resp = urllib.urlopen(url)\n",
    "            except Exception:\n",
    "                print (str(num), \"Exception\")\n",
    "                im_pix.insert(num,\"n/a\")\n",
    "                im_nm.insert(num,list500_names[num]) \n",
    "                continue\n",
    "            image = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
    "            #print(image)\n",
    "            pixels = 0\n",
    "            #add up all the pixels of the page\n",
    "            for n in range (len(image)):\n",
    "                pixels = pixels + image[n]\n",
    "            print (str(num), pixels)\n",
    "            im_nm.insert(num,list500_names[num])            \n",
    "            im_pix.insert(num,pixels)\n",
    "        time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Run the function\n",
    "url_to_image(list500_names, list500_url) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Finally we create a dataframe in order to see the results of the function\n",
    "dpix = {'company' : pd.Series(im_nm, index=[list500_num]),\n",
    "      'pixels' : pd.Series(im_pix, index=[list500_num])}\n",
    "images_pixels = pd.DataFrame(dpix)    \n",
    "images_pixels.head(3) #we see the first 3 in the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#In order to validate the html code we will use the w3 validator\n",
    "#We will validate each url and then we will open the url of the validation page\n",
    "#so as to extract the errors, the info warnings and the non-document-error io informations \n",
    "#First we create the empty lists we would use later on\n",
    "num_errors = []\n",
    "num_info_warnings = []\n",
    "num_non_doc = [] \n",
    "nm = []\n",
    "num_open_page = []\n",
    "empty = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Then we create the function that will pull the informations we want\n",
    "def html_validation (list500_url,list500_names):\n",
    "    from time import time # I used it to see how much time it does to run the function\n",
    "    start = time ()\n",
    "    for num in range(len(list500_names)):\n",
    "        line = list500_url[num] \n",
    "        url_check = \"https://validator.w3.org/nu/?doc=https://\" + line\n",
    "        browser = urllib2.build_opener()\n",
    "        browser.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
    "        response = browser.open(url_check)\n",
    "        html_check = response.read()\n",
    "        html_check\n",
    "        check = str(html_check)\n",
    "        er = 0\n",
    "        err = 0\n",
    "        errr = 0\n",
    "        e = False\n",
    "        if check != empty:\n",
    "            e = True\n",
    "            soup = BeautifulSoup(check,\"lxml\")\n",
    "            o = 0\n",
    "            keyf = []\n",
    "            for row in soup.html.body.findAll('div'):\n",
    "                keyf.insert(o,row)\n",
    "                o = o + 1\n",
    "            #print(len(keyf),list500_url[num], \"site number: \", str(num))        \n",
    "            if len(keyf) != 0:       \n",
    "                    keyfin = str(keyf[2]) \n",
    "                    #the elements we need is in the 2nd div of the code\n",
    "                    dol= re.findall('class=\"error\"',keyfin)            \n",
    "                    er = er + len(dol)\n",
    "                    doll= re.findall('class=\"info warning\"'\n",
    "                                     ,keyfin)            \n",
    "                    err = err + len(doll)\n",
    "                    dolll= re.findall('class=\"non-document-error io\"'\n",
    "                                      ,keyfin)            \n",
    "                    errr = errr + len(dolll)\n",
    "        num_errors.insert(num,er)\n",
    "        num_info_warnings.insert(num,err)\n",
    "        num_non_doc.insert(num,errr)  \n",
    "        nm.insert(num,num) \n",
    "        num_open_page.insert(num,e)\n",
    "    end = time ()\n",
    "    duration = round (end - start, 3)\n",
    "    minutes = round (duration /60, 1)\n",
    "    print 'The lists are ready in ', minutes, ' minutes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Now we will run the function we created\n",
    "html_validation (list500_url,list500_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#After the checks we will create the dataframe with the informations we want\n",
    "d8 = {'company' : pd.Series(list500_names, index=[nm]),\n",
    "      'The_page_opened' : pd.Series(num_open_page, index=[nm])\n",
    "      ,'number_of_errors' : pd.Series(num_errors, index=[nm]),\n",
    "      'number_of_warning' : pd.Series(num_info_warnings, index=[nm])\n",
    "      ,'non-document-error' : pd.Series(num_non_doc, index=[nm])}\n",
    "html_val = pd.DataFrame(d8)    \n",
    "html_val.head(3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The next step is to take some informations from the fortune 500 site for each company\n",
    "#In order to achieve that we should open the pages for each one of the sites seperately\n",
    "#Since there is a pattern in the way the pages are named it shouldn't be difficult\n",
    "#Firstly we should create the pattern with which we will download the pages\n",
    "#By running the code we can see that the names of each comany are not \n",
    "#written exactly as we have saved them\n",
    "#So we do need to alter the names first in order for the below function to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creating a new list with alterations in order for the names\n",
    "#to match the ones that fortune 500 uses so that we can download the html page\n",
    "list_company_name_new = []\n",
    "for num in range (0,500):\n",
    "    cn = list_company_name[num]\n",
    "    cn = cn.replace(\" \", \"-\")\n",
    "    cn = cn.replace(\"&\", \"\")\n",
    "    cn = cn.replace(\"’\", \"\")\n",
    "    cn = cn.replace(\".\", \"-\")\n",
    "    cn = cn.replace(\"amp;\", \"\")    \n",
    "    company = cn.lower()\n",
    "    list_company_name_new.insert(num,cn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fortune_pages = []\n",
    "def fortune500 (list_company_name_new):\n",
    "    from time import time # I used it to see how much time it does to run the function\n",
    "    start = time ()\n",
    "    for num3 in range (0,500):\n",
    "        i = str (num3 +1)    \n",
    "        companyname =  list_company_name_new[num3]\n",
    "        browser = urllib2.build_opener() \n",
    "        #because i work from different computers with different \n",
    "        #pyhton version some commands are not recognizable in each version\n",
    "        browser.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
    "        site_fortune = \"http://beta.fortune.com/fortune500/\"+companyname+\"-\"+ i    \n",
    "        page_fortune = browser.open(site_fortune)\n",
    "        html_fortune = page_fortune.read()    \n",
    "        #print(\"fortune page for company: \", list_company_name_new[num3],i)\n",
    "        fortune_pages.insert(num3, html_fortune)\n",
    "    end = time ()\n",
    "    duration = round (end - start, 3)\n",
    "    minutes = round (duration /60, 1)\n",
    "    print 'The lists are ready in ', minutes, ' minutes'\n",
    "    print 'The lists are ready in ', duration, ' seconds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Run the function we created\n",
    "fortune500 (list_company_name_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now that we have opened the url we are going to extract \n",
    "#some informations that we need from them\n",
    "#In order to do that initially we have to create \n",
    "#the variables we will need\n",
    "keyf =[]\n",
    "keyfaw = []\n",
    "per =[]\n",
    "rev_dol = []\n",
    "prof_dol = []\n",
    "assets_dol = []\n",
    "tse_dol = []\n",
    "tse_per = []\n",
    "mar_dol = []\n",
    "market = []\n",
    "nm = []\n",
    "ln = []\n",
    "urln = []\n",
    "empty = []\n",
    "kewfawin = 0\n",
    "pef = []\n",
    "pe_rat =[]\n",
    "pr_r = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fortune_metrics (list_company_name,list_company_website):\n",
    "    for n in range (0,500):   \n",
    "        x = 0\n",
    "        nm.insert(x,x)\n",
    "        ln.insert(x,list_company_name[n])\n",
    "        urln.insert(x,list_company_website[n])\n",
    "        files = fortune_pages[n]\n",
    "        soup = BeautifulSoup(files, 'lxml')\n",
    "        #P/E Ratio\n",
    "        aw = 0\n",
    "        for raw in soup.html.body.findAll('li'):\n",
    "            keyfaw.insert(aw,raw)\n",
    "            aw = aw + 1\n",
    "        keyfawin = keyfaw[14]\n",
    "        keyfawin\n",
    "        pe = str(keyfawin)        \n",
    "        pef = pe.split('>')\n",
    "        pef\n",
    "        pef2 = pef[4]\n",
    "        pef3 = pef2.split(\"<\")\n",
    "        peratio = pef3[0]\n",
    "        peratio\n",
    "        if peratio[0] != empty:\n",
    "            w = peratio[0]\n",
    "            a = w.replace(\"[\", \"\")\n",
    "            r = a.replace(\"]\",\"\")\n",
    "            pe_rat.insert(x,peratio)\n",
    "        else:\n",
    "            pe_rat.insert(x,'not available')        \n",
    "        ###############################################\n",
    "        o=0\n",
    "        for row in soup.html.body.findAll('tbody'):\n",
    "            keyf.insert(o,row)\n",
    "            o = o + 1\n",
    "        keyfin = keyf[0]  #the elements we need is in the first tbody of the code  \n",
    "        keyfon = keyf[1]\n",
    "        data = keyfin.findAll('td')\n",
    "        ##################################\n",
    "        # revenue\n",
    "        two = str(data[1]) \n",
    "        # revenue in dollars we need to extract this\n",
    "        revdol= re.findall('>\\$(.+?)</td>',two) \n",
    "        #we keep only the numbers\n",
    "        if revdol[0] != empty:\n",
    "            w = revdol[0]\n",
    "            a = w.replace(\"[\", \"\")\n",
    "            r = a.replace(\"]\",\"\")\n",
    "            rev_dol.insert(x,r)\n",
    "        else:\n",
    "            rev_dol.insert(x,'not available')\n",
    "        five = str(data[5])\n",
    "        #################################\n",
    "        # profit in dollars we need to extract this   \n",
    "        profdol= re.findall('>\\$(.+?)</td>',five) \n",
    "        #we keep only the numbers\n",
    "        if profdol != empty:\n",
    "            w = profdol[0]\n",
    "            a = w.replace(\"[\", \"\")\n",
    "            p = a.replace(\"]\",\"\")\n",
    "            prof_dol.insert(x,p)\n",
    "        else:\n",
    "            prof_dol.insert(x,'not available')\n",
    "        ###############################\n",
    "        #assets in dollard\n",
    "        eight = str(data[7]) #assets in dollars we need to extract this\n",
    "        assetsdol= re.findall('>\\$(.+?)</td>',eight) \n",
    "        #we keep only the numbers\n",
    "        if assetsdol != empty:\n",
    "            w = assetsdol[0]\n",
    "            a = w.replace(\"[\", \"\")\n",
    "            ass = a.replace(\"]\",\"\")\n",
    "            assets_dol.insert(x,ass)\n",
    "        else:\n",
    "            assets_dol.insert(x,'not available')\n",
    "        ###################################\n",
    "       #Total Stockholder Equity ($M)    \n",
    "        eleven = str(data[10]) \n",
    "        #Total Stockholder Equity ($M) in dollars we need to extract this\n",
    "        tsedol= re.findall('>\\$(.+?)</td>',eleven) \n",
    "        #we keep only the numbers\n",
    "        if tsedol != empty:\n",
    "            w = tsedol[0]\n",
    "            a = w.replace(\"[\", \"\")\n",
    "            ts = a.replace(\"]\",\"\")\n",
    "            tse_dol.insert(x,ts)\n",
    "        else:\n",
    "            tse_dol.insert(x,'not available')\n",
    "        ########################################\n",
    "        # market value\n",
    "        fourteen = str(data[13]) \n",
    "        # market value in dollars we need to extract this\n",
    "        mardol= re.findall('>\\$(.+?)</td>',fourteen) \n",
    "        #we keep only the numbers\n",
    "        if mardol != empty:\n",
    "            w = mardol[0]\n",
    "            a = w.replace(\"[\", \"\")\n",
    "            mar = a.replace(\"]\",\"\")\n",
    "            mar_dol.insert(x,mar)\n",
    "        else:\n",
    "            mar_dol.insert(x,'not available')\n",
    "        ##############################################################    \n",
    "        datao = keyfon.findAll('td')        \n",
    "        # Profit as % of Revenues\n",
    "        twoo = str(datao[2]) \n",
    "        prof_rev= re.findall('>(.+?)%</td>',twoo) \n",
    "        #we keep only the numbers\n",
    "        if  prof_rev[0] != empty:\n",
    "            w = prof_rev[0]\n",
    "            a = w.replace(\"[\", \"\")\n",
    "            r = a.replace(\"]\",\"\")\n",
    "            pr_r.insert(x,r)\n",
    "        else:\n",
    "            pr_r.insert(x,'not available')\n",
    "        x = x + 1\n",
    "    print \"The function is complete!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fortune_metrics (list_company_name,list_company_website)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d9 = {'company' : pd.Series(ln, index=[nm]),\n",
    "      'Revenues $' : pd.Series(rev_dol, index=[nm]),\n",
    "      'Profits $' : pd.Series(prof_dol, index=[nm]),\n",
    "      'Assets $' : pd.Series(assets_dol, index=[nm]),\n",
    "      'Total Stockholder Equity $' : pd.Series(tse_dol, index=[nm]),\n",
    "      'PE Ratio' : pd.Series(pe_rat, index=[nm]),\n",
    "'Proft_%_Revenues' : pd.Series(pr_r, index=[nm]),\n",
    "      'Market value $' : pd.Series(mar_dol, index=[nm])}\n",
    "fort500 = pd.DataFrame(d9)    \n",
    "fort500.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = pd.merge(fort500, html_val, how='inner', on=['company', 'company'])\n",
    "result2 = pd.merge(social_media, fre, how='inner', on=['company', 'company'])\n",
    "result3 = pd.merge(sites_links, images_pixels, how='inner', on=['company', 'company'])\n",
    "result4 = pd.merge(images_types, loading_time, how='inner', on=['company', 'company'])\n",
    "result5 = pd.merge(result,result2 , how='inner', on=['company', 'company'])\n",
    "result6 = pd.merge(result3, result4, how='inner', on=['company', 'company'])\n",
    "final = pd.merge(result5, result6, how='inner', on=['company', 'company'])\n",
    "final.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final.to_csv('total_500_final.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data500 = pd.read_csv(\"total_500_final.csv\", sep=';') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data500.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
